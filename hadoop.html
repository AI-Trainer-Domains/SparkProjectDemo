<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <title>Title</title>
  </head>
  <body>
    <main>
      <div id="bodyColumn">
        <div id="contentBox">
          <!---
            Licensed under the Apache License, Version 2.0 (the "License");
            you may not use this file except in compliance with the License.
            You may obtain a copy of the License at

             http://www.apache.org/licenses/LICENSE-2.0

            Unless required by applicable law or agreed to in writing, software
            distributed under the License is distributed on an "AS IS" BASIS,
            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            See the License for the specific language governing permissions and
            limitations under the License. See accompanying LICENSE file.
            -->
          <h1>Hadoop: Setting up a Single Node Cluster.</h1>
          <ul>
            <li><a href="#Purpose">Purpose</a></li>
            <li>
              <a href="#Prerequisites">Prerequisites</a>
              <ul>
                <li><a href="#Supported_Platforms">Supported Platforms</a></li>
                <li><a href="#Required_Software">Required Software</a></li>
                <li><a href="#Installing_Software">Installing Software</a></li>
              </ul>
            </li>
            <li><a href="#Download">Download</a></li>
            <li><a href="#Prepare_to_Start_the_Hadoop_Cluster">Prepare to Start the Hadoop Cluster</a></li>
            <li><a href="#Standalone_Operation">Standalone Operation</a></li>
            <li>
              <a href="#Pseudo-Distributed_Operation">Pseudo-Distributed Operation</a>
              <ul>
                <li><a href="#Configuration">Configuration</a></li>
                <li><a href="#Setup_passphraseless_ssh">Setup passphraseless ssh</a></li>
                <li><a href="#Execution">Execution</a></li>
                <li><a href="#YARN_on_a_Single_Node">YARN on a Single Node</a></li>
              </ul>
            </li>
            <li><a href="#Fully-Distributed_Operation">Fully-Distributed Operation</a></li>
          </ul>
          <section>
            <h2><a name="Purpose"></a>Purpose</h2>
            <p>This document describes how to set up and configure a single-node Hadoop installation so that you can quickly perform simple operations using Hadoop MapReduce and the Hadoop Distributed File System (HDFS).</p>
            <p><i>Important</i>: all production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.).</p>
            <p>These instructions do not cover integration with any Kerberos services, -everyone bringing up a production cluster should include connecting to their organisation’s Kerberos infrastructure as a key part of the deployment.</p>
            <p>See <a href="./SecureMode.html">Security</a> for details on how to secure a cluster.</p>
          </section>
          <section>
            <h2><a name="Prerequisites"></a>Prerequisites</h2>
            <section>
              <h3><a name="Supported_Platforms"></a>Supported Platforms</h3>
              <ul>
                <li>GNU/Linux is supported as a development and production platform. Hadoop has been demonstrated on GNU/Linux clusters with 2000 nodes.</li>
              </ul>
            </section>
            <section>
              <h3><a name="Required_Software"></a>Required Software</h3>
              <p>Required software for Linux include:</p>
              <ol style="list-style-type: decimal">
                <li>
                  <p>Java™ must be installed. Recommended Java versions are described at <a class="externalLink" href="https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions">HadoopJavaVersions</a>.</p>
                </li>
                <li>
                  <p>ssh must be installed and sshd must be running to use the Hadoop scripts that manage remote Hadoop daemons if the optional start and stop scripts are to be used. Additionally, it is recommmended that pdsh also be installed for better ssh resource management.</p>
                </li>
              </ol>
            </section>
            <section>
              <h3><a name="Installing_Software"></a>Installing Software</h3>
              <p>If your cluster doesn’t have the requisite software you will need to install it.</p>
              <p>For example on Ubuntu Linux:</p>
              <div class="source">
                <div class="source">
                  <pre>  $ sudo apt-get install ssh
  $ sudo apt-get install pdsh
</pre>
                </div>
              </div>
            </section>
          </section>
          <section>
            <h2><a name="Download"></a>Download</h2>
            <p>To get a Hadoop distribution, download a recent stable release from one of the <a class="externalLink" href="http://www.apache.org/dyn/closer.cgi/hadoop/common/">Apache Download Mirrors</a>.</p>
          </section>
          <section>
            <h2><a name="Prepare_to_Start_the_Hadoop_Cluster"></a>Prepare to Start the Hadoop Cluster</h2>
            <p>Unpack the downloaded Hadoop distribution. In the distribution, edit the file <code>etc/hadoop/hadoop-env.sh</code> to define some parameters as follows:</p>
            <div class="source">
              <div class="source">
                <pre>  # set to the root of your Java installation
  export JAVA_HOME=/usr/java/latest
</pre>
              </div>
            </div>
            <p>Try the following command:</p>
            <div class="source">
              <div class="source">
                <pre>  $ bin/hadoop
</pre>
              </div>
            </div>
            <p>This will display the usage documentation for the hadoop script.</p>
            <p>Now you are ready to start your Hadoop cluster in one of the three supported modes:</p>
            <ul>
              <li><a href="#Standalone_Operation">Local (Standalone) Mode</a></li>
              <li><a href="#Pseudo-Distributed_Operation">Pseudo-Distributed Mode</a></li>
              <li><a href="#Fully-Distributed_Operation">Fully-Distributed Mode</a></li>
            </ul>
          </section>
          <section>
            <h2><a name="Standalone_Operation"></a>Standalone Operation</h2>
            <p>By default, Hadoop is configured to run in a non-distributed mode, as a single Java process. This is useful for debugging.</p>
            <p>The following example copies the unpacked conf directory to use as input and then finds and displays every match of the given regular expression. Output is written to the given output directory.</p>
            <div class="source">
              <div class="source">
                <pre>  $ mkdir input
  $ cp etc/hadoop/*.xml input
  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
  $ cat output/*
</pre>
              </div>
            </div>
          </section>
          <section>
            <h2><a name="Pseudo-Distributed_Operation"></a>Pseudo-Distributed Operation</h2>
            <p>Hadoop can also be run on a single-node in a pseudo-distributed mode where each Hadoop daemon runs in a separate Java process.</p>
            <section>
              <h3><a name="Configuration"></a>Configuration</h3>
              <p>Use the following:</p>
              <p>etc/hadoop/core-site.xml:</p>
              <div class="source">
                <div class="source">
                  <pre>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;fs.defaultFS&lt;/name&gt;
        &lt;value&gt;hdfs://localhost:9000&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
                </div>
              </div>
              <p>etc/hadoop/hdfs-site.xml:</p>
              <div class="source">
                <div class="source">
                  <pre>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;dfs.replication&lt;/name&gt;
        &lt;value&gt;1&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
                </div>
              </div>
            </section>
            <section>
              <h3><a name="Setup_passphraseless_ssh"></a>Setup passphraseless ssh</h3>
              <p>Now check that you can ssh to the localhost without a passphrase:</p>
              <div class="source">
                <div class="source">
                  <pre>  $ ssh localhost
</pre>
                </div>
              </div>
              <p>If you cannot ssh to localhost without a passphrase, execute the following commands:</p>
              <div class="source">
                <div class="source">
                  <pre>  $ ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa
  $ cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
  $ chmod 0600 ~/.ssh/authorized_keys
</pre>
                </div>
              </div>
            </section>
            <section>
              <h3><a name="Execution"></a>Execution</h3>
              <p>The following instructions are to run a MapReduce job locally. If you want to execute a job on YARN, see <a href="#YARN_on_a_Single_Node">YARN on Single Node</a>.</p>
              <ol style="list-style-type: decimal">
                <li>
                  <p>Format the filesystem:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hdfs namenode -format
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Start NameNode daemon and DataNode daemon:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ sbin/start-dfs.sh
</pre>
                    </div>
                  </div>
                  <p>The hadoop daemon log output is written to the <code>$HADOOP_LOG_DIR</code> directory (defaults to <code>$HADOOP_HOME/logs</code>).</p>
                </li>
                <li>
                  <p>Browse the web interface for the NameNode; by default it is available at:</p>
                  <ul>
                    <li>NameNode - <code>http://localhost:9870/</code></li>
                  </ul>
                </li>
                <li>
                  <p>Make the HDFS directories required to execute MapReduce jobs:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hdfs dfs -mkdir -p /user/&lt;username&gt;
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Copy the input files into the distributed filesystem:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hdfs dfs -mkdir input
  $ bin/hdfs dfs -put etc/hadoop/*.xml input
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Run some of the examples provided:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.6.jar grep input output 'dfs[a-z.]+'
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Examine the output files: Copy the output files from the distributed filesystem to the local filesystem and examine them:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hdfs dfs -get output output
  $ cat output/*
</pre>
                    </div>
                  </div>
                  <p>or</p>
                  <p>View the output files on the distributed filesystem:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ bin/hdfs dfs -cat output/*
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>When you’re done, stop the daemons with:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ sbin/stop-dfs.sh
</pre>
                    </div>
                  </div>
                </li>
              </ol>
            </section>
            <section>
              <h3><a name="YARN_on_a_Single_Node"></a>YARN on a Single Node</h3>
              <p>You can run a MapReduce job on YARN in a pseudo-distributed mode by setting a few parameters and running ResourceManager daemon and NodeManager daemon in addition.</p>
              <p>The following instructions assume that 1. ~ 4. steps of <a href="#Execution">the above instructions</a> are already executed.</p>
              <ol style="list-style-type: decimal">
                <li>
                  <p>Configure parameters as follows:</p>
                  <p><code>etc/hadoop/mapred-site.xml</code>:</p>
                  <div class="source">
                    <div class="source">
                      <pre>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
        &lt;value&gt;yarn&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;mapreduce.application.classpath&lt;/name&gt;
        &lt;value&gt;$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*:$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
                    </div>
                  </div>
                  <p><code>etc/hadoop/yarn-site.xml</code>:</p>
                  <div class="source">
                    <div class="source">
                      <pre>&lt;configuration&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
    &lt;/property&gt;
    &lt;property&gt;
        &lt;name&gt;yarn.nodemanager.env-whitelist&lt;/name&gt;
        &lt;value&gt;JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Start ResourceManager daemon and NodeManager daemon:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ sbin/start-yarn.sh
</pre>
                    </div>
                  </div>
                </li>
                <li>
                  <p>Browse the web interface for the ResourceManager; by default it is available at:</p>
                  <ul>
                    <li>ResourceManager - <code>http://localhost:8088/</code></li>
                  </ul>
                </li>
                <li>
                  <p>Run a MapReduce job.</p>
                </li>
                <li>
                  <p>When you’re done, stop the daemons with:</p>
                  <div class="source">
                    <div class="source">
                      <pre>  $ sbin/stop-yarn.sh
</pre>
                    </div>
                  </div>
                </li>
              </ol>
            </section>
          </section>
          <section>
            <h2><a name="Fully-Distributed_Operation"></a>Fully-Distributed Operation</h2>
            <p>For information on setting up fully-distributed, non-trivial clusters see <a href="./ClusterSetup.html">Cluster Setup</a>.</p>
          </section>
        </div>
      </div>
      <div id="setup">
        <div id="contentBox">
          <!---
            Licensed under the Apache License, Version 2.0 (the "License");
            you may not use this file except in compliance with the License.
            You may obtain a copy of the License at

             http://www.apache.org/licenses/LICENSE-2.0

            Unless required by applicable law or agreed to in writing, software
            distributed under the License is distributed on an "AS IS" BASIS,
            WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
            See the License for the specific language governing permissions and
            limitations under the License. See accompanying LICENSE file.
            -->
          <h1>Hadoop Cluster Setup</h1>
          <ul>
            <li><a href="#Purpose">Purpose</a></li>
            <li><a href="#Prerequisites">Prerequisites</a></li>
            <li><a href="#Installation">Installation</a></li>
            <li>
              <a href="#Configuring_Hadoop_in_Non-Secure_Mode">Configuring Hadoop in Non-Secure Mode</a>
              <ul>
                <li><a href="#Configuring_Environment_of_Hadoop_Daemons">Configuring Environment of Hadoop Daemons</a></li>
                <li><a href="#Configuring_the_Hadoop_Daemons">Configuring the Hadoop Daemons</a></li>
              </ul>
            </li>
            <li><a href="#Monitoring_Health_of_NodeManagers">Monitoring Health of NodeManagers</a></li>
            <li><a href="#Slaves_File">Slaves File</a></li>
            <li><a href="#Hadoop_Rack_Awareness">Hadoop Rack Awareness</a></li>
            <li><a href="#Logging">Logging</a></li>
            <li>
              <a href="#Operating_the_Hadoop_Cluster">Operating the Hadoop Cluster</a>
              <ul>
                <li><a href="#Hadoop_Startup">Hadoop Startup</a></li>
                <li><a href="#Hadoop_Shutdown">Hadoop Shutdown</a></li>
              </ul>
            </li>
            <li><a href="#Web_Interfaces">Web Interfaces</a></li>
          </ul>
          <section>
            <h2><a name="Purpose"></a>Purpose</h2>
            <p>This document describes how to install and configure Hadoop clusters ranging from a few nodes to extremely large clusters with thousands of nodes. To play with Hadoop, you may first want to install it on a single machine (see <a href="./SingleCluster.html">Single Node Setup</a>).</p>
            <p>This document does not cover advanced topics such as High Availability.</p>
            <p><i>Important</i>: all production Hadoop clusters use Kerberos to authenticate callers and secure access to HDFS data as well as restriction access to computation services (YARN etc.).</p>
            <p>These instructions do not cover integration with any Kerberos services, -everyone bringing up a production cluster should include connecting to their organisation’s Kerberos infrastructure as a key part of the deployment.</p>
            <p>See <a href="./SecureMode.html">Security</a> for details on how to secure a cluster.</p>
          </section>
          <section>
            <h2><a name="Prerequisites"></a>Prerequisites</h2>
            <ul>
              <li>Install Java. See the <a class="externalLink" href="https://cwiki.apache.org/confluence/display/HADOOP/Hadoop+Java+Versions">Hadoop Wiki</a> for known good versions.</li>
              <li>Download a stable version of Hadoop from Apache mirrors.</li>
            </ul>
          </section>
          <section>
            <h2><a name="Installation"></a>Installation</h2>
            <p>Installing a Hadoop cluster typically involves unpacking the software on all the machines in the cluster or installing it via a packaging system as appropriate for your operating system. It is important to divide up the hardware into functions.</p>
            <p>Typically one machine in the cluster is designated as the NameNode and another machine as the ResourceManager, exclusively. These are the masters. Other services (such as Web App Proxy Server and MapReduce Job History server) are usually run either on dedicated hardware or on shared infrastructure, depending upon the load.</p>
            <p>The rest of the machines in the cluster act as both DataNode and NodeManager. These are the workers.</p>
          </section>
          <section>
            <h2><a name="Configuring_Hadoop_in_Non-Secure_Mode"></a>Configuring Hadoop in Non-Secure Mode</h2>
            <p>Hadoop’s Java configuration is driven by two types of important configuration files:</p>
            <ul>
              <li>
                <p>Read-only default configuration - <code>core-default.xml</code>, <code>hdfs-default.xml</code>, <code>yarn-default.xml</code> and <code>mapred-default.xml</code>.</p>
              </li>
              <li>
                <p>Site-specific configuration - <code>etc/hadoop/core-site.xml</code>, <code>etc/hadoop/hdfs-site.xml</code>, <code>etc/hadoop/yarn-site.xml</code> and <code>etc/hadoop/mapred-site.xml</code>.</p>
              </li>
            </ul>
            <p>Additionally, you can control the Hadoop scripts found in the bin/ directory of the distribution, by setting site-specific values via the <code>etc/hadoop/hadoop-env.sh</code> and <code>etc/hadoop/yarn-env.sh</code>.</p>
            <p>To configure the Hadoop cluster you will need to configure the <code>environment</code> in which the Hadoop daemons execute as well as the <code>configuration parameters</code> for the Hadoop daemons.</p>
            <p>HDFS daemons are NameNode, SecondaryNameNode, and DataNode. YARN daemons are ResourceManager, NodeManager, and WebAppProxy. If MapReduce is to be used, then the MapReduce Job History Server will also be running. For large installations, these are generally running on separate hosts.</p>
            <section>
              <h3><a name="Configuring_Environment_of_Hadoop_Daemons"></a>Configuring Environment of Hadoop Daemons</h3>
              <p>Administrators should use the <code>etc/hadoop/hadoop-env.sh</code> and optionally the <code>etc/hadoop/mapred-env.sh</code> and <code>etc/hadoop/yarn-env.sh</code> scripts to do site-specific customization of the Hadoop daemons’ process environment.</p>
              <p>At the very least, you must specify the <code>JAVA_HOME</code> so that it is correctly defined on each remote node.</p>
              <p>Administrators can configure individual daemons using the configuration options shown below in the table:</p>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Daemon </th>
                    <th align="left"> Environment Variable </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> NameNode </td>
                    <td align="left"> HDFS_NAMENODE_OPTS </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> DataNode </td>
                    <td align="left"> HDFS_DATANODE_OPTS </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> Secondary NameNode </td>
                    <td align="left"> HDFS_SECONDARYNAMENODE_OPTS </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> ResourceManager </td>
                    <td align="left"> YARN_RESOURCEMANAGER_OPTS </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> NodeManager </td>
                    <td align="left"> YARN_NODEMANAGER_OPTS </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> WebAppProxy </td>
                    <td align="left"> YARN_PROXYSERVER_OPTS </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> Map Reduce Job History Server </td>
                    <td align="left"> MAPRED_HISTORYSERVER_OPTS </td>
                  </tr>
                </tbody>
              </table>
              <p>For example, To configure Namenode to use parallelGC and a 4GB Java Heap, the following statement should be added in hadoop-env.sh :</p>
              <div class="source">
                <div class="source">
                  <pre>  export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"
          </pre>
                </div>
              </div>
              <p>See <code>etc/hadoop/hadoop-env.sh</code> for other examples.</p>
              <p>Other useful configuration parameters that you can customize include:</p>
              <ul>
                <li><code>HADOOP_PID_DIR</code> - The directory where the daemons’ process id files are stored.</li>
                <li><code>HADOOP_LOG_DIR</code> - The directory where the daemons’ log files are stored. Log files are automatically created if they don’t exist.</li>
                <li><code>HADOOP_HEAPSIZE_MAX</code> - The maximum amount of memory to use for the Java heapsize. Units supported by the JVM are also supported here. If no unit is present, it will be assumed the number is in megabytes. By default, Hadoop will let the JVM determine how much to use. This value can be overriden on a per-daemon basis using the appropriate <code>_OPTS</code> variable listed above. For example, setting <code>HADOOP_HEAPSIZE_MAX=1g</code> and <code>HADOOP_NAMENODE_OPTS="-Xmx5g"</code> will configure the NameNode with 5GB heap.</li>
              </ul>
              <p>In most cases, you should specify the <code>HADOOP_PID_DIR</code> and <code>HADOOP_LOG_DIR</code> directories such that they can only be written to by the users that are going to run the hadoop daemons. Otherwise there is the potential for a symlink attack.</p>
              <p>It is also traditional to configure <code>HADOOP_HOME</code> in the system-wide shell environment configuration. For example, a simple script inside <code>/etc/profile.d</code>:</p>
              <div class="source">
                <div class="source">
                  <pre>  HADOOP_HOME=/path/to/hadoop
            export HADOOP_HOME
          </pre>
                </div>
              </div>
            </section>
            <section>
              <h3><a name="Configuring_the_Hadoop_Daemons"></a>Configuring the Hadoop Daemons</h3>
              <p>This section deals with important parameters to be specified in the given configuration files:</p>
              <ul>
                <li><code>etc/hadoop/core-site.xml</code></li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>fs.defaultFS</code> </td>
                    <td align="left"> NameNode URI </td>
                    <td align="left"> <a class="externalLink" href="hdfs://host:port/">hdfs://host:port/</a> </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>io.file.buffer.size</code> </td>
                    <td align="left"> 131072 </td>
                    <td align="left"> Size of read/write buffer used in SequenceFiles. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>
                  <p><code>etc/hadoop/hdfs-site.xml</code></p>
                </li>
                <li>
                  <p>Configurations for NameNode:</p>
                </li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>dfs.namenode.name.dir</code> </td>
                    <td align="left"> Path on the local filesystem where the NameNode stores the namespace and transactions logs persistently. </td>
                    <td align="left"> If this is a comma-delimited list of directories then the name table is replicated in all of the directories, for redundancy. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>dfs.hosts</code> / <code>dfs.hosts.exclude</code> </td>
                    <td align="left"> List of permitted/excluded DataNodes. </td>
                    <td align="left"> If necessary, use these files to control the list of allowable datanodes. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>dfs.blocksize</code> </td>
                    <td align="left"> 268435456 </td>
                    <td align="left"> HDFS blocksize of 256MB for large file-systems. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>dfs.namenode.handler.count</code> </td>
                    <td align="left"> 100 </td>
                    <td align="left"> More NameNode server threads to handle RPCs from large number of DataNodes. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>Configurations for DataNode:</li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>dfs.datanode.data.dir</code> </td>
                    <td align="left"> Comma separated list of paths on the local filesystem of a <code>DataNode</code> where it should store its blocks. </td>
                    <td align="left"> If this is a comma-delimited list of directories, then data will be stored in all named directories, typically on different devices. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>
                  <p><code>etc/hadoop/yarn-site.xml</code></p>
                </li>
                <li>
                  <p>Configurations for ResourceManager and NodeManager:</p>
                </li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>yarn.acl.enable</code> </td>
                    <td align="left"> <code>true</code> / <code>false</code> </td>
                    <td align="left"> Enable ACLs? Defaults to <i>false</i>. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.admin.acl</code> </td>
                    <td align="left"> Admin ACL </td>
                    <td align="left"> ACL to set admins on the cluster. ACLs are of for <i>comma-separated-usersspacecomma-separated-groups</i>. Defaults to special value of <b>*</b> which means <i>anyone</i>. Special value of just <i>space</i> means no one has access. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.log-aggregation-enable</code> </td>
                    <td align="left"> <i>false</i> </td>
                    <td align="left"> Configuration to enable or disable log aggregation </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>Configurations for ResourceManager:</li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>yarn.resourcemanager.address</code> </td>
                    <td align="left"> <code>ResourceManager</code> host:port for clients to submit jobs. </td>
                    <td align="left"> <i>host:port</i>&nbsp;If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.resourcemanager.scheduler.address</code> </td>
                    <td align="left"> <code>ResourceManager</code> host:port for ApplicationMasters to talk to Scheduler to obtain resources. </td>
                    <td align="left"> <i>host:port</i>&nbsp;If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.resourcemanager.resource-tracker.address</code> </td>
                    <td align="left"> <code>ResourceManager</code> host:port for NodeManagers. </td>
                    <td align="left"> <i>host:port</i>&nbsp;If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.resourcemanager.admin.address</code> </td>
                    <td align="left"> <code>ResourceManager</code> host:port for administrative commands. </td>
                    <td align="left"> <i>host:port</i>&nbsp;If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.resourcemanager.webapp.address</code> </td>
                    <td align="left"> <code>ResourceManager</code> web-ui host:port. </td>
                    <td align="left"> <i>host:port</i>&nbsp;If set, overrides the hostname set in <code>yarn.resourcemanager.hostname</code>. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.resourcemanager.hostname</code> </td>
                    <td align="left"> <code>ResourceManager</code> host. </td>
                    <td align="left"> <i>host</i>&nbsp;Single hostname that can be set in place of setting all <code>yarn.resourcemanager*address</code> resources. Results in default ports for ResourceManager components. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.resourcemanager.scheduler.class</code> </td>
                    <td align="left"> <code>ResourceManager</code> Scheduler class. </td>
                    <td align="left"> <code>CapacityScheduler</code> (recommended), <code>FairScheduler</code> (also recommended), or <code>FifoScheduler</code>. Use a fully qualified class name, e.g., <code>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler</code>. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.scheduler.minimum-allocation-mb</code> </td>
                    <td align="left"> Minimum limit of memory to allocate to each container request at the <code>Resource Manager</code>. </td>
                    <td align="left"> In MBs </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.scheduler.maximum-allocation-mb</code> </td>
                    <td align="left"> Maximum limit of memory to allocate to each container request at the <code>Resource Manager</code>. </td>
                    <td align="left"> In MBs </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.resourcemanager.nodes.include-path</code> / <code>yarn.resourcemanager.nodes.exclude-path</code> </td>
                    <td align="left"> List of permitted/excluded NodeManagers. </td>
                    <td align="left"> If necessary, use these files to control the list of allowable NodeManagers. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>Configurations for NodeManager:</li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>yarn.nodemanager.resource.memory-mb</code> </td>
                    <td align="left"> Resource i.e. available physical memory, in MB, for given <code>NodeManager</code> </td>
                    <td align="left"> Defines total available resources on the <code>NodeManager</code> to be made available to running containers </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.nodemanager.vmem-pmem-ratio</code> </td>
                    <td align="left"> Maximum ratio by which virtual memory usage of tasks may exceed physical memory </td>
                    <td align="left"> The virtual memory usage of each task may exceed its physical memory limit by this ratio. The total amount of virtual memory used by tasks on the NodeManager may exceed its physical memory usage by this ratio. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.nodemanager.local-dirs</code> </td>
                    <td align="left"> Comma-separated list of paths on the local filesystem where intermediate data is written. </td>
                    <td align="left"> Multiple paths help spread disk i/o. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.nodemanager.log-dirs</code> </td>
                    <td align="left"> Comma-separated list of paths on the local filesystem where logs are written. </td>
                    <td align="left"> Multiple paths help spread disk i/o. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.nodemanager.log.retain-seconds</code> </td>
                    <td align="left"> <i>10800</i> </td>
                    <td align="left"> Default time (in seconds) to retain log files on the NodeManager Only applicable if log-aggregation is disabled. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.nodemanager.remote-app-log-dir</code> </td>
                    <td align="left"> <i>/logs</i> </td>
                    <td align="left"> HDFS directory where the application logs are moved on application completion. Need to set appropriate permissions. Only applicable if log-aggregation is enabled. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.nodemanager.remote-app-log-dir-suffix</code> </td>
                    <td align="left"> <i>logs</i> </td>
                    <td align="left"> Suffix appended to the remote log dir. Logs will be aggregated to ${yarn.nodemanager.remote-app-log-dir}/${user}/${thisParam} Only applicable if log-aggregation is enabled. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.nodemanager.aux-services</code> </td>
                    <td align="left"> mapreduce_shuffle </td>
                    <td align="left"> Shuffle service that needs to be set for Map Reduce applications. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>yarn.nodemanager.env-whitelist</code> </td>
                    <td align="left"> Environment properties to be inherited by containers from NodeManagers </td>
                    <td align="left"> For mapreduce application in addition to the default values HADOOP_MAPRED_HOME should to be added. Property value should JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_HOME,PATH,LANG,TZ,HADOOP_MAPRED_HOME </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>Configurations for History Server (Needs to be moved elsewhere):</li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>yarn.log-aggregation.retain-seconds</code> </td>
                    <td align="left"> <i>-1</i> </td>
                    <td align="left"> How long to keep aggregation logs before deleting them. -1 disables. Be careful, set this too small and you will spam the name node. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>yarn.log-aggregation.retain-check-interval-seconds</code> </td>
                    <td align="left"> <i>-1</i> </td>
                    <td align="left"> Time between checks for aggregated log retention. If set to 0 or a negative value then the value is computed as one-tenth of the aggregated log retention time. Be careful, set this too small and you will spam the name node. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>
                  <p><code>etc/hadoop/mapred-site.xml</code></p>
                </li>
                <li>
                  <p>Configurations for MapReduce Applications:</p>
                </li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.framework.name</code> </td>
                    <td align="left"> yarn </td>
                    <td align="left"> Execution framework set to Hadoop YARN. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.map.memory.mb</code> </td>
                    <td align="left"> 1536 </td>
                    <td align="left"> Larger resource limit for maps. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.map.java.opts</code> </td>
                    <td align="left"> -Xmx1024M </td>
                    <td align="left"> Larger heap-size for child jvms of maps. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.reduce.memory.mb</code> </td>
                    <td align="left"> 3072 </td>
                    <td align="left"> Larger resource limit for reduces. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.reduce.java.opts</code> </td>
                    <td align="left"> -Xmx2560M </td>
                    <td align="left"> Larger heap-size for child jvms of reduces. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.task.io.sort.mb</code> </td>
                    <td align="left"> 512 </td>
                    <td align="left"> Higher memory-limit while sorting data for efficiency. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.task.io.sort.factor</code> </td>
                    <td align="left"> 100 </td>
                    <td align="left"> More streams merged at once while sorting files. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.reduce.shuffle.parallelcopies</code> </td>
                    <td align="left"> 50 </td>
                    <td align="left"> Higher number of parallel copies run by reduces to fetch outputs from very large number of maps. </td>
                  </tr>
                </tbody>
              </table>
              <ul>
                <li>Configurations for MapReduce JobHistory Server:</li>
              </ul>
              <table border="0" class="bodyTable">
                <thead>
                  <tr class="a">
                    <th align="left"> Parameter </th>
                    <th align="left"> Value </th>
                    <th align="left"> Notes </th>
                  </tr>
                </thead>
                <tbody>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.jobhistory.address</code> </td>
                    <td align="left"> MapReduce JobHistory Server <i>host:port</i> </td>
                    <td align="left"> Default port is 10020. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.jobhistory.webapp.address</code> </td>
                    <td align="left"> MapReduce JobHistory Server Web UI <i>host:port</i> </td>
                    <td align="left"> Default port is 19888. </td>
                  </tr>
                  <tr class="b">
                    <td align="left"> <code>mapreduce.jobhistory.intermediate-done-dir</code> </td>
                    <td align="left"> /mr-history/tmp </td>
                    <td align="left"> Directory where history files are written by MapReduce jobs. </td>
                  </tr>
                  <tr class="a">
                    <td align="left"> <code>mapreduce.jobhistory.done-dir</code> </td>
                    <td align="left"> /mr-history/done </td>
                    <td align="left"> Directory where history files are managed by the MR JobHistory Server. </td>
                  </tr>
                </tbody>
              </table>
            </section>
          </section>
          <section>
            <h2><a name="Monitoring_Health_of_NodeManagers"></a>Monitoring Health of NodeManagers</h2>
            <p>Hadoop provides a mechanism by which administrators can configure the NodeManager to run an administrator supplied script periodically to determine if a node is healthy or not.</p>
            <p>Administrators can determine if the node is in a healthy state by performing any checks of their choice in the script. If the script detects the node to be in an unhealthy state, it must print a line to standard output beginning with the string ERROR. The NodeManager spawns the script periodically and checks its output. If the script’s output contains the string ERROR, as described above, the node’s status is reported as <code>unhealthy</code> and the node is black-listed by the ResourceManager. No further tasks will be assigned to this node. However, the NodeManager continues to run the script, so that if the node becomes healthy again, it will be removed from the blacklisted nodes on the ResourceManager automatically. The node’s health along with the output of the script, if it is unhealthy, is available to the administrator in the ResourceManager web interface. The time since the node was healthy is also displayed on the web interface.</p>
            <p>The following parameters can be used to control the node health monitoring script in <code>etc/hadoop/yarn-site.xml</code>.</p>
            <table border="0" class="bodyTable">
              <thead>
                <tr class="a">
                  <th align="left"> Parameter </th>
                  <th align="left"> Value </th>
                  <th align="left"> Notes </th>
                </tr>
              </thead>
              <tbody>
                <tr class="b">
                  <td align="left"> <code>yarn.nodemanager.health-checker.script.path</code> </td>
                  <td align="left"> Node health script </td>
                  <td align="left"> Script to check for node’s health status. </td>
                </tr>
                <tr class="a">
                  <td align="left"> <code>yarn.nodemanager.health-checker.script.opts</code> </td>
                  <td align="left"> Node health script options </td>
                  <td align="left"> Options for script to check for node’s health status. </td>
                </tr>
                <tr class="b">
                  <td align="left"> <code>yarn.nodemanager.health-checker.interval-ms</code> </td>
                  <td align="left"> Node health script interval </td>
                  <td align="left"> Time interval for running health script. </td>
                </tr>
                <tr class="a">
                  <td align="left"> <code>yarn.nodemanager.health-checker.script.timeout-ms</code> </td>
                  <td align="left"> Node health script timeout interval </td>
                  <td align="left"> Timeout for health script execution. </td>
                </tr>
              </tbody>
            </table>
            <p>The health checker script is not supposed to give ERROR if only some of the local disks become bad. NodeManager has the ability to periodically check the health of the local disks (specifically checks nodemanager-local-dirs and nodemanager-log-dirs) and after reaching the threshold of number of bad directories based on the value set for the config property yarn.nodemanager.disk-health-checker.min-healthy-disks, the whole node is marked unhealthy and this info is sent to resource manager also. The boot disk is either raided or a failure in the boot disk is identified by the health checker script.</p>
          </section>
          <section>
            <h2><a name="Slaves_File"></a>Slaves File</h2>
            <p>List all worker hostnames or IP addresses in your <code>etc/hadoop/workers</code> file, one per line. Helper scripts (described below) will use the <code>etc/hadoop/workers</code> file to run commands on many hosts at once. It is not used for any of the Java-based Hadoop configuration. In order to use this functionality, ssh trusts (via either passphraseless ssh or some other means, such as Kerberos) must be established for the accounts used to run Hadoop.</p>
          </section>
          <section>
            <h2><a name="Hadoop_Rack_Awareness"></a>Hadoop Rack Awareness</h2>
            <p>Many Hadoop components are rack-aware and take advantage of the network topology for performance and safety. Hadoop daemons obtain the rack information of the workers in the cluster by invoking an administrator configured module. See the <a href="./RackAwareness.html">Rack Awareness</a> documentation for more specific information.</p>
            <p>It is highly recommended configuring rack awareness prior to starting HDFS.</p>
          </section>
          <section>
            <h2><a name="Logging"></a>Logging</h2>
            <p>Hadoop uses the <a class="externalLink" href="http://logging.apache.org/log4j/2.x/">Apache log4j</a> via the Apache Commons Logging framework for logging. Edit the <code>etc/hadoop/log4j.properties</code> file to customize the Hadoop daemons’ logging configuration (log-formats and so on).</p>
          </section>
          <section>
            <h2><a name="Operating_the_Hadoop_Cluster"></a>Operating the Hadoop Cluster</h2>
            <p>Once all the necessary configuration is complete, distribute the files to the <code>HADOOP_CONF_DIR</code> directory on all the machines. This should be the same directory on all machines.</p>
            <p>In general, it is recommended that HDFS and YARN run as separate users. In the majority of installations, HDFS processes execute as ‘hdfs’. YARN is typically using the ‘yarn’ account.</p>
            <section>
              <h3><a name="Hadoop_Startup"></a>Hadoop Startup</h3>
              <p>To start a Hadoop cluster you will need to start both the HDFS and YARN cluster.</p>
              <p>The first time you bring up HDFS, it must be formatted. Format a new distributed filesystem as <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/bin/hdfs namenode -format
          </pre>
                </div>
              </div>
              <p>Start the HDFS NameNode with the following command on the designated node as <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start namenode
          </pre>
                </div>
              </div>
              <p>Start a HDFS DataNode with the following command on each designated node as <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon start datanode
          </pre>
                </div>
              </div>
              <p>If <code>etc/hadoop/workers</code> and ssh trusted access is configured (see <a href="./SingleCluster.html">Single Node Setup</a>), all of the HDFS processes can be started with a utility script. As <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/sbin/start-dfs.sh
          </pre>
                </div>
              </div>
              <p>Start the YARN with the following command, run on the designated ResourceManager as <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn --daemon start resourcemanager
          </pre>
                </div>
              </div>
              <p>Run a script to start a NodeManager on each designated host as <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn --daemon start nodemanager
          </pre>
                </div>
              </div>
              <p>Start a standalone WebAppProxy server. Run on the WebAppProxy server as <i>yarn</i>. If multiple servers are used with load balancing it should be run on each of them:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn --daemon start proxyserver
          </pre>
                </div>
              </div>
              <p>If <code>etc/hadoop/workers</code> and ssh trusted access is configured (see <a href="./SingleCluster.html">Single Node Setup</a>), all of the YARN processes can be started with a utility script. As <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/sbin/start-yarn.sh
          </pre>
                </div>
              </div>
              <p>Start the MapReduce JobHistory Server with the following command, run on the designated server as <i>mapred</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[mapred]$ $HADOOP_HOME/bin/mapred --daemon start historyserver
          </pre>
                </div>
              </div>
            </section>
            <section>
              <h3><a name="Hadoop_Shutdown"></a>Hadoop Shutdown</h3>
              <p>Stop the NameNode with the following command, run on the designated NameNode as <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop namenode
          </pre>
                </div>
              </div>
              <p>Run a script to stop a DataNode as <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/bin/hdfs --daemon stop datanode
          </pre>
                </div>
              </div>
              <p>If <code>etc/hadoop/workers</code> and ssh trusted access is configured (see <a href="./SingleCluster.html">Single Node Setup</a>), all of the HDFS processes may be stopped with a utility script. As <i>hdfs</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[hdfs]$ $HADOOP_HOME/sbin/stop-dfs.sh
          </pre>
                </div>
              </div>
              <p>Stop the ResourceManager with the following command, run on the designated ResourceManager as <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn --daemon stop resourcemanager
          </pre>
                </div>
              </div>
              <p>Run a script to stop a NodeManager on a worker as <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn --daemon stop nodemanager
          </pre>
                </div>
              </div>
              <p>If <code>etc/hadoop/workers</code> and ssh trusted access is configured (see <a href="./SingleCluster.html">Single Node Setup</a>), all of the YARN processes can be stopped with a utility script. As <i>yarn</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/sbin/stop-yarn.sh
          </pre>
                </div>
              </div>
              <p>Stop the WebAppProxy server. Run on the WebAppProxy server as <i>yarn</i>. If multiple servers are used with load balancing it should be run on each of them:</p>
              <div class="source">
                <div class="source">
                  <pre>[yarn]$ $HADOOP_HOME/bin/yarn stop proxyserver
          </pre>
                </div>
              </div>
              <p>Stop the MapReduce JobHistory Server with the following command, run on the designated server as <i>mapred</i>:</p>
              <div class="source">
                <div class="source">
                  <pre>[mapred]$ $HADOOP_HOME/bin/mapred --daemon stop historyserver
          </pre>
                </div>
              </div>
            </section>
          </section>
          <section>
            <h2><a name="Web_Interfaces"></a>Web Interfaces</h2>
            <p>Once the Hadoop cluster is up and running check the web-ui of the components as described below:</p>
            <table border="0" class="bodyTable">
              <thead>
                <tr class="a">
                  <th align="left"> Daemon </th>
                  <th align="left"> Web Interface </th>
                  <th align="left"> Notes </th>
                </tr>
              </thead>
              <tbody>
                <tr class="b">
                  <td align="left"> NameNode </td>
                  <td align="left"> <a class="externalLink" href="http://nn_host:port/">http://nn_host:port/</a> </td>
                  <td align="left"> Default HTTP port is 9870. </td>
                </tr>
                <tr class="a">
                  <td align="left"> ResourceManager </td>
                  <td align="left"> <a class="externalLink" href="http://rm_host:port/">http://rm_host:port/</a> </td>
                  <td align="left"> Default HTTP port is 8088. </td>
                </tr>
                <tr class="b">
                  <td align="left"> MapReduce JobHistory Server </td>
                  <td align="left"> <a class="externalLink" href="http://jhs_host:port/">http://jhs_host:port/</a> </td>
                  <td align="left"> Default HTTP port is 19888. </td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <div class="container mt-5">
              <div class="mt-3 d-flex flex-row align-items-center flex-wrap">
                <span class="d-flex align-items-center gap-2 dropdown">
                  <img src="/api/flex/medias/obj-2705" class="avatar bg-light-subtle border cursor dropdown-toggle" title="Raymond" alt="Raymond" data-bs-toggle="dropdown">
                  <span class="">Raymond</span>
                  <div class="dropdown-menu shadow">
                    <div class="dropdown-item text-center"><img src="/api/flex/medias/obj-2705" class="avatar avatar-md bg-light-subtles cursor dropdown-toggle" title="Raymond" alt="Raymond" data-bs-toggle="dropdown"></div>
                    <span class="dropdown-header text-center fw-bold">Raymond</span>
                    <div class="dropdown-divider"></div>
                    <div class="dropdown-item"><i class="md me-1">article</i><small class="me-1">Articles</small> <small class="ms-auto fw-bold">578</small></div>
                    <div class="dropdown-item"><i class="md me-1">image</i><small class="me-1">Diagrams</small> <small class="ms-auto fw-bold">58</small></div>
                    <div class="dropdown-item"><i class="md me-1">comment</i><small class="me-1">Comments</small> <small class="ms-auto fw-bold">303</small></div>
                    <div class="dropdown-item"><i class="md me-1">loyalty</i><small class="me-1">Kontext Points</small> <small class="ms-auto fw-bold">6373</small></div>
                  </div>
                </span>
                <span class="mx-2 text-muted">|</span> <a class="mx-1 text-body" href="/column/hadoop">
                <em class="kontext-em">Hadoop, Hive &amp; HBase</em>
                </a>
                <button type="button" data-sid="43" data-btn-label="Close" data-title="Subscribe" data-site-title="Subscribe to column: <b>Hadoop, Hive &amp; HBase</b>. <br /> You will be notified when new articles are published." data-toggle="subscribe" title="Subscribe" class="btn btn-outline-primary ms-1 align-items-center" onclick="showSiteSubModal(this)">
                <i class="md me-1">send</i>Subscribe
                </button>
              </div>
              <h1 class="my-3 py-3">Default Ports Used by Hadoop Services (HDFS, MapReduce, YARN)</h1>
              <div class="text-muted flex-wrap d-flex justify-content-around my-1 align-items-center mb-3 border-top border-bottom py-2">
                <span class="text-muted d-none d-sm-inline" title="Creation date"><i class="md md-24">event</i> 2018-04-29</span>
                <!--like button-->
                <span class="like-button justify-content-center align-items-center text-center" data-bs-toggle="modal" data-bs-target="#loginModal">
                <i class="md md-24 text-primary" title="Like this article?" data-bs-toggle="tooltip">thumb_up</i>
                <span id="likes" class="ms-1 ms-sm-0 fw-bold text-primary">0</span>
                </span>
                <span>
                <i class="md md-24">visibility</i> <span id="viewCount" data-update-url="/api/flex/contents/265/views">36,263</span>
                </span>
                <a href="#comments" data-bs-toggle="tooltip" title="View comments">
                <i class="md md-24 me-1">comment</i> 2
                </a>
                <a data-bs-toggle="offcanvas" href="#statsCanvas" role="button" aria-controls="statsCanvas" title="Stats"><i class="md md-24">insights</i></a>
                <a data-bs-toggle="offcanvas" href="#offcanvasToc" role="button" aria-controls="offcanvasToc" title="Table of contents"><i class="md md-24">toc</i></a>
                <!--sharing button-->
                <div class="dropdown">
                  <a class="dropdown-toggle btn btn-link" href="#" data-bs-toggle="dropdown" aria-expanded="true">
                  <i class="md md-24">ios_share</i>
                  </a>
                  <div class="dropdown-menu dropdown-menu-end shadow">
                    <a class="dropdown-item" target="_blank" title="Share this on Twitter" href="https://twitter.com/intent/tweet?text=Default%20Ports%20Used%20by%20Hadoop%20Services%20(HDFS,%20MapReduce,%20YARN)&amp;url=https://kontext.tech/article/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">
                      <svg xmlns="http://www.w3.org/2000/svg" height="16" width="16" viewBox="0 0 512 512">
                        <path d="M389.2 48h70.6L305.6 224.2 487 464H345L233.7 318.6 106.5 464H35.8L200.7 275.5 26.8 48H172.4L272.9 180.9 389.2 48zM364.4 421.8h39.1L151.1 88h-42L364.4 421.8z"></path>
                      </svg>
                      <span>Twitter</span>
                    </a>
                    <a class="dropdown-item" target="_blank" title="Share this on Facebook" href="http://www.facebook.com/sharer/sharer.php?u=https://kontext.tech/article/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">
                      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-facebook" viewBox="0 0 16 16">
                        <path d="M16 8.049c0-4.446-3.582-8.05-8-8.05C3.58 0-.002 3.603-.002 8.05c0 4.017 2.926 7.347 6.75 7.951v-5.625h-2.03V8.05H6.75V6.275c0-2.017 1.195-3.131 3.022-3.131.876 0 1.791.157 1.791.157v1.98h-1.009c-.993 0-1.303.621-1.303 1.258v1.51h2.218l-.354 2.326H9.25V16c3.824-.604 6.75-3.934 6.75-7.951z"></path>
                      </svg>
                      <span>Facebook</span>
                    </a>
                    <a class="dropdown-item" target="_blank" title="Share this on LinkedIn" href="http://www.linkedin.com/shareArticle?mini=true&amp;url=https://kontext.tech/article/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn&amp;title=Default%20Ports%20Used%20by%20Hadoop%20Services%20(HDFS,%20MapReduce,%20YARN)&amp;summary=This%20page%20summarizes%20the%20default%20ports%20used%20by%20Hadoop%20services.%20It%20is%20useful%20when%20configuring%20network%20interfaces%20in%20a%20cluster.%20The%20secondary%20namenode%20http%2Fhttps%20server%20address%20and%20port.%20%20%0D%0A%20%0D%0A%20%0D%0A%20Service%20%0D%0A%20Servers%20%0D%0A%20Default%20Ports%20Used%20%20Protocol%20%20Configuration%20Parameter%20%20Comments%20%0D%0A%20%0D%0A%20%0D%0A%20%20WebUI%20...&amp;source=https://kontext.tech/article/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">
                      <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-linkedin" viewBox="0 0 16 16">
                        <path d="M0 1.146C0 .513.526 0 1.175 0h13.65C15.474 0 16 .513 16 1.146v13.708c0 .633-.526 1.146-1.175 1.146H1.175C.526 16 0 15.487 0 14.854V1.146zm4.943 12.248V6.169H2.542v7.225h2.401zm-1.2-8.212c.837 0 1.358-.554 1.358-1.248-.015-.709-.52-1.248-1.342-1.248-.822 0-1.359.54-1.359 1.248 0 .694.521 1.248 1.327 1.248h.016zm4.908 8.212V9.359c0-.216.016-.432.08-.586.173-.431.568-.878 1.232-.878.869 0 1.216.662 1.216 1.634v3.865h2.401V9.25c0-2.22-1.184-3.252-2.764-3.252-1.274 0-1.845.7-2.165 1.193v.025h-.016a5.54 5.54 0 0 1 .016-.025V6.169h-2.4c.03.678 0 7.225 0 7.225h2.4z"></path>
                      </svg>
                      <span>LinkedIn</span>
                    </a>
                  </div>
                </div>
                <!--Options-->
                <div>
                  <i class="md md-24 text-muted">more_vert</i>
                </div>
              </div>
              <div class="offcanvas offcanvas-start" tabindex="-1" id="statsCanvas" aria-labelledby="statsCanvasLabel">
                <div class="offcanvas-header">
                  <h5 class="offcanvas-title" id="statsCanvasLabel"><i class="md md-24 me-1 text-muted">insights</i>  Stats</h5>
                  <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                  <div class="alert alert-warning" role="alert">
                    <i class="md md-24">warning</i> Please login first to view stats information.
                  </div>
                </div>
              </div>
              <!-- table of content -->
              <div class="offcanvas offcanvas-start" data-bs-scroll="true" data-bs-backdrop="false" tabindex="-1" id="offcanvasToc" aria-labelledby="offcanvasTocLabel">
                <div class="offcanvas-header">
                  <h5 class="offcanvas-title" id="offcanvasTocLabel"><i class="md md-24 me-1 text-muted">toc</i> Table of contents</h5>
                  <button type="button" class="btn-close" data-bs-dismiss="offcanvas" aria-label="Close"></button>
                </div>
                <div class="offcanvas-body">
                  <div id="tocContainer" data-toc="#tocNav" data-bs-target="#content" data-headings="h1,h2,h3,h4">
                    <ul class="toc">
                      <li class="toc-item level-h3"><a href="#h-hadoop-3-1-0">Hadoop 3.1.0</a></li>
                      <li class="toc-item level-h4"><a href="#h-hdfs">HDFS</a></li>
                      <li class="toc-item level-h4"><a href="#h-mapreduce">MapReduce</a></li>
                      <li class="toc-item level-h4"><a href="#h-yarn">YARN</a></li>
                      <li class="toc-item level-h4"><a href="#h-references">References</a></li>
                    </ul>
                  </div>
                </div>
              </div>
              <div class="mt-5 content col-lg-7 mx-auto">
                <p>This page summarizes the default ports used by Hadoop services. It is useful when configuring network interfaces in a cluster.</p>
                <h3 id="h-hadoop-3-1-0">Hadoop 3.1.0</h3>
                <h4 id="h-hdfs">HDFS</h4>
                The secondary namenode http/https server address and port.
                <div class="table-responsive">
                  <table class="table" border="1" cellspacing="0" cellpadding="2">
                    <tbody>
                      <tr>
                        <td valign="top">Service</td>
                        <td valign="top">Servers</td>
                        <td valign="top">Default Ports Used</td>
                        <td valign="top">Protocol</td>
                        <td valign="top">Configuration Parameter</td>
                        <td valign="top">Comments</td>
                      </tr>
                      <tr>
                        <td valign="top">
                          <p>WebUI for NameNode</p>
                        </td>
                        <td valign="top">Master (incl. back-up NameNodes)</td>
                        <td valign="top">9870/9871</td>
                        <td valign="top">http/https</td>
                        <td valign="top">
                          <p><a name="dfs.namenode.http-address">dfs.namenode.http-address</a></p>
                          <p><a name="dfs.namenode.https-address">dfs.namenode.https-address</a></p>
                        </td>
                        <td valign="top">
                          <p>The address and the base port where the dfs namenode web ui will listen on.</p>
                          <p>The namenode secure http server address and port. </p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">Metadata service (NameNode)</td>
                        <td valign="top">Master (incl. back-up NameNodes)</td>
                        <td valign="top"></td>
                        <td valign="top">IPC</td>
                        <td valign="top">
                          <p><a name="fs.defaultFS">fs.defaultFS</a></p>
                        </td>
                        <td valign="top">
                          <p>The name of the default file system. </p>
                          <p>For example,</p>
                          <p>hdfs://hdp-master:19000</p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">Data Node</td>
                        <td valign="top">All slave nodes</td>
                        <td valign="top">9864/9865</td>
                        <td valign="top">http/https</td>
                        <td valign="top">
                          <p>dfs.datanode.http.address</p>
                          <p><a name="dfs.datanode.https.address">dfs.datanode.https.address</a></p>
                        </td>
                        <td valign="top">
                          <p>The secondary namenode http/https server address and port.</p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">Data Node</td>
                        <td valign="top">All slave nodes</td>
                        <td valign="top">9866</td>
                        <td valign="top"></td>
                        <td valign="top"><a name="dfs.datanode.address">dfs.datanode.address</a></td>
                        <td valign="top">
                          <p>The datanode server address and port for data transfer. </p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">Data Node</td>
                        <td valign="top">All slave nodes</td>
                        <td valign="top">9867</td>
                        <td valign="top">IPC</td>
                        <td valign="top"><a name="dfs.datanode.ipc.address">dfs.datanode.ipc.address</a></td>
                        <td valign="top">
                          <p>The datanode ipc server address and port (for metadata operations). </p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">Secondary NameNode</td>
                        <td valign="top">Secondary NameNode and any backup NameNodes</td>
                        <td valign="top">9868/9869</td>
                        <td valign="top">http/https</td>
                        <td valign="top">
                          <p><a name="dfs.namenode.secondary.http-address">dfs.namenode.secondary.http-address</a></p>
                          <p><a name="dfs.namenode.secondary.https-address">fs.namenode.secondary.https-address</a></p>
                        </td>
                        <td valign="top"></td>
                      </tr>
                      <tr>
                        <td valign="top">JournalNode</td>
                        <td valign="top"></td>
                        <td valign="top">8485</td>
                        <td valign="top">IPC</td>
                        <td valign="top"><a name="dfs.journalnode.rpc-address">dfs.journalnode.rpc-address</a></td>
                        <td valign="top">The JournalNode RPC server address and port.</td>
                      </tr>
                      <tr>
                        <td valign="top">JournalNode</td>
                        <td valign="top"></td>
                        <td valign="top">8480/8481</td>
                        <td valign="top">http/https</td>
                        <td valign="top">
                          <p><a name="dfs.journalnode.http-address">dfs.journalnode.http-address</a></p>
                          <p><a name="dfs.journalnode.https-address">dfs.journalnode.https-address</a></p>
                        </td>
                        <td valign="top">The address and port the JournalNode http/https server listens on.</td>
                      </tr>
                      <tr>
                        <td valign="top"><span>Aliasmap Server</span><br><br></td>
                        <td valign="top">NameNode </td>
                        <td valign="top">50200<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="dfs.provided.aliasmap.inmemory.dnrpc-address">dfs.provided.aliasmap.inmemory.dnrpc-address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address where the aliasmap server will be running
                          </span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <h4 id="h-mapreduce">MapReduce</h4>
                <div class="table-responsive">
                  <table class="table" border="1" cellspacing="0" cellpadding="2">
                    <tbody>
                      <tr>
                        <td valign="top">Service</td>
                        <td valign="top">Servers</td>
                        <td valign="top">Default Ports Used</td>
                        <td valign="top">Protocol</td>
                        <td valign="top">Configuration Parameter</td>
                        <td valign="top">Comments</td>
                      </tr>
                      <tr>
                        <td valign="top">MapReduce Job History</td>
                        <td valign="top"></td>
                        <td valign="top">10020</td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="mapreduce.jobhistory.address">mapreduce.jobhistory.address</a></td>
                        <td valign="top"><span>MapReduce JobHistory Server IPC host:port</span></td>
                      </tr>
                      <tr>
                        <td valign="top">MapReduce Job History UI</td>
                        <td valign="top"></td>
                        <td valign="top"><span>19888/19890</span></td>
                        <td valign="top">http/https<br></td>
                        <td valign="top">
                          <p><a name="mapreduce.jobhistory.webapp.address">mapreduce.jobhistory.webapp.address</a><br></p>
                          <p><a name="mapreduce.jobhistory.webapp.https.address">mapreduce.jobhistory.webapp.https.address</a></p>
                        </td>
                        <td valign="top">
                          <p><span>M<span>apReduce JobHistory Server Web UI URL (<span>http/https</span>)</span></span></p>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top">History server admin<br></td>
                        <td valign="top"><br></td>
                        <td valign="top">10033<br></td>
                        <td valign="top">IPC<br></td>
                        <td valign="top"><a name="mapreduce.jobhistory.admin.address">mapreduce.jobhistory.admin.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the History server admin interface.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <p><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                <h4 id="h-yarn">YARN</h4>
                <div class="table-responsive">
                  <table class="table" border="1" cellspacing="0" cellpadding="2">
                    <tbody>
                      <tr>
                        <td valign="top">Service</td>
                        <td valign="top">Servers</td>
                        <td valign="top">Default Ports Used</td>
                        <td valign="top">Protocol</td>
                        <td valign="top">Configuration Parameter</td>
                        <td valign="top">Comments</td>
                      </tr>
                      <tr>
                        <td valign="top"></td>
                        <td valign="top"><br></td>
                        <td valign="top">8032</td>
                        <td valign="top">IPC</td>
                        <td valign="top"><a name="yarn.resourcemanager.address">yarn.resourcemanager.address</a></td>
                        <td valign="top"><span>The address of the applications manager interface in the RM.</span></td>
                      </tr>
                      <tr>
                        <td valign="top"></td>
                        <td valign="top"></td>
                        <td valign="top">8030</td>
                        <td valign="top">IPC</td>
                        <td valign="top"><a name="yarn.resourcemanager.scheduler.address">yarn.resourcemanager.scheduler.address</a></td>
                        <td valign="top"><span>The address of the scheduler interface.</span></td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8088/8090<br></td>
                        <td valign="top">http/https<br></td>
                        <td valign="top">
                          <p><a name="yarn.resourcemanager.webapp.address">yarn.resourcemanager.webapp.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                          <p><a name="yarn.resourcemanager.webapp.https.address">yarn.resourcemanager.webapp.https.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                        </td>
                        <td valign="top"><span>The http/https address of the RM web application.</span><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8031<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.resourcemanager.resource-tracker.address">yarn.resourcemanager.resource-tracker.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8033<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.resourcemanager.admin.address">yarn.resourcemanager.admin.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the RM admin interface.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">0<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.nodemanager.address">yarn.nodemanager.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the container manager in the NM.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8040<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.nodemanager.localizer.address">yarn.nodemanager.localizer.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>Address where the localizer IPC is.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8048<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.nodemanager.collector-service.address">yarn.nodemanager.collector-service.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>Address where the collector service IPC is.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8042/8044<br></td>
                        <td valign="top">http/https<br></td>
                        <td valign="top">
                          <p><a name="yarn.nodemanager.webapp.address">yarn.nodemanager.webapp.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                          <p><a name="yarn.nodemanager.webapp.https.address">yarn.nodemanager.webapp.https.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                        </td>
                        <td valign="top"><span>The http/https address of the NM web application.
                          </span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><span>10200</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><a name="yarn.timeline-service.address">yarn.timeline-service.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>This is default address for the timeline server to start the
                          RPC server.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8188/8190<br></td>
                        <td valign="top"><br></td>
                        <td valign="top">
                          <p><a name="yarn.timeline-service.webapp.address">yarn.timeline-service.webapp.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                          <p><a name="yarn.timeline-service.webapp.https.address">yarn.timeline-service.webapp.https.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                        </td>
                        <td valign="top"><span>The http/https address of the timeline service web application.</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8047<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.sharedcache.admin.address">yarn.sharedcache.admin.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the admin interface in the SCM (shared cache manager)</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8788<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.sharedcache.webapp.address">yarn.sharedcache.webapp.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the web application in the SCM (shared cache manager)</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8046<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.sharedcache.uploader.server.address">yarn.sharedcache.uploader.server.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the node manager interface in the SCM
                          (shared cache manager)</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8045<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.sharedcache.client-server.address">yarn.sharedcache.client-server.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the client interface in the SCM
                          (shared cache manager)</span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8049<br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><a name="yarn.nodemanager.amrmproxy.address">yarn.nodemanager.amrmproxy.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><span>The address of the AMRMProxyService listener.
                          </span><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br>
                        </td>
                      </tr>
                      <tr>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top">8089/8091<br></td>
                        <td valign="top"><br></td>
                        <td valign="top">
                          <p><a name="yarn.router.webapp.address">yarn.router.webapp.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                          <p><a name=" yarn.router.webapp.https.address">yarn.router.webapp.https.address</a><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></p>
                        </td>
                        <td valign="top"><span>The http/https address of the Router web application. </span><br></td>
                      </tr>
                      <tr>
                        <td valign="top"><b></b><i></i><u></u><sub></sub><sup></sup><strike></strike><br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                        <td valign="top"><br></td>
                      </tr>
                    </tbody>
                  </table>
                </div>
                <h4 id="h-references">References</h4>
                <p>The following links provide information about all the default configurations for Hadoop v3.1.0.</p>
                <ul>
                  <li><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-common/core-default.xml" rel="nofollow">core-default.xml</a></li>
                  <li><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-project-dist/hadoop-hdfs/hdfs-default.xml" rel="nofollow">hdfs-default.xml</a></li>
                  <li><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-mapreduce-client/hadoop-mapreduce-client-core/mapred-default.xml" rel="nofollow">mapred-default.xml</a></li>
                  <li><a href="http://hadoop.apache.org/docs/r3.1.0/hadoop-yarn/hadoop-yarn-common/yarn-default.xml" rel="nofollow">yarn-default.xml</a></li>
                </ul>
              </div>
              <!-- similar content -->
              <div class="list-group similar-items mt-3 col-lg-7 mx-auto" data-url="/api/flex/contents/265/related" id="relatedList" data-toggle="related">
                <div class="list-group-item py-3">
                  <span class="h6">More from Kontext</span>
                </div>
                <a title="Configure YARN and MapReduce Resources in Hadoop Cluster" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/267/configure-yarn-and-mapreduce-resources-in-hadoop-cluster">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">Configure YARN and MapReduce Resources in Hadoop Cluster</text>
                  </span>
                </a>
                <a title="How to Kill Running Jobs in Hadoop" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/323/list-and-kill-hadoop-jobs">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">How to Kill Running Jobs in Hadoop</text>
                  </span>
                </a>
                <a title="Check HDFS folder size in Shell / Hadoop" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/329/check-hdfs-file-folder-size-shell">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">Check HDFS folder size in Shell / Hadoop</text>
                  </span>
                </a>
                <a title="Ingest Data into Hadoop HDFS through Jupyter Notebook" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/388/ingest-data-into-hadoop-hdfs-through-jupyter-notebook">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">Ingest Data into Hadoop HDFS through Jupyter Notebook</text>
                  </span>
                </a>
                <a title="List Hadoop running jobs" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/330/list-hadoop-running-jobs-shell">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">List Hadoop running jobs</text>
                  </span>
                </a>
                <a title="Load File into HDFS through WebHDFS APIs" class="list-group-item list-group-item-action text-body d-inline-block w-100 text-truncate small" href="/article/460/load-file-into-hdfs-through-webhdfs-apis">
                  <span>
                    <i class="md md-18 text-muted">article</i>
                    <text class="ms-1">Load File into HDFS through WebHDFS APIs</text>
                  </span>
                </a>
              </div>
              <div class="d-flex flex-wrap my-3 col-lg-7 mx-auto">
                <a class="tag btn btn-sm btn-outline-secondary rounded-pill me-1 my-1" href="/tag/hadoop" rel="tag">hadoop</a>
                <a class="tag btn btn-sm btn-outline-secondary rounded-pill me-1 my-1" href="/tag/yarn" rel="tag">yarn</a>
                <a class="tag btn btn-sm btn-outline-secondary rounded-pill me-1 my-1" href="/tag/hdfs" rel="tag">hdfs</a>
              </div>
              <div class="my-3 col-lg-7 mx-auto">
                <div class="alert alert-light d-flex align-items-center flex-wrap" role="alert">
                  <span class="flex-grow-1"><i class="md">info</i> Last modified by Raymond 2 years ago </span>
                  <!-- Legal -->
                  <span> <i class="md md-24">copyright</i>
                  This page is subject to <a href="https://kontext.tech/terms" target="_blank" rel="license">Site terms</a>.</span>
                </div>
              </div>
              <!--Application setting starts - PageContentBottom-->
              <!--Application setting ends - PageContentBottom-->
              <div class="col-lg-7 mx-auto">
                <div class="my-3 h6" id="comments"><i class="md md-24">comment</i> Comments</div>
                <div class="mb-3 shadow-sm">
                  <div id="comment1766" class="card comment-list-item mb-3   bg-body">
                    <div class="card-header bg-transparent d-flex align-items-center border-bottom-0">
                      <span class="me-auto">
                        <span class="d-flex align-items-center gap-2 dropdown">
                          <img src="/api/flex/medias/obj-2705" class="avatar bg-light-subtle border cursor dropdown-toggle" title="Raymond" alt="Raymond" data-bs-toggle="dropdown">
                          <span class="d-none">Raymond</span>
                          <div class="dropdown-menu shadow">
                            <div class="dropdown-item text-center"><img src="/api/flex/medias/obj-2705" class="avatar avatar-md bg-light-subtles cursor dropdown-toggle" title="Raymond" alt="Raymond" data-bs-toggle="dropdown"></div>
                            <span class="dropdown-header text-center fw-bold">Raymond</span>
                            <div class="dropdown-divider"></div>
                            <div class="dropdown-item"><i class="md me-1">article</i><small class="me-1">Articles</small> <small class="ms-auto fw-bold">578</small></div>
                            <div class="dropdown-item"><i class="md me-1">image</i><small class="me-1">Diagrams</small> <small class="ms-auto fw-bold">58</small></div>
                            <div class="dropdown-item"><i class="md me-1">comment</i><small class="me-1">Comments</small> <small class="ms-auto fw-bold">303</small></div>
                            <div class="dropdown-item"><i class="md me-1">loyalty</i><small class="me-1">Kontext Points</small> <small class="ms-auto fw-bold">6373</small></div>
                          </div>
                        </span>
                      </span>
                      <small class="text-muted mx-1">#1766</small>
                      <small class="text-muted mx-1"><i class="md md-24">access_time</i>  2 years ago </small>
                      <i class="md md-24 text-muted">more_vert</i>
                    </div>
                    <div class="card-body">
                      <div class="d-flex flex-column">
                        <div class="nofollow flex-grow-1">
                          <div class="comment p-3 ">
                            <div class="comment-body " id="comment-body-1766">
                              <p>Thanks for pointing this out and you are right. I've updated them accordingly.</p>
                            </div>
                          </div>
                          <i class="md md-36 text-primary">format_quote</i>
                          <div class="card m-3">
                            <div class="card-body font-italic">
                              <div class="comment bg-light-subtle rounded-3 p-3 ">
                                <p class="text-muted small"><i class="md">person</i> geek <i class="md">access_time</i> 2 years ago</p>
                                <div class="card-text small comment-body " id="comment-body-1766-1765">
                                  <p>8032 8030 10033 are ipc port , not http port</p>
                                </div>
                              </div>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="card-footer bg-transparent border-top-0 d-flex justify-content-end">
                      <a href="#" class="disabled text-muted"><i class="md md-24">reply</i> Reply</a>
                    </div>
                  </div>
                </div>
                <div class="mb-3 shadow-sm">
                  <div id="comment1765" class="card comment-list-item mb-3   bg-body">
                    <div class="card-header bg-transparent d-flex align-items-center border-bottom-0">
                      <span class="me-auto">
                        <span class="d-flex align-items-center gap-2 dropdown">
                          <span class="btn btn-circle btn-outline-secondary dropdown-toggle" data-bs-toggle="dropdown">
                          <span>G</span>
                          </span>
                          <span class="d-none">geek youth</span>
                          <div class="dropdown-menu shadow">
                            <span class="dropdown-header text-center fw-bold">geek youth</span>
                            <div class="dropdown-divider"></div>
                            <div class="dropdown-item"><i class="md me-1">article</i><small class="me-1">Articles</small> <small class="ms-auto fw-bold">0</small></div>
                            <div class="dropdown-item"><i class="md me-1">image</i><small class="me-1">Diagrams</small> <small class="ms-auto fw-bold">0</small></div>
                            <div class="dropdown-item"><i class="md me-1">comment</i><small class="me-1">Comments</small> <small class="ms-auto fw-bold">1</small></div>
                            <div class="dropdown-item"><i class="md me-1">loyalty</i><small class="me-1">Kontext Points</small> <small class="ms-auto fw-bold">1</small></div>
                          </div>
                        </span>
                      </span>
                      <small class="text-muted mx-1">#1765</small>
                      <small class="text-muted mx-1"><i class="md md-24">access_time</i>  2 years ago </small>
                      <i class="md md-24 text-muted">more_vert</i>
                    </div>
                    <div class="card-body">
                      <div class="d-flex flex-column">
                        <div class="nofollow flex-grow-1">
                          <div class="comment p-3 ">
                            <div class="comment-body " id="comment-body-1765">
                              <p>8032 8030 10033 are ipc port , not http port</p>
                            </div>
                          </div>
                        </div>
                      </div>
                    </div>
                    <div class="card-footer bg-transparent border-top-0 d-flex justify-content-end">
                      <a href="#" class="disabled text-muted"><i class="md md-24">reply</i> Reply</a>
                    </div>
                  </div>
                </div>
                <!--Comments-->
                <div class="my-3">
                  <div class="card shadow-sm my-3">
                    <div class="card-body">
                      <div class="row">
                        <div class="col-md">
                          <p><span>Please log in or register to comment.</span></p>
                          <a class="btn btn-outline-primary" href="/login?rurl=%2Farticle%2F265%2Fdefault-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">
                          <i class="md md-24">account_circle</i>&nbsp;Log in
                          </a>
                          <a class="btn btn-outline-secondary" href="/account/sign-up?rurl=%2Farticle%2F265%2Fdefault-ports-used-by-hadoop-services-hdfs-mapreduce-yarn">
                          <i class="md md-24">person_add</i>&nbsp;Register
                          </a>
                        </div>
                        <div class="col-md border-left-md">
                          <h4>Log in with external accounts</h4>
                          <form id="external-account" method="post" class="form-horizontal" action="/account/external-login?rurl=%2Farticle%2F265%2Fdefault-ports-used-by-hadoop-services-hdfs-mapreduce-yarn" novalidate="true">
                            <div class="p-1">
                              <button type="submit" class="btn btn-outline-primary w-100" name="provider" value="Microsoft" title="Log in using your Microsoft account">
                                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-google" viewBox="0 0 21 21">
                                  <path fill="#f35325" d="M0 0h10v10H0z"></path>
                                  <path fill="#81bc06" d="M11 0h10v10H11z"></path>
                                  <path fill="#05a6f0" d="M0 11h10v10H0z"></path>
                                  <path fill="#ffba08" d="M11 11h10v10H11z"></path>
                                </svg>
                                Log in with Microsoft account
                              </button>
                            </div>
                            <div class="p-1">
                              <button type="submit" class="btn btn-outline-danger w-100" name="provider" value="Google" title="Log in using your Google account">
                                <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-google" viewBox="0 0 16 16">
                                  <path d="M15.545 6.558a9.42 9.42 0 0 1 .139 1.626c0 2.434-.87 4.492-2.384 5.885h.002C11.978 15.292 10.158 16 8 16A8 8 0 1 1 8 0a7.689 7.689 0 0 1 5.352 2.082l-2.284 2.284A4.347 4.347 0 0 0 8 3.166c-2.087 0-3.86 1.408-4.492 3.304a4.792 4.792 0 0 0 0 3.063h.003c.635 1.893 2.405 3.301 4.492 3.301 1.078 0 2.004-.276 2.722-.764h-.003a3.702 3.702 0 0 0 1.599-2.431H8v-3.08h7.545z"></path>
                                </svg>
                                Log in with Google account
                              </button>
                            </div>
                            <input name="__RequestVerificationToken" type="hidden" value="CfDJ8MV_R_8gCe9KjnzAK9MON1_-XXkbMxjVyj2Dbn5d87gT9Llxjyu5blbKD6jMLuWLnnexS9yalSf36B5Py5NjdFeUiQZRxzSMj3gjJb40mgGyFgaexaE7UgyvdQHxA0Dp9L-ncsDPO_IqqA-tuYiGh1s">
                          </form>
                        </div>
                      </div>
                    </div>
                  </div>
                </div>
              </div>
            </div>
          </section>
          <section>
            <table border="1">
              <tbody>
                <tr>
                  <td>name</td>
                  <td>value</td>
                  <td>description</td>
                </tr>
                <tr>
                  <td><a name="hadoop.hdfs.configuration.version">hadoop.hdfs.configuration.version</a></td>
                  <td>1</td>
                  <td>version of this configuration file</td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.rpc-address">dfs.namenode.rpc-address</a></td>
                  <td></td>
                  <td>
                    RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist,
                    the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1
                    dfs.namenode.rpc-address.EXAMPLENAMESERVICE
                    The value of this property will take the form of nn-host1:rpc-port. The NameNode's default RPC port is 9820.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.rpc-bind-host">dfs.namenode.rpc-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the RPC server will bind to. If this optional address is
                    set, it overrides only the hostname portion of dfs.namenode.rpc-address.
                    It can also be specified per name node or name service for HA/Federation.
                    This is useful for making the name node listen on all interfaces by
                    setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.servicerpc-address">dfs.namenode.servicerpc-address</a></td>
                  <td></td>
                  <td>
                    RPC address for HDFS Services communication. BackupNode, Datanodes and all other services should be
                    connecting to this address if it is configured. In the case of HA/Federation where multiple namenodes exist,
                    the name service id is added to the name e.g. dfs.namenode.servicerpc-address.ns1
                    dfs.namenode.rpc-address.EXAMPLENAMESERVICE
                    The value of this property will take the form of nn-host1:rpc-port.
                    If the value of this property is unset the value of dfs.namenode.rpc-address will be used as the default.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.servicerpc-bind-host">dfs.namenode.servicerpc-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the service RPC server will bind to. If this optional address is
                    set, it overrides only the hostname portion of dfs.namenode.servicerpc-address.
                    It can also be specified per name node or name service for HA/Federation.
                    This is useful for making the name node listen on all interfaces by
                    setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lifeline.rpc-address">dfs.namenode.lifeline.rpc-address</a></td>
                  <td></td>
                  <td>
                    NameNode RPC lifeline address.  This is an optional separate RPC address
                    that can be used to isolate health checks and liveness to protect against
                    resource exhaustion in the main RPC handler pool.  In the case of
                    HA/Federation where multiple NameNodes exist, the name service ID is added
                    to the name e.g. dfs.namenode.lifeline.rpc-address.ns1.  The value of this
                    property will take the form of nn-host1:rpc-port.  If this property is not
                    defined, then the NameNode will not start a lifeline RPC server.  By
                    default, the property is not defined.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lifeline.rpc-bind-host">dfs.namenode.lifeline.rpc-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the lifeline RPC server will bind to.  If this optional
                    address is set, it overrides only the hostname portion of
                    dfs.namenode.lifeline.rpc-address.  It can also be specified per name node
                    or name service for HA/Federation.  This is useful for making the name node
                    listen on all interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.secondary.http-address">dfs.namenode.secondary.http-address</a></td>
                  <td>0.0.0.0:9868</td>
                  <td>
                    The secondary namenode http server address and port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.secondary.https-address">dfs.namenode.secondary.https-address</a></td>
                  <td>0.0.0.0:9869</td>
                  <td>
                    The secondary namenode HTTPS server address and port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.address">dfs.datanode.address</a></td>
                  <td>0.0.0.0:9866</td>
                  <td>
                    The datanode server address and port for data transfer.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.http.address">dfs.datanode.http.address</a></td>
                  <td>0.0.0.0:9864</td>
                  <td>
                    The datanode http server address and port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ipc.address">dfs.datanode.ipc.address</a></td>
                  <td>0.0.0.0:9867</td>
                  <td>
                    The datanode ipc server address and port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.handler.count">dfs.datanode.handler.count</a></td>
                  <td>10</td>
                  <td>The number of server threads for the datanode.</td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.http-address">dfs.namenode.http-address</a></td>
                  <td>0.0.0.0:9870</td>
                  <td>
                    The address and the base port where the dfs namenode web ui will listen on.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.http-bind-host">dfs.namenode.http-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the HTTP server will bind to. If this optional address
                    is set, it overrides only the hostname portion of dfs.namenode.http-address.
                    It can also be specified per name node or name service for HA/Federation.
                    This is useful for making the name node HTTP server listen on all
                    interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.heartbeat.recheck-interval">dfs.namenode.heartbeat.recheck-interval</a></td>
                  <td>300000</td>
                  <td>
                    This time decides the interval to check for expired datanodes.
                    With this value and dfs.heartbeat.interval, the interval of
                    deciding the datanode is stale or not is also calculated.
                    The unit of this configuration is millisecond.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.policy">dfs.http.policy</a></td>
                  <td>HTTP_ONLY</td>
                  <td>Decide if HTTPS(SSL) is supported on HDFS
                    This configures the HTTP endpoint for HDFS daemons:
                    The following values are supported:
                    - HTTP_ONLY : Service is provided only on http
                    - HTTPS_ONLY : Service is provided only on https
                    - HTTP_AND_HTTPS : Service is provided both on http and https
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.https.need-auth">dfs.client.https.need-auth</a></td>
                  <td>false</td>
                  <td>Whether SSL client certificate authentication is required
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.cached.conn.retry">dfs.client.cached.conn.retry</a></td>
                  <td>3</td>
                  <td>The number of times the HDFS client will pull a socket from the
                    cache.  Once this number is exceeded, the client will try to create a new
                    socket.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.https.server.keystore.resource">dfs.https.server.keystore.resource</a></td>
                  <td>ssl-server.xml</td>
                  <td>Resource file from which ssl server keystore
                    information will be extracted
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.https.keystore.resource">dfs.client.https.keystore.resource</a></td>
                  <td>ssl-client.xml</td>
                  <td>Resource file from which ssl client keystore
                    information will be extracted
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.https.address">dfs.datanode.https.address</a></td>
                  <td>0.0.0.0:9865</td>
                  <td>The datanode secure http server address and port.</td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.https-address">dfs.namenode.https-address</a></td>
                  <td>0.0.0.0:9871</td>
                  <td>The namenode secure http server address and port.</td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.https-bind-host">dfs.namenode.https-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the HTTPS server will bind to. If this optional address
                    is set, it overrides only the hostname portion of dfs.namenode.https-address.
                    It can also be specified per name node or name service for HA/Federation.
                    This is useful for making the name node HTTPS server listen on all
                    interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.dns.interface">dfs.datanode.dns.interface</a></td>
                  <td>default</td>
                  <td>
                    The name of the Network Interface from which a data node should
                    report its IP address. e.g. eth2. This setting may be required for some
                    multi-homed nodes where the DataNodes are assigned multiple hostnames
                    and it is desirable for the DataNodes to use a non-default hostname.
                    Prefer using hadoop.security.dns.interface over
                    dfs.datanode.dns.interface.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.dns.nameserver">dfs.datanode.dns.nameserver</a></td>
                  <td>default</td>
                  <td>
                    The host name or IP address of the name server (DNS) which a DataNode
                    should use to determine its own host name.
                    Prefer using hadoop.security.dns.nameserver over
                    dfs.datanode.dns.nameserver.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.backup.address">dfs.namenode.backup.address</a></td>
                  <td>0.0.0.0:50100</td>
                  <td>
                    The backup node server address and port.
                    If the port is 0 then the server will start on a free port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.backup.http-address">dfs.namenode.backup.http-address</a></td>
                  <td>0.0.0.0:50105</td>
                  <td>
                    The backup node http server address and port.
                    If the port is 0 then the server will start on a free port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.redundancy.considerLoad">dfs.namenode.redundancy.considerLoad</a></td>
                  <td>true</td>
                  <td>Decide if chooseTarget considers the target's load or not
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.redundancy.considerLoad.factor">dfs.namenode.redundancy.considerLoad.factor</a></td>
                  <td>2.0</td>
                  <td>The factor by which a node's load can exceed the average
                    before being rejected for writes, only if considerLoad is true.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.default.chunk.view.size">dfs.default.chunk.view.size</a></td>
                  <td>32768</td>
                  <td>The number of bytes to view for a file on the browser.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.du.reserved">dfs.datanode.du.reserved</a></td>
                  <td>0</td>
                  <td>Reserved space in bytes per volume. Always leave this much space free for non dfs use.
                    Specific storage type based reservation is also supported. The property can be followed with
                    corresponding storage types ([ssd]/[disk]/[archive]/[ram_disk]) for cluster with heterogeneous storage.
                    For example, reserved space for RAM_DISK storage can be configured using property
                    'dfs.datanode.du.reserved.ram_disk'. If specific storage type reservation is not configured
                    then dfs.datanode.du.reserved will be used.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.name.dir">dfs.namenode.name.dir</a></td>
                  <td>file://${hadoop.tmp.dir}/dfs/name</td>
                  <td>Determines where on the local filesystem the DFS name node
                    should store the name table(fsimage).  If this is a comma-delimited list
                    of directories then the name table is replicated in all of the
                    directories, for redundancy.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.name.dir.restore">dfs.namenode.name.dir.restore</a></td>
                  <td>false</td>
                  <td>Set to true to enable NameNode to attempt recovering a
                    previously failed dfs.namenode.name.dir. When enabled, a recovery of any
                    failed directory is attempted during checkpoint.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.max-component-length">dfs.namenode.fs-limits.max-component-length</a></td>
                  <td>255</td>
                  <td>Defines the maximum number of bytes in UTF-8 encoding in each
                    component of a path.  A value of 0 will disable the check.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.max-directory-items">dfs.namenode.fs-limits.max-directory-items</a></td>
                  <td>1048576</td>
                  <td>Defines the maximum number of items that a directory may
                    contain. Cannot set the property to a value less than 1 or more than
                    6400000.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.min-block-size">dfs.namenode.fs-limits.min-block-size</a></td>
                  <td>1048576</td>
                  <td>Minimum block size in bytes, enforced by the Namenode at create
                    time. This prevents the accidental creation of files with tiny block
                    sizes (and thus many blocks), which can degrade
                    performance.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.max-blocks-per-file">dfs.namenode.fs-limits.max-blocks-per-file</a></td>
                  <td>10000</td>
                  <td>Maximum number of blocks per file, enforced by the Namenode on
                    write. This prevents the creation of extremely large files which can
                    degrade performance.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.dir">dfs.namenode.edits.dir</a></td>
                  <td>${dfs.namenode.name.dir}</td>
                  <td>Determines where on the local filesystem the DFS name node
                    should store the transaction (edits) file. If this is a comma-delimited list
                    of directories then the transaction file is replicated in all of the
                    directories, for redundancy. Default value is same as dfs.namenode.name.dir
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.dir.required">dfs.namenode.edits.dir.required</a></td>
                  <td></td>
                  <td>This should be a subset of dfs.namenode.edits.dir,
                    to ensure that the transaction (edits) file
                    in these places is always up-to-date.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.shared.edits.dir">dfs.namenode.shared.edits.dir</a></td>
                  <td></td>
                  <td>A directory on shared storage between the multiple namenodes
                    in an HA cluster. This directory will be written by the active and read
                    by the standby in order to keep the namespaces synchronized. This directory
                    does not need to be listed in dfs.namenode.edits.dir above. It should be
                    left empty in a non-HA cluster.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.journal-plugin.qjournal">dfs.namenode.edits.journal-plugin.qjournal</a></td>
                  <td>org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager</td>
                  <td></td>
                </tr>
                <tr>
                  <td><a name="dfs.permissions.enabled">dfs.permissions.enabled</a></td>
                  <td>true</td>
                  <td>
                    If "true", enable permission checking in HDFS.
                    If "false", permission checking is turned off,
                    but all other behavior is unchanged.
                    Switching from one parameter value to the other does not change the mode,
                    owner or group of files or directories.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.permissions.superusergroup">dfs.permissions.superusergroup</a></td>
                  <td>supergroup</td>
                  <td>The name of the group of super-users.
                    The value should be a single group name.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.cluster.administrators">dfs.cluster.administrators</a></td>
                  <td></td>
                  <td>ACL for the admins, this configuration is used to control
                    who can access the default servlets in the namenode, etc. The value
                    should be a comma separated list of users and groups. The user list
                    comes first and is separated by a space followed by the group list,
                    e.g. "user1,user2 group1,group2". Both users and groups are optional,
                    so "user1", " group1", "", "user1 group1", "user1,user2 group1,group2"
                    are all valid (note the leading space in " group1"). '*' grants access
                    to all users and groups, e.g. '*', '* ' and ' *' are all valid.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.acls.enabled">dfs.namenode.acls.enabled</a></td>
                  <td>false</td>
                  <td>
                    Set to true to enable support for HDFS ACLs (Access Control Lists).  By
                    default, ACLs are disabled.  When ACLs are disabled, the NameNode rejects
                    all RPCs related to setting or getting ACLs.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.posix.acl.inheritance.enabled">dfs.namenode.posix.acl.inheritance.enabled</a></td>
                  <td>true</td>
                  <td>
                    Set to true to enable POSIX style ACL inheritance. When it is enabled
                    and the create request comes from a compatible client, the NameNode
                    will apply default ACLs from the parent directory to the create mode
                    and ignore the client umask. If no default ACL found, it will apply the
                    client umask.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lazypersist.file.scrub.interval.sec">dfs.namenode.lazypersist.file.scrub.interval.sec</a></td>
                  <td>300</td>
                  <td>
                    The NameNode periodically scans the namespace for LazyPersist files with
                    missing blocks and unlinks them from the namespace. This configuration key
                    controls the interval between successive scans. Set it to a negative value
                    to disable this behavior.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.access.token.enable">dfs.block.access.token.enable</a></td>
                  <td>false</td>
                  <td>
                    If "true", access tokens are used as capabilities for accessing datanodes.
                    If "false", no access tokens are checked on accessing datanodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.access.key.update.interval">dfs.block.access.key.update.interval</a></td>
                  <td>600</td>
                  <td>
                    Interval in minutes at which namenode updates its access keys.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.access.token.lifetime">dfs.block.access.token.lifetime</a></td>
                  <td>600</td>
                  <td>The lifetime of access tokens in minutes.</td>
                </tr>
                <tr>
                  <td><a name="dfs.block.access.token.protobuf.enable">dfs.block.access.token.protobuf.enable</a></td>
                  <td>false</td>
                  <td>
                    If "true", block tokens are written using Protocol Buffers.
                    If "false", block tokens are written using Legacy format.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.data.dir">dfs.datanode.data.dir</a></td>
                  <td>file://${hadoop.tmp.dir}/dfs/data</td>
                  <td>Determines where on the local filesystem an DFS data node
                    should store its blocks.  If this is a comma-delimited
                    list of directories, then data will be stored in all named
                    directories, typically on different devices. The directories should be tagged
                    with corresponding storage types ([SSD]/[DISK]/[ARCHIVE]/[RAM_DISK]) for HDFS
                    storage policies. The default storage type will be DISK if the directory does
                    not have a storage type tagged explicitly. Directories that do not exist will
                    be created if local filesystem permission allows.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.data.dir.perm">dfs.datanode.data.dir.perm</a></td>
                  <td>700</td>
                  <td>Permissions for the directories on on the local filesystem where
                    the DFS data node store its blocks. The permissions can either be octal or
                    symbolic.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.replication">dfs.replication</a></td>
                  <td>3</td>
                  <td>Default block replication.
                    The actual number of replications can be specified when the file is created.
                    The default is used if replication is not specified in create time.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.replication.max">dfs.replication.max</a></td>
                  <td>512</td>
                  <td>Maximal block replication.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.replication.min">dfs.namenode.replication.min</a></td>
                  <td>1</td>
                  <td>Minimal block replication.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.maintenance.replication.min">dfs.namenode.maintenance.replication.min</a></td>
                  <td>1</td>
                  <td>Minimal live block replication in existence of maintenance mode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.safemode.replication.min">dfs.namenode.safemode.replication.min</a></td>
                  <td></td>
                  <td>
                    a separate minimum replication factor for calculating safe block count.
                    This is an expert level setting.
                    Setting this lower than the dfs.namenode.replication.min
                    is not recommend and/or dangerous for production setups.
                    When it's not set it takes value from dfs.namenode.replication.min
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.blocksize">dfs.blocksize</a></td>
                  <td>134217728</td>
                  <td>
                    The default block size for new files, in bytes.
                    You can use the following suffix (case insensitive):
                    k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
                    Or provide complete size in bytes (such as 134217728 for 128 MB).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.retries">dfs.client.block.write.retries</a></td>
                  <td>3</td>
                  <td>The number of retries for writing blocks to the data nodes,
                    before we signal failure to the application.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.replace-datanode-on-failure.enable">dfs.client.block.write.replace-datanode-on-failure.enable</a></td>
                  <td>true</td>
                  <td>
                    If there is a datanode/network failure in the write pipeline,
                    DFSClient will try to remove the failed datanode from the pipeline
                    and then continue writing with the remaining datanodes. As a result,
                    the number of datanodes in the pipeline is decreased.  The feature is
                    to add new datanodes to the pipeline.
                    This is a site-wide property to enable/disable the feature.
                    When the cluster size is extremely small, e.g. 3 nodes or less, cluster
                    administrators may want to set the policy to NEVER in the default
                    configuration file or disable this feature.  Otherwise, users may
                    experience an unusually high rate of pipeline failures since it is
                    impossible to find new datanodes for replacement.
                    See also dfs.client.block.write.replace-datanode-on-failure.policy
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.replace-datanode-on-failure.policy">dfs.client.block.write.replace-datanode-on-failure.policy</a></td>
                  <td>DEFAULT</td>
                  <td>
                    This property is used only if the value of
                    dfs.client.block.write.replace-datanode-on-failure.enable is true.
                    ALWAYS: always add a new datanode when an existing datanode is removed.
                    NEVER: never add a new datanode.
                    DEFAULT:
                    Let r be the replication number.
                    Let n be the number of existing datanodes.
                    Add a new datanode only if r is greater than or equal to 3 and either
                    (1) floor(r/2) is greater than or equal to n; or
                    (2) r is greater than n and the block is hflushed/appended.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.replace-datanode-on-failure.best-effort">dfs.client.block.write.replace-datanode-on-failure.best-effort</a></td>
                  <td>false</td>
                  <td>
                    This property is used only if the value of
                    dfs.client.block.write.replace-datanode-on-failure.enable is true.
                    Best effort means that the client will try to replace a failed datanode
                    in write pipeline (provided that the policy is satisfied), however, it
                    continues the write operation in case that the datanode replacement also
                    fails.
                    Suppose the datanode replacement fails.
                    false: An exception should be thrown so that the write will fail.
                    true : The write should be resumed with the remaining datandoes.
                    Note that setting this property to true allows writing to a pipeline
                    with a smaller number of datanodes.  As a result, it increases the
                    probability of data loss.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.replace-datanode-on-failure.min-replication">dfs.client.block.write.replace-datanode-on-failure.min-replication</a></td>
                  <td>0</td>
                  <td>
                    The minimum number of replications that are needed to not to fail
                    the write pipeline if new datanodes can not be found to replace
                    failed datanodes (could be due to network failure) in the write pipeline.
                    If the number of the remaining datanodes in the write pipeline is greater
                    than or equal to this property value, continue writing to the remaining nodes.
                    Otherwise throw exception.
                    If this is set to 0, an exception will be thrown, when a replacement
                    can not be found.
                    See also dfs.client.block.write.replace-datanode-on-failure.policy
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.blockreport.intervalMsec">dfs.blockreport.intervalMsec</a></td>
                  <td>21600000</td>
                  <td>Determines block reporting interval in milliseconds.</td>
                </tr>
                <tr>
                  <td><a name="dfs.blockreport.initialDelay">dfs.blockreport.initialDelay</a></td>
                  <td>0s</td>
                  <td>
                    Delay for first block report in seconds. Support multiple time unit
                    suffix(case insensitive), as described in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.blockreport.split.threshold">dfs.blockreport.split.threshold</a></td>
                  <td>1000000</td>
                  <td>If the number of blocks on the DataNode is below this
                    threshold then it will send block reports for all Storage Directories
                    in a single message.
                    If the number of blocks exceeds this threshold then the DataNode will
                    send block reports for each Storage Directory in separate messages.
                    Set to zero to always split.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max.full.block.report.leases">dfs.namenode.max.full.block.report.leases</a></td>
                  <td>6</td>
                  <td>The maximum number of leases for full block reports that the
                    NameNode will issue at any given time.  This prevents the NameNode from
                    being flooded with full block reports that use up all the RPC handler
                    threads.  This number should never be more than the number of RPC handler
                    threads or less than 1.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.full.block.report.lease.length.ms">dfs.namenode.full.block.report.lease.length.ms</a></td>
                  <td>300000</td>
                  <td>
                    The number of milliseconds that the NameNode will wait before invalidating
                    a full block report lease.  This prevents a crashed DataNode from
                    permanently using up a full block report lease.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.directoryscan.interval">dfs.datanode.directoryscan.interval</a></td>
                  <td>21600s</td>
                  <td>Interval in seconds for Datanode to scan data directories and
                    reconcile the difference between blocks in memory and on the disk.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.directoryscan.threads">dfs.datanode.directoryscan.threads</a></td>
                  <td>1</td>
                  <td>How many threads should the threadpool used to compile reports
                    for volumes in parallel have.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.directoryscan.throttle.limit.ms.per.sec">dfs.datanode.directoryscan.throttle.limit.ms.per.sec</a></td>
                  <td>1000</td>
                  <td>The report compilation threads are limited to only running for
                    a given number of milliseconds per second, as configured by the
                    property. The limit is taken per thread, not in aggregate, e.g. setting
                    a limit of 100ms for 4 compiler threads will result in each thread being
                    limited to 100ms, not 25ms.
                    Note that the throttle does not interrupt the report compiler threads, so the
                    actual running time of the threads per second will typically be somewhat
                    higher than the throttle limit, usually by no more than 20%.
                    Setting this limit to 1000 disables compiler thread throttling. Only
                    values between 1 and 1000 are valid. Setting an invalid value will result
                    in the throttle being disabled and an error message being logged. 1000 is
                    the default setting.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.heartbeat.interval">dfs.heartbeat.interval</a></td>
                  <td>3s</td>
                  <td>
                    Determines datanode heartbeat interval in seconds.
                    Can use the following suffix (case insensitive):
                    ms(millis), s(sec), m(min), h(hour), d(day)
                    to specify the time (such as 2s, 2m, 1h, etc.).
                    Or provide complete number in seconds (such as 30 for 30 seconds).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.lifeline.interval.seconds">dfs.datanode.lifeline.interval.seconds</a></td>
                  <td></td>
                  <td>
                    Sets the interval in seconds between sending DataNode Lifeline Protocol
                    messages from the DataNode to the NameNode.  The value must be greater than
                    the value of dfs.heartbeat.interval.  If this property is not defined, then
                    the default behavior is to calculate the interval as 3x the value of
                    dfs.heartbeat.interval.  Note that normal heartbeat processing may cause the
                    DataNode to postpone sending lifeline messages if they are not required.
                    Under normal operations with speedy heartbeat processing, it is possible
                    that no lifeline messages will need to be sent at all.  This property has no
                    effect if dfs.namenode.lifeline.rpc-address is not defined.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.handler.count">dfs.namenode.handler.count</a></td>
                  <td>10</td>
                  <td>The number of Namenode RPC server threads that listen to
                    requests from clients.
                    If dfs.namenode.servicerpc-address is not configured then
                    Namenode RPC server threads listen to requests from all nodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.service.handler.count">dfs.namenode.service.handler.count</a></td>
                  <td>10</td>
                  <td>The number of Namenode RPC server threads that listen to
                    requests from DataNodes and from all other non-client nodes.
                    dfs.namenode.service.handler.count will be valid only if
                    dfs.namenode.servicerpc-address is configured.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lifeline.handler.ratio">dfs.namenode.lifeline.handler.ratio</a></td>
                  <td>0.10</td>
                  <td>
                    A ratio applied to the value of dfs.namenode.handler.count, which then
                    provides the number of RPC server threads the NameNode runs for handling the
                    lifeline RPC server.  For example, if dfs.namenode.handler.count is 100, and
                    dfs.namenode.lifeline.handler.factor is 0.10, then the NameNode starts
                    100 * 0.10 = 10 threads for handling the lifeline RPC server.  It is common
                    to tune the value of dfs.namenode.handler.count as a function of the number
                    of DataNodes in a cluster.  Using this property allows for the lifeline RPC
                    server handler threads to be tuned automatically without needing to touch a
                    separate property.  Lifeline message processing is lightweight, so it is
                    expected to require many fewer threads than the main NameNode RPC server.
                    This property is not used if dfs.namenode.lifeline.handler.count is defined,
                    which sets an absolute thread count.  This property has no effect if
                    dfs.namenode.lifeline.rpc-address is not defined.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lifeline.handler.count">dfs.namenode.lifeline.handler.count</a></td>
                  <td></td>
                  <td>
                    Sets an absolute number of RPC server threads the NameNode runs for handling
                    the DataNode Lifeline Protocol and HA health check requests from ZKFC.  If
                    this property is defined, then it overrides the behavior of
                    dfs.namenode.lifeline.handler.ratio.  By default, it is not defined.  This
                    property has no effect if dfs.namenode.lifeline.rpc-address is not defined.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.safemode.threshold-pct">dfs.namenode.safemode.threshold-pct</a></td>
                  <td>0.999f</td>
                  <td>
                    Specifies the percentage of blocks that should satisfy
                    the minimal replication requirement defined by dfs.namenode.replication.min.
                    Values less than or equal to 0 mean not to wait for any particular
                    percentage of blocks before exiting safemode.
                    Values greater than 1 will make safe mode permanent.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.safemode.min.datanodes">dfs.namenode.safemode.min.datanodes</a></td>
                  <td>0</td>
                  <td>
                    Specifies the number of datanodes that must be considered alive
                    before the name node exits safemode.
                    Values less than or equal to 0 mean not to take the number of live
                    datanodes into account when deciding whether to remain in safe mode
                    during startup.
                    Values greater than the number of datanodes in the cluster
                    will make safe mode permanent.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.safemode.extension">dfs.namenode.safemode.extension</a></td>
                  <td>30000</td>
                  <td>
                    Determines extension of safe mode in milliseconds after the threshold level
                    is reached.  Support multiple time unit suffix (case insensitive), as
                    described in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.resource.check.interval">dfs.namenode.resource.check.interval</a></td>
                  <td>5000</td>
                  <td>
                    The interval in milliseconds at which the NameNode resource checker runs.
                    The checker calculates the number of the NameNode storage volumes whose
                    available spaces are more than dfs.namenode.resource.du.reserved, and
                    enters safemode if the number becomes lower than the minimum value
                    specified by dfs.namenode.resource.checked.volumes.minimum.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.resource.du.reserved">dfs.namenode.resource.du.reserved</a></td>
                  <td>104857600</td>
                  <td>
                    The amount of space to reserve/require for a NameNode storage directory
                    in bytes. The default is 100MB.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.resource.checked.volumes">dfs.namenode.resource.checked.volumes</a></td>
                  <td></td>
                  <td>
                    A list of local directories for the NameNode resource checker to check in
                    addition to the local edits directories.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.resource.checked.volumes.minimum">dfs.namenode.resource.checked.volumes.minimum</a></td>
                  <td>1</td>
                  <td>
                    The minimum number of redundant NameNode storage volumes required.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.balance.bandwidthPerSec">dfs.datanode.balance.bandwidthPerSec</a></td>
                  <td>10m</td>
                  <td>
                    Specifies the maximum amount of bandwidth that each datanode
                    can utilize for the balancing purpose in term of
                    the number of bytes per second. You can use the following
                    suffix (case insensitive):
                    k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa)to specify the size
                    (such as 128k, 512m, 1g, etc.).
                    Or provide complete size in bytes (such as 134217728 for 128 MB).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.hosts">dfs.hosts</a></td>
                  <td></td>
                  <td>Names a file that contains a list of hosts that are
                    permitted to connect to the namenode. The full pathname of the file
                    must be specified.  If the value is empty, all hosts are
                    permitted.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.hosts.exclude">dfs.hosts.exclude</a></td>
                  <td></td>
                  <td>Names a file that contains a list of hosts that are
                    not permitted to connect to the namenode.  The full pathname of the
                    file must be specified.  If the value is empty, no hosts are
                    excluded.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max.objects">dfs.namenode.max.objects</a></td>
                  <td>0</td>
                  <td>The maximum number of files, directories and blocks
                    dfs supports. A value of zero indicates no limit to the number
                    of objects that dfs supports.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.datanode.registration.ip-hostname-check">dfs.namenode.datanode.registration.ip-hostname-check</a></td>
                  <td>true</td>
                  <td>
                    If true (the default), then the namenode requires that a connecting
                    datanode's address must be resolved to a hostname.  If necessary, a reverse
                    DNS lookup is performed.  All attempts to register a datanode from an
                    unresolvable address are rejected.
                    It is recommended that this setting be left on to prevent accidental
                    registration of datanodes listed by hostname in the excludes file during a
                    DNS outage.  Only set this to false in environments where there is no
                    infrastructure to support reverse DNS lookup.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.decommission.interval">dfs.namenode.decommission.interval</a></td>
                  <td>30s</td>
                  <td>Namenode periodicity in seconds to check if
                    decommission or maintenance is complete. Support multiple time unit
                    suffix(case insensitive), as described in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.decommission.blocks.per.interval">dfs.namenode.decommission.blocks.per.interval</a></td>
                  <td>500000</td>
                  <td>The approximate number of blocks to process per decommission
                    or maintenance interval, as defined in dfs.namenode.decommission.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.decommission.max.concurrent.tracked.nodes">dfs.namenode.decommission.max.concurrent.tracked.nodes</a></td>
                  <td>100</td>
                  <td>
                    The maximum number of decommission-in-progress or
                    entering-maintenance datanodes nodes that will be tracked at one time by
                    the namenode. Tracking these datanode consumes additional NN memory
                    proportional to the number of blocks on the datnode. Having a conservative
                    limit reduces the potential impact of decommissioning or maintenance of
                    a large number of nodes at once.
                    A value of 0 means no limit will be enforced.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.redundancy.interval.seconds">dfs.namenode.redundancy.interval.seconds</a></td>
                  <td>3s</td>
                  <td>The periodicity in seconds with which the namenode computes
                    low redundancy work for datanodes. Support multiple time unit suffix(case insensitive),
                    as described in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.accesstime.precision">dfs.namenode.accesstime.precision</a></td>
                  <td>3600000</td>
                  <td>The access time for HDFS file is precise upto this value.
                    The default value is 1 hour. Setting a value of 0 disables
                    access times for HDFS.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.plugins">dfs.datanode.plugins</a></td>
                  <td></td>
                  <td>Comma-separated list of datanode plug-ins to be activated.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.plugins">dfs.namenode.plugins</a></td>
                  <td></td>
                  <td>Comma-separated list of namenode plug-ins to be activated.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.block-placement-policy.default.prefer-local-node">dfs.namenode.block-placement-policy.default.prefer-local-node</a></td>
                  <td>true</td>
                  <td>Controls how the default block placement policy places
                    the first replica of a block. When true, it will prefer the node where
                    the client is running.  When false, it will prefer a node in the same rack
                    as the client. Setting to false avoids situations where entire copies of
                    large files end up on a single node, thus creating hotspots.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.stream-buffer-size">dfs.stream-buffer-size</a></td>
                  <td>4096</td>
                  <td>The size of buffer to stream files.
                    The size of this buffer should probably be a multiple of hardware
                    page size (4096 on Intel x86), and it determines how much data is
                    buffered during read and write operations.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.bytes-per-checksum">dfs.bytes-per-checksum</a></td>
                  <td>512</td>
                  <td>The number of bytes per checksum.  Must not be larger than
                    dfs.stream-buffer-size
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client-write-packet-size">dfs.client-write-packet-size</a></td>
                  <td>65536</td>
                  <td>Packet size for clients to write</td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.exclude.nodes.cache.expiry.interval.millis">dfs.client.write.exclude.nodes.cache.expiry.interval.millis</a></td>
                  <td>600000</td>
                  <td>The maximum period to keep a DN in the excluded nodes list
                    at a client. After this period, in milliseconds, the previously excluded node(s) will
                    be removed automatically from the cache and will be considered good for block allocations
                    again. Useful to lower or raise in situations where you keep a file open for very long
                    periods (such as a Write-Ahead-Log (WAL) file) to make the writer tolerant to cluster maintenance
                    restarts. Defaults to 10 minutes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.dir">dfs.namenode.checkpoint.dir</a></td>
                  <td>file://${hadoop.tmp.dir}/dfs/namesecondary</td>
                  <td>Determines where on the local filesystem the DFS secondary
                    name node should store the temporary images to merge.
                    If this is a comma-delimited list of directories then the image is
                    replicated in all of the directories for redundancy.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.edits.dir">dfs.namenode.checkpoint.edits.dir</a></td>
                  <td>${dfs.namenode.checkpoint.dir}</td>
                  <td>Determines where on the local filesystem the DFS secondary
                    name node should store the temporary edits to merge.
                    If this is a comma-delimited list of directories then the edits is
                    replicated in all of the directories for redundancy.
                    Default value is same as dfs.namenode.checkpoint.dir
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.period">dfs.namenode.checkpoint.period</a></td>
                  <td>3600s</td>
                  <td>
                    The number of seconds between two periodic checkpoints.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.txns">dfs.namenode.checkpoint.txns</a></td>
                  <td>1000000</td>
                  <td>The Secondary NameNode or CheckpointNode will create a checkpoint
                    of the namespace every 'dfs.namenode.checkpoint.txns' transactions, regardless
                    of whether 'dfs.namenode.checkpoint.period' has expired.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.check.period">dfs.namenode.checkpoint.check.period</a></td>
                  <td>60s</td>
                  <td>The SecondaryNameNode and CheckpointNode will poll the NameNode
                    every 'dfs.namenode.checkpoint.check.period' seconds to query the number
                    of uncheckpointed transactions. Support multiple time unit suffix(case insensitive),
                    as described in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.max-retries">dfs.namenode.checkpoint.max-retries</a></td>
                  <td>3</td>
                  <td>The SecondaryNameNode retries failed checkpointing. If the
                    failure occurs while loading fsimage or replaying edits, the number of
                    retries is limited by this variable.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.checkpoint.check.quiet-multiplier">dfs.namenode.checkpoint.check.quiet-multiplier</a></td>
                  <td>1.5</td>
                  <td>
                    Used to calculate the amount of time between retries when in the 'quiet' period
                    for creating checkpoints (active namenode already has an up-to-date image from another
                    checkpointer), so we wait a multiplier of the dfs.namenode.checkpoint.check.period before
                    retrying the checkpoint because another node likely is already managing the checkpoints,
                    allowing us to save bandwidth to transfer checkpoints that don't need to be used.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.num.checkpoints.retained">dfs.namenode.num.checkpoints.retained</a></td>
                  <td>2</td>
                  <td>The number of image checkpoint files (fsimage_*) that will be retained by
                    the NameNode and Secondary NameNode in their storage directories. All edit
                    logs (stored on edits_* files) necessary to recover an up-to-date namespace from the oldest retained
                    checkpoint will also be retained.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.num.extra.edits.retained">dfs.namenode.num.extra.edits.retained</a></td>
                  <td>1000000</td>
                  <td>The number of extra transactions which should be retained
                    beyond what is minimally necessary for a NN restart.
                    It does not translate directly to file's age, or the number of files kept,
                    but to the number of transactions (here "edits" means transactions).
                    One edit file may contain several transactions (edits).
                    During checkpoint, NameNode will identify the total number of edits to retain as extra by
                    checking the latest checkpoint transaction value, subtracted by the value of this property.
                    Then, it scans edits files to identify the older ones that don't include the computed range of
                    retained transactions that are to be kept around, and purges them subsequently.
                    The retainment can be useful for audit purposes or for an HA setup where a remote Standby Node may have
                    been offline for some time and need to have a longer backlog of retained
                    edits in order to start again.
                    Typically each edit is on the order of a few hundred bytes, so the default
                    of 1 million edits should be on the order of hundreds of MBs or low GBs.
                    NOTE: Fewer extra edits may be retained than value specified for this setting
                    if doing so would mean that more segments would be retained than the number
                    configured by dfs.namenode.max.extra.edits.segments.retained.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max.extra.edits.segments.retained">dfs.namenode.max.extra.edits.segments.retained</a></td>
                  <td>10000</td>
                  <td>The maximum number of extra edit log segments which should be retained
                    beyond what is minimally necessary for a NN restart. When used in conjunction with
                    dfs.namenode.num.extra.edits.retained, this configuration property serves to cap
                    the number of extra edits files to a reasonable value.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.delegation.key.update-interval">dfs.namenode.delegation.key.update-interval</a></td>
                  <td>86400000</td>
                  <td>The update interval for master key for delegation tokens
                    in the namenode in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.delegation.token.max-lifetime">dfs.namenode.delegation.token.max-lifetime</a></td>
                  <td>604800000</td>
                  <td>The maximum lifetime in milliseconds for which a delegation
                    token is valid.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.delegation.token.renew-interval">dfs.namenode.delegation.token.renew-interval</a></td>
                  <td>86400000</td>
                  <td>The renewal interval for delegation token in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.failed.volumes.tolerated">dfs.datanode.failed.volumes.tolerated</a></td>
                  <td>0</td>
                  <td>The number of volumes that are allowed to
                    fail before a datanode stops offering service. By default
                    any volume failure will cause a datanode to shutdown.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.compress">dfs.image.compress</a></td>
                  <td>false</td>
                  <td>Should the dfs image be compressed?
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.compression.codec">dfs.image.compression.codec</a></td>
                  <td>org.apache.hadoop.io.compress.DefaultCodec</td>
                  <td>If the dfs image is compressed, how should they be compressed?
                    This has to be a codec defined in io.compression.codecs.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.transfer.timeout">dfs.image.transfer.timeout</a></td>
                  <td>60000</td>
                  <td>
                    Socket timeout for image transfer in milliseconds. This timeout and the related
                    dfs.image.transfer.bandwidthPerSec parameter should be configured such
                    that normal image transfer can complete successfully.
                    This timeout prevents client hangs when the sender fails during
                    image transfer. This is socket timeout during image transfer.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.transfer.bandwidthPerSec">dfs.image.transfer.bandwidthPerSec</a></td>
                  <td>0</td>
                  <td>
                    Maximum bandwidth used for regular image transfers (instead of
                    bootstrapping the standby namenode), in bytes per second.
                    This can help keep normal namenode operations responsive during
                    checkpointing. The maximum bandwidth and timeout in
                    dfs.image.transfer.timeout should be set such that normal image
                    transfers can complete successfully.
                    A default value of 0 indicates that throttling is disabled.
                    The maximum bandwidth used for bootstrapping standby namenode is
                    configured with dfs.image.transfer-bootstrap-standby.bandwidthPerSec.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.transfer-bootstrap-standby.bandwidthPerSec">dfs.image.transfer-bootstrap-standby.bandwidthPerSec</a></td>
                  <td>0</td>
                  <td>
                    Maximum bandwidth used for transferring image to bootstrap standby
                    namenode, in bytes per second.
                    A default value of 0 indicates that throttling is disabled. This default
                    value should be used in most cases, to ensure timely HA operations.
                    The maximum bandwidth used for regular image transfers is configured
                    with dfs.image.transfer.bandwidthPerSec.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.image.transfer.chunksize">dfs.image.transfer.chunksize</a></td>
                  <td>65536</td>
                  <td>
                    Chunksize in bytes to upload the checkpoint.
                    Chunked streaming is used to avoid internal buffering of contents
                    of image file of huge size.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.edit.log.transfer.timeout">dfs.edit.log.transfer.timeout</a></td>
                  <td>30000</td>
                  <td>
                    Socket timeout for edit log transfer in milliseconds. This timeout
                    should be configured such that normal edit log transfer for journal
                    node syncing can complete successfully.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.edit.log.transfer.bandwidthPerSec">dfs.edit.log.transfer.bandwidthPerSec</a></td>
                  <td>0</td>
                  <td>
                    Maximum bandwidth used for transferring edit log to between journal nodes
                    for syncing, in bytes per second.
                    A default value of 0 indicates that throttling is disabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.support.allow.format">dfs.namenode.support.allow.format</a></td>
                  <td>true</td>
                  <td>Does HDFS namenode allow itself to be formatted?
                    You may consider setting this to false for any production
                    cluster, to avoid any possibility of formatting a running DFS.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.max.transfer.threads">dfs.datanode.max.transfer.threads</a></td>
                  <td>4096</td>
                  <td>
                    Specifies the maximum number of threads to use for transferring data
                    in and out of the DN.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.scan.period.hours">dfs.datanode.scan.period.hours</a></td>
                  <td>504</td>
                  <td>
                    If this is positive, the DataNode will not scan any
                    individual block more than once in the specified scan period.
                    If this is negative, the block scanner is disabled.
                    If this is set to zero, then the default value of 504 hours
                    or 3 weeks is used. Prior versions of HDFS incorrectly documented
                    that setting this key to zero will disable the block scanner.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.scanner.volume.bytes.per.second">dfs.block.scanner.volume.bytes.per.second</a></td>
                  <td>1048576</td>
                  <td>
                    If this is 0, the DataNode's block scanner will be disabled.  If this
                    is positive, this is the number of bytes per second that the DataNode's
                    block scanner will try to scan from each volume.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.readahead.bytes">dfs.datanode.readahead.bytes</a></td>
                  <td>4194304</td>
                  <td>
                    While reading block files, if the Hadoop native libraries are available,
                    the datanode can use the posix_fadvise system call to explicitly
                    page data into the operating system buffer cache ahead of the current
                    reader's position. This can improve performance especially when
                    disks are highly contended.
                    This configuration specifies the number of bytes ahead of the current
                    read position which the datanode will attempt to read ahead. This
                    feature may be disabled by configuring this property to 0.
                    If the native libraries are not available, this configuration has no
                    effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.drop.cache.behind.reads">dfs.datanode.drop.cache.behind.reads</a></td>
                  <td>false</td>
                  <td>
                    In some workloads, the data read from HDFS is known to be significantly
                    large enough that it is unlikely to be useful to cache it in the
                    operating system buffer cache. In this case, the DataNode may be
                    configured to automatically purge all data from the buffer cache
                    after it is delivered to the client. This behavior is automatically
                    disabled for workloads which read only short sections of a block
                    (e.g HBase random-IO workloads).
                    This may improve performance for some workloads by freeing buffer
                    cache space usage for more cacheable data.
                    If the Hadoop native libraries are not available, this configuration
                    has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.drop.cache.behind.writes">dfs.datanode.drop.cache.behind.writes</a></td>
                  <td>false</td>
                  <td>
                    In some workloads, the data written to HDFS is known to be significantly
                    large enough that it is unlikely to be useful to cache it in the
                    operating system buffer cache. In this case, the DataNode may be
                    configured to automatically purge all data from the buffer cache
                    after it is written to disk.
                    This may improve performance for some workloads by freeing buffer
                    cache space usage for more cacheable data.
                    If the Hadoop native libraries are not available, this configuration
                    has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.sync.behind.writes">dfs.datanode.sync.behind.writes</a></td>
                  <td>false</td>
                  <td>
                    If this configuration is enabled, the datanode will instruct the
                    operating system to enqueue all written data to the disk immediately
                    after it is written. This differs from the usual OS policy which
                    may wait for up to 30 seconds before triggering writeback.
                    This may improve performance for some workloads by smoothing the
                    IO profile for data written to disk.
                    If the Hadoop native libraries are not available, this configuration
                    has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.max.attempts">dfs.client.failover.max.attempts</a></td>
                  <td>15</td>
                  <td>
                    Expert only. The number of client failover attempts that should be
                    made before the failover is considered failed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.sleep.base.millis">dfs.client.failover.sleep.base.millis</a></td>
                  <td>500</td>
                  <td>
                    Expert only. The time to wait, in milliseconds, between failover
                    attempts increases exponentially as a function of the number of
                    attempts made so far, with a random factor of +/- 50%. This option
                    specifies the base value used in the failover calculation. The
                    first failover will retry immediately. The 2nd failover attempt
                    will delay at least dfs.client.failover.sleep.base.millis
                    milliseconds. And so on.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.sleep.max.millis">dfs.client.failover.sleep.max.millis</a></td>
                  <td>15000</td>
                  <td>
                    Expert only. The time to wait, in milliseconds, between failover
                    attempts increases exponentially as a function of the number of
                    attempts made so far, with a random factor of +/- 50%. This option
                    specifies the maximum value to wait between failovers.
                    Specifically, the time between two failover attempts will not
                    exceed +/- 50% of dfs.client.failover.sleep.max.millis
                    milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.connection.retries">dfs.client.failover.connection.retries</a></td>
                  <td>0</td>
                  <td>
                    Expert only. Indicates the number of retries a failover IPC client
                    will make to establish a server connection.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.connection.retries.on.timeouts">dfs.client.failover.connection.retries.on.timeouts</a></td>
                  <td>0</td>
                  <td>
                    Expert only. The number of retry attempts a failover IPC client
                    will make on socket timeout when establishing a server connection.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.datanode-restart.timeout">dfs.client.datanode-restart.timeout</a></td>
                  <td>30s</td>
                  <td>
                    Expert only. The time to wait, in seconds, from reception of an
                    datanode shutdown notification for quick restart, until declaring
                    the datanode dead and invoking the normal recovery mechanisms.
                    The notification is sent by a datanode when it is being shutdown
                    using the shutdownDatanode admin command with the upgrade option.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.nameservices">dfs.nameservices</a></td>
                  <td></td>
                  <td>
                    Comma-separated list of nameservices.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.nameservice.id">dfs.nameservice.id</a></td>
                  <td></td>
                  <td>
                    The ID of this nameservice. If the nameservice ID is not
                    configured or more than one nameservice is configured for
                    dfs.nameservices it is determined automatically by
                    matching the local node's address with the configured address.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.internal.nameservices">dfs.internal.nameservices</a></td>
                  <td></td>
                  <td>
                    Comma-separated list of nameservices that belong to this cluster.
                    Datanode will report to all the nameservices in this list. By default
                    this is set to the value of dfs.nameservices.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.namenodes.EXAMPLENAMESERVICE">dfs.ha.namenodes.EXAMPLENAMESERVICE</a></td>
                  <td></td>
                  <td>
                    The prefix for a given nameservice, contains a comma-separated
                    list of namenodes for a given nameservice (eg EXAMPLENAMESERVICE).
                    Unique identifiers for each NameNode in the nameservice, delimited by
                    commas. This will be used by DataNodes to determine all the NameNodes
                    in the cluster. For example, if you used “mycluster” as the nameservice
                    ID previously, and you wanted to use “nn1” and “nn2” as the individual
                    IDs of the NameNodes, you would configure a property
                    dfs.ha.namenodes.mycluster, and its value "nn1,nn2".
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.namenode.id">dfs.ha.namenode.id</a></td>
                  <td></td>
                  <td>
                    The ID of this namenode. If the namenode ID is not configured it
                    is determined automatically by matching the local node's address
                    with the configured address.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.log-roll.period">dfs.ha.log-roll.period</a></td>
                  <td>120s</td>
                  <td>
                    How often, in seconds, the StandbyNode should ask the active to
                    roll edit logs. Since the StandbyNode only reads from finalized
                    log segments, the StandbyNode will only be as up-to-date as how
                    often the logs are rolled. Note that failover triggers a log roll
                    so the StandbyNode will be up to date before it becomes active.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.tail-edits.period">dfs.ha.tail-edits.period</a></td>
                  <td>60s</td>
                  <td>
                    How often, in seconds, the StandbyNode should check for new
                    finalized log segments in the shared edits log.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.tail-edits.namenode-retries">dfs.ha.tail-edits.namenode-retries</a></td>
                  <td>3</td>
                  <td>
                    Number of retries to use when contacting the namenode when tailing the log.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.tail-edits.rolledits.timeout">dfs.ha.tail-edits.rolledits.timeout</a></td>
                  <td>60</td>
                  <td>The timeout in seconds of calling rollEdits RPC on Active NN.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.automatic-failover.enabled">dfs.ha.automatic-failover.enabled</a></td>
                  <td>false</td>
                  <td>
                    Whether automatic failover is enabled. See the HDFS High
                    Availability documentation for details on automatic HA
                    configuration.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.use.datanode.hostname">dfs.client.use.datanode.hostname</a></td>
                  <td>false</td>
                  <td>Whether clients should use datanode hostnames when
                    connecting to datanodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.use.datanode.hostname">dfs.datanode.use.datanode.hostname</a></td>
                  <td>false</td>
                  <td>Whether datanodes should use datanode hostnames when
                    connecting to other datanodes for data transfer.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.local.interfaces">dfs.client.local.interfaces</a></td>
                  <td></td>
                  <td>A comma separated list of network interface names to use
                    for data transfer between the client and datanodes. When creating
                    a connection to read from or write to a datanode, the client
                    chooses one of the specified interfaces at random and binds its
                    socket to the IP of that interface. Individual names may be
                    specified as either an interface name (eg "eth0"), a subinterface
                    name (eg "eth0:0"), or an IP address (which may be specified using
                    CIDR notation to match a range of IPs).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.shared.file.descriptor.paths">dfs.datanode.shared.file.descriptor.paths</a></td>
                  <td>/dev/shm,/tmp</td>
                  <td>
                    A comma-separated list of paths to use when creating file descriptors that
                    will be shared between the DataNode and the DFSClient.  Typically we use
                    /dev/shm, so that the file descriptors will not be written to disk.
                    Systems that don't have /dev/shm will fall back to /tmp by default.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.short.circuit.shared.memory.watcher.interrupt.check.ms">dfs.short.circuit.shared.memory.watcher.interrupt.check.ms</a></td>
                  <td>60000</td>
                  <td>
                    The length of time in milliseconds that the short-circuit shared memory
                    watcher will go between checking for java interruptions sent from other
                    threads.  This is provided mainly for unit tests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.kerberos.principal">dfs.namenode.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The NameNode service principal. This is typically set to
                    nn/_HOST@REALM.TLD. Each NameNode will substitute _HOST with its
                    own fully qualified hostname at startup. The _HOST placeholder
                    allows using the same configuration setting on both NameNodes
                    in an HA setup.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.keytab.file">dfs.namenode.keytab.file</a></td>
                  <td></td>
                  <td>
                    The keytab file used by each NameNode daemon to login as its
                    service principal. The principal name is configured with
                    dfs.namenode.kerberos.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.kerberos.principal">dfs.datanode.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The DataNode service principal. This is typically set to
                    dn/_HOST@REALM.TLD. Each DataNode will substitute _HOST with its
                    own fully qualified hostname at startup. The _HOST placeholder
                    allows using the same configuration setting on all DataNodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.keytab.file">dfs.datanode.keytab.file</a></td>
                  <td></td>
                  <td>
                    The keytab file used by each DataNode daemon to login as its
                    service principal. The principal name is configured with
                    dfs.datanode.kerberos.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.kerberos.principal">dfs.journalnode.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The JournalNode service principal. This is typically set to
                    jn/_HOST@REALM.TLD. Each JournalNode will substitute _HOST with its
                    own fully qualified hostname at startup. The _HOST placeholder
                    allows using the same configuration setting on all JournalNodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.keytab.file">dfs.journalnode.keytab.file</a></td>
                  <td></td>
                  <td>
                    The keytab file used by each JournalNode daemon to login as its
                    service principal. The principal name is configured with
                    dfs.journalnode.kerberos.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.kerberos.internal.spnego.principal">dfs.namenode.kerberos.internal.spnego.principal</a></td>
                  <td>${dfs.web.authentication.kerberos.principal}</td>
                  <td>
                    The server principal used by the NameNode for web UI SPNEGO
                    authentication when Kerberos security is enabled. This is
                    typically set to HTTP/_HOST@REALM.TLD The SPNEGO server principal
                    begins with the prefix HTTP/ by convention.
                    If the value is '*', the web server will attempt to login with
                    every principal specified in the keytab file
                    dfs.web.authentication.kerberos.keytab.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.kerberos.internal.spnego.principal">dfs.journalnode.kerberos.internal.spnego.principal</a></td>
                  <td></td>
                  <td>
                    The server principal used by the JournalNode HTTP Server for
                    SPNEGO authentication when Kerberos security is enabled. This is
                    typically set to HTTP/_HOST@REALM.TLD. The SPNEGO server principal
                    begins with the prefix HTTP/ by convention.
                    If the value is '*', the web server will attempt to login with
                    every principal specified in the keytab file
                    dfs.web.authentication.kerberos.keytab.
                    For most deployments this can be set to ${dfs.web.authentication.kerberos.principal}
                    i.e use the value of dfs.web.authentication.kerberos.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.secondary.namenode.kerberos.internal.spnego.principal">dfs.secondary.namenode.kerberos.internal.spnego.principal</a></td>
                  <td>${dfs.web.authentication.kerberos.principal}</td>
                  <td>
                    The server principal used by the Secondary NameNode for web UI SPNEGO
                    authentication when Kerberos security is enabled. Like all other
                    Secondary NameNode settings, it is ignored in an HA setup.
                    If the value is '*', the web server will attempt to login with
                    every principal specified in the keytab file
                    dfs.web.authentication.kerberos.keytab.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.web.authentication.kerberos.principal">dfs.web.authentication.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The server principal used by the NameNode for WebHDFS SPNEGO
                    authentication.
                    Required when WebHDFS and security are enabled. In most secure clusters this
                    setting is also used to specify the values for
                    dfs.namenode.kerberos.internal.spnego.principal and
                    dfs.journalnode.kerberos.internal.spnego.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.web.authentication.kerberos.keytab">dfs.web.authentication.kerberos.keytab</a></td>
                  <td></td>
                  <td>
                    The keytab file for the principal corresponding to
                    dfs.web.authentication.kerberos.principal.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.kerberos.principal.pattern">dfs.namenode.kerberos.principal.pattern</a></td>
                  <td>*</td>
                  <td>
                    A client-side RegEx that can be configured to control
                    allowed realms to authenticate with (useful in cross-realm env.)
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.avoid.read.stale.datanode">dfs.namenode.avoid.read.stale.datanode</a></td>
                  <td>false</td>
                  <td>
                    Indicate whether or not to avoid reading from "stale" datanodes whose
                    heartbeat messages have not been received by the namenode
                    for more than a specified time interval. Stale datanodes will be
                    moved to the end of the node list returned for reading. See
                    dfs.namenode.avoid.write.stale.datanode for a similar setting for writes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.avoid.write.stale.datanode">dfs.namenode.avoid.write.stale.datanode</a></td>
                  <td>false</td>
                  <td>
                    Indicate whether or not to avoid writing to "stale" datanodes whose
                    heartbeat messages have not been received by the namenode
                    for more than a specified time interval. Writes will avoid using
                    stale datanodes unless more than a configured ratio
                    (dfs.namenode.write.stale.datanode.ratio) of datanodes are marked as
                    stale. See dfs.namenode.avoid.read.stale.datanode for a similar setting
                    for reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.stale.datanode.interval">dfs.namenode.stale.datanode.interval</a></td>
                  <td>30000</td>
                  <td>
                    Default time interval in milliseconds for marking a datanode as "stale",
                    i.e., if the namenode has not received heartbeat msg from a datanode for
                    more than this time interval, the datanode will be marked and treated
                    as "stale" by default. The stale interval cannot be too small since
                    otherwise this may cause too frequent change of stale states.
                    We thus set a minimum stale interval value (the default value is 3 times
                    of heartbeat interval) and guarantee that the stale interval cannot be less
                    than the minimum value. A stale data node is avoided during lease/block
                    recovery. It can be conditionally avoided for reads (see
                    dfs.namenode.avoid.read.stale.datanode) and for writes (see
                    dfs.namenode.avoid.write.stale.datanode).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.write.stale.datanode.ratio">dfs.namenode.write.stale.datanode.ratio</a></td>
                  <td>0.5f</td>
                  <td>
                    When the ratio of number stale datanodes to total datanodes marked
                    is greater than this ratio, stop avoiding writing to stale nodes so
                    as to prevent causing hotspots.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.invalidate.work.pct.per.iteration">dfs.namenode.invalidate.work.pct.per.iteration</a></td>
                  <td>0.32f</td>
                  <td>
                    *Note*: Advanced property. Change with caution.
                    This determines the percentage amount of block
                    invalidations (deletes) to do over a single DN heartbeat
                    deletion command. The final deletion count is determined by applying this
                    percentage to the number of live nodes in the system.
                    The resultant number is the number of blocks from the deletion list
                    chosen for proper invalidation over a single heartbeat of a single DN.
                    Value should be a positive, non-zero percentage in float notation (X.Yf),
                    with 1.0f meaning 100%.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.replication.work.multiplier.per.iteration">dfs.namenode.replication.work.multiplier.per.iteration</a></td>
                  <td>2</td>
                  <td>
                    *Note*: Advanced property. Change with caution.
                    This determines the total amount of block transfers to begin in
                    parallel at a DN, for replication, when such a command list is being
                    sent over a DN heartbeat by the NN. The actual number is obtained by
                    multiplying this multiplier with the total number of live nodes in the
                    cluster. The result number is the number of blocks to begin transfers
                    immediately for, per DN heartbeat. This number can be any positive,
                    non-zero integer.
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.server.port">nfs.server.port</a></td>
                  <td>2049</td>
                  <td>
                    Specify the port number used by Hadoop NFS.
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.mountd.port">nfs.mountd.port</a></td>
                  <td>4242</td>
                  <td>
                    Specify the port number used by Hadoop mount daemon.
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.dump.dir">nfs.dump.dir</a></td>
                  <td>/tmp/.hdfs-nfs</td>
                  <td>
                    This directory is used to temporarily save out-of-order writes before
                    writing to HDFS. For each file, the out-of-order writes are dumped after
                    they are accumulated to exceed certain threshold (e.g., 1MB) in memory.
                    One needs to make sure the directory has enough space.
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.rtmax">nfs.rtmax</a></td>
                  <td>1048576</td>
                  <td>This is the maximum size in bytes of a READ request
                    supported by the NFS gateway. If you change this, make sure you
                    also update the nfs mount's rsize(add rsize= # of bytes to the
                    mount directive).
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.wtmax">nfs.wtmax</a></td>
                  <td>1048576</td>
                  <td>This is the maximum size in bytes of a WRITE request
                    supported by the NFS gateway. If you change this, make sure you
                    also update the nfs mount's wsize(add wsize= # of bytes to the
                    mount directive).
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.keytab.file">nfs.keytab.file</a></td>
                  <td></td>
                  <td>
                    *Note*: Advanced property. Change with caution.
                    This is the path to the keytab file for the hdfs-nfs gateway.
                    This is required when the cluster is kerberized.
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.kerberos.principal">nfs.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    *Note*: Advanced property. Change with caution.
                    This is the name of the kerberos principal. This is required when
                    the cluster is kerberized.It must be of this format:
                    nfs-gateway-user/nfs-gateway-host@kerberos-realm
                  </td>
                </tr>
                <tr>
                  <td><a name="nfs.allow.insecure.ports">nfs.allow.insecure.ports</a></td>
                  <td>true</td>
                  <td>
                    When set to false, client connections originating from unprivileged ports
                    (those above 1023) will be rejected. This is to ensure that clients
                    connecting to this NFS Gateway must have had root privilege on the machine
                    where they're connecting from.
                  </td>
                </tr>
                <tr>
                  <td><a name="hadoop.fuse.connection.timeout">hadoop.fuse.connection.timeout</a></td>
                  <td>300</td>
                  <td>
                    The minimum number of seconds that we'll cache libhdfs connection objects
                    in fuse_dfs. Lower values will result in lower memory consumption; higher
                    values may speed up access by avoiding the overhead of creating new
                    connection objects.
                  </td>
                </tr>
                <tr>
                  <td><a name="hadoop.fuse.timer.period">hadoop.fuse.timer.period</a></td>
                  <td>5</td>
                  <td>
                    The number of seconds between cache expiry checks in fuse_dfs. Lower values
                    will result in fuse_dfs noticing changes to Kerberos ticket caches more
                    quickly.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.metrics.logger.period.seconds">dfs.namenode.metrics.logger.period.seconds</a></td>
                  <td>600</td>
                  <td>
                    This setting controls how frequently the NameNode logs its metrics. The
                    logging configuration must also define one or more appenders for
                    NameNodeMetricsLog for the metrics to be logged.
                    NameNode metrics logging is disabled if this value is set to zero or
                    less than zero.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.metrics.logger.period.seconds">dfs.datanode.metrics.logger.period.seconds</a></td>
                  <td>600</td>
                  <td>
                    This setting controls how frequently the DataNode logs its metrics. The
                    logging configuration must also define one or more appenders for
                    DataNodeMetricsLog for the metrics to be logged.
                    DataNode metrics logging is disabled if this value is set to zero or
                    less than zero.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.metrics.percentiles.intervals">dfs.metrics.percentiles.intervals</a></td>
                  <td></td>
                  <td>
                    Comma-delimited set of integers denoting the desired rollover intervals
                    (in seconds) for percentile latency metrics on the Namenode and Datanode.
                    By default, percentile latency metrics are disabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.peer.stats.enabled">dfs.datanode.peer.stats.enabled</a></td>
                  <td>false</td>
                  <td>
                    A switch to turn on/off tracking DataNode peer statistics.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.outliers.report.interval">dfs.datanode.outliers.report.interval</a></td>
                  <td>30m</td>
                  <td>
                    This setting controls how frequently DataNodes will report their peer
                    latencies to the NameNode via heartbeats.  This setting supports
                    multiple time unit suffixes as described in dfs.heartbeat.interval.
                    If no suffix is specified then milliseconds is assumed.
                    It is ignored if dfs.datanode.peer.stats.enabled is false.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.fileio.profiling.sampling.percentage">dfs.datanode.fileio.profiling.sampling.percentage</a></td>
                  <td>0</td>
                  <td>
                    This setting controls the percentage of file I/O events which will be
                    profiled for DataNode disk statistics. The default value of 0 disables
                    disk statistics. Set to an integer value between 1 and 100 to enable disk
                    statistics.
                  </td>
                </tr>
                <tr>
                  <td><a name="hadoop.user.group.metrics.percentiles.intervals">hadoop.user.group.metrics.percentiles.intervals</a></td>
                  <td></td>
                  <td>
                    A comma-separated list of the granularity in seconds for the metrics
                    which describe the 50/75/90/95/99th percentile latency for group resolution
                    in milliseconds.
                    By default, percentile latency metrics are disabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.encrypt.data.transfer">dfs.encrypt.data.transfer</a></td>
                  <td>false</td>
                  <td>
                    Whether or not actual block data that is read/written from/to HDFS should
                    be encrypted on the wire. This only needs to be set on the NN and DNs,
                    clients will deduce this automatically. It is possible to override this setting
                    per connection by specifying custom logic via dfs.trustedchannel.resolver.class.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.encrypt.data.transfer.algorithm">dfs.encrypt.data.transfer.algorithm</a></td>
                  <td></td>
                  <td>
                    This value may be set to either "3des" or "rc4". If nothing is set, then
                    the configured JCE default on the system is used (usually 3DES.) It is
                    widely believed that 3DES is more cryptographically secure, but RC4 is
                    substantially faster.
                    Note that if AES is supported by both the client and server then this
                    encryption algorithm will only be used to initially transfer keys for AES.
                    (See dfs.encrypt.data.transfer.cipher.suites.)
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.encrypt.data.transfer.cipher.suites">dfs.encrypt.data.transfer.cipher.suites</a></td>
                  <td></td>
                  <td>
                    This value may be either undefined or AES/CTR/NoPadding.  If defined, then
                    dfs.encrypt.data.transfer uses the specified cipher suite for data
                    encryption.  If not defined, then only the algorithm specified in
                    dfs.encrypt.data.transfer.algorithm is used.  By default, the property is
                    not defined.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.encrypt.data.transfer.cipher.key.bitlength">dfs.encrypt.data.transfer.cipher.key.bitlength</a></td>
                  <td>128</td>
                  <td>
                    The key bitlength negotiated by dfsclient and datanode for encryption.
                    This value may be set to either 128, 192 or 256.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.trustedchannel.resolver.class">dfs.trustedchannel.resolver.class</a></td>
                  <td></td>
                  <td>
                    TrustedChannelResolver is used to determine whether a channel
                    is trusted for plain data transfer. The TrustedChannelResolver is
                    invoked on both client and server side. If the resolver indicates
                    that the channel is trusted, then the data transfer will not be
                    encrypted even if dfs.encrypt.data.transfer is set to true. The
                    default implementation returns false indicating that the channel
                    is not trusted.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.data.transfer.protection">dfs.data.transfer.protection</a></td>
                  <td></td>
                  <td>
                    A comma-separated list of SASL protection values used for secured
                    connections to the DataNode when reading or writing block data.  Possible
                    values are authentication, integrity and privacy.  authentication means
                    authentication only and no integrity or privacy; integrity implies
                    authentication and integrity are enabled; and privacy implies all of
                    authentication, integrity and privacy are enabled.  If
                    dfs.encrypt.data.transfer is set to true, then it supersedes the setting for
                    dfs.data.transfer.protection and enforces that all connections must use a
                    specialized encrypted SASL handshake.  This property is ignored for
                    connections to a DataNode listening on a privileged port.  In this case, it
                    is assumed that the use of a privileged port establishes sufficient trust.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.data.transfer.saslproperties.resolver.class">dfs.data.transfer.saslproperties.resolver.class</a></td>
                  <td></td>
                  <td>
                    SaslPropertiesResolver used to resolve the QOP used for a connection to the
                    DataNode when reading or writing block data. If not specified, the value of
                    hadoop.security.saslproperties.resolver.class is used as the default value.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.rpc-address">dfs.journalnode.rpc-address</a></td>
                  <td>0.0.0.0:8485</td>
                  <td>
                    The JournalNode RPC server address and port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.http-address">dfs.journalnode.http-address</a></td>
                  <td>0.0.0.0:8480</td>
                  <td>
                    The address and port the JournalNode HTTP server listens on.
                    If the port is 0 then the server will start on a free port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.https-address">dfs.journalnode.https-address</a></td>
                  <td>0.0.0.0:8481</td>
                  <td>
                    The address and port the JournalNode HTTPS server listens on.
                    If the port is 0 then the server will start on a free port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.audit.loggers">dfs.namenode.audit.loggers</a></td>
                  <td>default</td>
                  <td>
                    List of classes implementing audit loggers that will receive audit events.
                    These should be implementations of org.apache.hadoop.hdfs.server.namenode.AuditLogger.
                    The special value "default" can be used to reference the default audit
                    logger, which uses the configured log system. Installing custom audit loggers
                    may affect the performance and stability of the NameNode. Refer to the custom
                    logger's documentation for more details.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold">dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</a></td>
                  <td>10737418240</td>
                  <td>
                    Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
                    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
                    This setting controls how much DN volumes are allowed to differ in terms of
                    bytes of free disk space before they are considered imbalanced. If the free
                    space of all the volumes are within this range of each other, the volumes
                    will be considered balanced and block assignments will be done on a pure
                    round robin basis.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction">dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</a></td>
                  <td>0.75f</td>
                  <td>
                    Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to
                    org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy.
                    This setting controls what percentage of new block allocations will be sent
                    to volumes with more available disk space than others. This setting should
                    be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should
                    be no reason to prefer that volumes with less available disk space receive
                    more block allocations.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.noeditlogchannelflush">dfs.namenode.edits.noeditlogchannelflush</a></td>
                  <td>false</td>
                  <td>
                    Specifies whether to flush edit log file channel. When set, expensive
                    FileChannel#force calls are skipped and synchronous disk writes are
                    enabled instead by opening the edit log file with RandomAccessFile("rws")
                    flags. This can significantly improve the performance of edit log writes
                    on the Windows platform.
                    Note that the behavior of the "rws" flags is platform and hardware specific
                    and might not provide the same level of guarantees as FileChannel#force.
                    For example, the write will skip the disk-cache on SAS and SCSI devices
                    while it might not on SATA devices. This is an expert level setting,
                    change with caution.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.cache.drop.behind.writes">dfs.client.cache.drop.behind.writes</a></td>
                  <td></td>
                  <td>
                    Just like dfs.datanode.drop.cache.behind.writes, this setting causes the
                    page cache to be dropped behind HDFS writes, potentially freeing up more
                    memory for other uses.  Unlike dfs.datanode.drop.cache.behind.writes, this
                    is a client-side setting rather than a setting for the entire datanode.
                    If present, this setting will override the DataNode default.
                    If the native libraries are not available to the DataNode, this
                    configuration has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.cache.drop.behind.reads">dfs.client.cache.drop.behind.reads</a></td>
                  <td></td>
                  <td>
                    Just like dfs.datanode.drop.cache.behind.reads, this setting causes the
                    page cache to be dropped behind HDFS reads, potentially freeing up more
                    memory for other uses.  Unlike dfs.datanode.drop.cache.behind.reads, this
                    is a client-side setting rather than a setting for the entire datanode.  If
                    present, this setting will override the DataNode default.
                    If the native libraries are not available to the DataNode, this
                    configuration has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.cache.readahead">dfs.client.cache.readahead</a></td>
                  <td></td>
                  <td>
                    When using remote reads, this setting causes the datanode to
                    read ahead in the block file using posix_fadvise, potentially decreasing
                    I/O wait times.  Unlike dfs.datanode.readahead.bytes, this is a client-side
                    setting rather than a setting for the entire datanode.  If present, this
                    setting will override the DataNode default.
                    When using local reads, this setting determines how much readahead we do in
                    BlockReaderLocal.
                    If the native libraries are not available to the DataNode, this
                    configuration has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.server-defaults.validity.period.ms">dfs.client.server-defaults.validity.period.ms</a></td>
                  <td>3600000</td>
                  <td>
                    The amount of milliseconds after which cached server defaults are updated.
                    By default this parameter is set to 1 hour.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.enable.retrycache">dfs.namenode.enable.retrycache</a></td>
                  <td>true</td>
                  <td>
                    This enables the retry cache on the namenode. Namenode tracks for
                    non-idempotent requests the corresponding response. If a client retries the
                    request, the response from the retry cache is sent. Such operations
                    are tagged with annotation @AtMostOnce in namenode protocols. It is
                    recommended that this flag be set to true. Setting it to false, will result
                    in clients getting failure responses to retried request. This flag must
                    be enabled in HA setup for transparent fail-overs.
                    The entries in the cache have expiration time configurable
                    using dfs.namenode.retrycache.expirytime.millis.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.retrycache.expirytime.millis">dfs.namenode.retrycache.expirytime.millis</a></td>
                  <td>600000</td>
                  <td>
                    The time for which retry cache entries are retained.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.retrycache.heap.percent">dfs.namenode.retrycache.heap.percent</a></td>
                  <td>0.03f</td>
                  <td>
                    This parameter configures the heap size allocated for retry cache
                    (excluding the response cached). This corresponds to approximately
                    4096 entries for every 64MB of namenode process java heap size.
                    Assuming retry cache entry expiration time (configured using
                    dfs.namenode.retrycache.expirytime.millis) of 10 minutes, this
                    enables retry cache to support 7 operations per second sustained
                    for 10 minutes. As the heap size is increased, the operation rate
                    linearly increases.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.mmap.enabled">dfs.client.mmap.enabled</a></td>
                  <td>true</td>
                  <td>
                    If this is set to false, the client won't attempt to perform memory-mapped reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.mmap.cache.size">dfs.client.mmap.cache.size</a></td>
                  <td>256</td>
                  <td>
                    When zero-copy reads are used, the DFSClient keeps a cache of recently used
                    memory mapped regions.  This parameter controls the maximum number of
                    entries that we will keep in that cache.
                    The larger this number is, the more file descriptors we will potentially
                    use for memory-mapped files.  mmaped files also use virtual address space.
                    You may need to increase your ulimit virtual address space limits before
                    increasing the client mmap cache size.
                    Note that you can still do zero-copy reads when this size is set to 0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.mmap.cache.timeout.ms">dfs.client.mmap.cache.timeout.ms</a></td>
                  <td>3600000</td>
                  <td>
                    The minimum length of time that we will keep an mmap entry in the cache
                    between uses.  If an entry is in the cache longer than this, and nobody
                    uses it, it will be removed by a background thread.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.mmap.retry.timeout.ms">dfs.client.mmap.retry.timeout.ms</a></td>
                  <td>300000</td>
                  <td>
                    The minimum amount of time that we will wait before retrying a failed mmap
                    operation.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.short.circuit.replica.stale.threshold.ms">dfs.client.short.circuit.replica.stale.threshold.ms</a></td>
                  <td>1800000</td>
                  <td>
                    The maximum amount of time that we will consider a short-circuit replica to
                    be valid, if there is no communication from the DataNode.  After this time
                    has elapsed, we will re-fetch the short-circuit replica even if it is in
                    the cache.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.path.based.cache.block.map.allocation.percent">dfs.namenode.path.based.cache.block.map.allocation.percent</a></td>
                  <td>0.25</td>
                  <td>
                    The percentage of the Java heap which we will allocate to the cached blocks
                    map.  The cached blocks map is a hash map which uses chained hashing.
                    Smaller maps may be accessed more slowly if the number of cached blocks is
                    large; larger maps will consume more memory.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.max.locked.memory">dfs.datanode.max.locked.memory</a></td>
                  <td>0</td>
                  <td>
                    The amount of memory in bytes to use for caching of block replicas in
                    memory on the datanode. The datanode's maximum locked memory soft ulimit
                    (RLIMIT_MEMLOCK) must be set to at least this value, else the datanode
                    will abort on startup.
                    By default, this parameter is set to 0, which disables in-memory caching.
                    If the native libraries are not available to the DataNode, this
                    configuration has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.list.cache.directives.num.responses">dfs.namenode.list.cache.directives.num.responses</a></td>
                  <td>100</td>
                  <td>
                    This value controls the number of cache directives that the NameNode will
                    send over the wire in response to a listDirectives RPC.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.list.cache.pools.num.responses">dfs.namenode.list.cache.pools.num.responses</a></td>
                  <td>100</td>
                  <td>
                    This value controls the number of cache pools that the NameNode will
                    send over the wire in response to a listPools RPC.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.path.based.cache.refresh.interval.ms">dfs.namenode.path.based.cache.refresh.interval.ms</a></td>
                  <td>30000</td>
                  <td>
                    The amount of milliseconds between subsequent path cache rescans.  Path
                    cache rescans are when we calculate which blocks should be cached, and on
                    what datanodes.
                    By default, this parameter is set to 30 seconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.path.based.cache.retry.interval.ms">dfs.namenode.path.based.cache.retry.interval.ms</a></td>
                  <td>30000</td>
                  <td>
                    When the NameNode needs to uncache something that is cached, or cache
                    something that is not cached, it must direct the DataNodes to do so by
                    sending a DNA_CACHE or DNA_UNCACHE command in response to a DataNode
                    heartbeat.  This parameter controls how frequently the NameNode will
                    resend these commands.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.fsdatasetcache.max.threads.per.volume">dfs.datanode.fsdatasetcache.max.threads.per.volume</a></td>
                  <td>4</td>
                  <td>
                    The maximum number of threads per volume to use for caching new data
                    on the datanode. These threads consume both I/O and CPU. This can affect
                    normal datanode operations.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.cachereport.intervalMsec">dfs.cachereport.intervalMsec</a></td>
                  <td>10000</td>
                  <td>
                    Determines cache reporting interval in milliseconds.  After this amount of
                    time, the DataNode sends a full report of its cache state to the NameNode.
                    The NameNode uses the cache report to update its map of cached blocks to
                    DataNode locations.
                    This configuration has no effect if in-memory caching has been disabled by
                    setting dfs.datanode.max.locked.memory to 0 (which is the default).
                    If the native libraries are not available to the DataNode, this
                    configuration has no effect.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edit.log.autoroll.multiplier.threshold">dfs.namenode.edit.log.autoroll.multiplier.threshold</a></td>
                  <td>2.0</td>
                  <td>
                    Determines when an active namenode will roll its own edit log.
                    The actual threshold (in number of edits) is determined by multiplying
                    this value by dfs.namenode.checkpoint.txns.
                    This prevents extremely large edit files from accumulating on the active
                    namenode, which can cause timeouts during namenode startup and pose an
                    administrative hassle. This behavior is intended as a failsafe for when
                    the standby or secondary namenode fail to roll the edit log by the normal
                    checkpoint threshold.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edit.log.autoroll.check.interval.ms">dfs.namenode.edit.log.autoroll.check.interval.ms</a></td>
                  <td>300000</td>
                  <td>
                    How often an active namenode will check if it needs to roll its edit log,
                    in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.user.provider.user.pattern">dfs.webhdfs.user.provider.user.pattern</a></td>
                  <td>^[A-Za-z_][A-Za-z0-9._-]*[$]?$</td>
                  <td>
                    Valid pattern for user and group names for webhdfs, it must be a valid java regex.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.acl.provider.permission.pattern">dfs.webhdfs.acl.provider.permission.pattern</a></td>
                  <td>^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$</td>
                  <td>
                    Valid pattern for user and group names in webhdfs acl operations, it must be a valid java regex.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.socket.connect-timeout">dfs.webhdfs.socket.connect-timeout</a></td>
                  <td>60s</td>
                  <td>
                    Socket timeout for connecting to WebHDFS servers. This prevents a
                    WebHDFS client from hanging if the server hostname is
                    misconfigured, or the server does not response before the timeout
                    expires. Value is followed by a unit specifier: ns, us, ms, s, m,
                    h, d for nanoseconds, microseconds, milliseconds, seconds,
                    minutes, hours, days respectively. Values should provide units,
                    but milliseconds are assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.socket.read-timeout">dfs.webhdfs.socket.read-timeout</a></td>
                  <td>60s</td>
                  <td>
                    Socket timeout for reading data from WebHDFS servers. This
                    prevents a WebHDFS client from hanging if the server stops sending
                    data. Value is followed by a unit specifier: ns, us, ms, s, m, h,
                    d for nanoseconds, microseconds, milliseconds, seconds, minutes,
                    hours, days respectively. Values should provide units,
                    but milliseconds are assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.context">dfs.client.context</a></td>
                  <td>default</td>
                  <td>
                    The name of the DFSClient context that we should use.  Clients that share
                    a context share a socket cache and short-circuit cache, among other things.
                    You should only change this if you don't want to share with another set of
                    threads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.shortcircuit">dfs.client.read.shortcircuit</a></td>
                  <td>false</td>
                  <td>
                    This configuration parameter turns on short-circuit local reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.socket.send.buffer.size">dfs.client.socket.send.buffer.size</a></td>
                  <td>0</td>
                  <td>
                    Socket send buffer size for a write pipeline in DFSClient side.
                    This may affect TCP connection throughput.
                    If it is set to zero or negative value,
                    no buffer size will be set explicitly,
                    thus enable tcp auto-tuning on some system.
                    The default value is 0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.domain.socket.path">dfs.domain.socket.path</a></td>
                  <td></td>
                  <td>
                    Optional.  This is a path to a UNIX domain socket that will be used for
                    communication between the DataNode and local HDFS clients.
                    If the string "_PORT" is present in this path, it will be replaced by the
                    TCP port of the DataNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.shortcircuit.skip.checksum">dfs.client.read.shortcircuit.skip.checksum</a></td>
                  <td>false</td>
                  <td>
                    If this configuration parameter is set,
                    short-circuit local reads will skip checksums.
                    This is normally not recommended,
                    but it may be useful for special setups.
                    You might consider using this
                    if you are doing your own checksumming outside of HDFS.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.shortcircuit.streams.cache.size">dfs.client.read.shortcircuit.streams.cache.size</a></td>
                  <td>256</td>
                  <td>
                    The DFSClient maintains a cache of recently opened file descriptors.
                    This parameter controls the maximum number of file descriptors in the cache.
                    Setting this higher will use more file descriptors,
                    but potentially provide better performance on workloads
                    involving lots of seeks.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.shortcircuit.streams.cache.expiry.ms">dfs.client.read.shortcircuit.streams.cache.expiry.ms</a></td>
                  <td>300000</td>
                  <td>
                    This controls the minimum amount of time
                    file descriptors need to sit in the client cache context
                    before they can be closed for being inactive for too long.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.shared.file.descriptor.paths">dfs.datanode.shared.file.descriptor.paths</a></td>
                  <td>/dev/shm,/tmp</td>
                  <td>
                    Comma separated paths to the directory on which
                    shared memory segments are created.
                    The client and the DataNode exchange information via
                    this shared memory segment.
                    It tries paths in order until creation of shared memory segment succeeds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.audit.log.debug.cmdlist">dfs.namenode.audit.log.debug.cmdlist</a></td>
                  <td></td>
                  <td>
                    A comma separated list of NameNode commands that are written to the HDFS
                    namenode audit log only if the audit log level is debug.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.use.legacy.blockreader.local">dfs.client.use.legacy.blockreader.local</a></td>
                  <td>false</td>
                  <td>
                    Legacy short-circuit reader implementation based on HDFS-2246 is used
                    if this configuration parameter is true.
                    This is for the platforms other than Linux
                    where the new implementation based on HDFS-347 is not available.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.local-path-access.user">dfs.block.local-path-access.user</a></td>
                  <td></td>
                  <td>
                    Comma separated list of the users allowed to open block files
                    on legacy short-circuit local read.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.domain.socket.data.traffic">dfs.client.domain.socket.data.traffic</a></td>
                  <td>false</td>
                  <td>
                    This control whether we will try to pass normal data traffic
                    over UNIX domain socket rather than over TCP socket
                    on node-local data transfer.
                    This is currently experimental and turned off by default.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reject-unresolved-dn-topology-mapping">dfs.namenode.reject-unresolved-dn-topology-mapping</a></td>
                  <td>false</td>
                  <td>
                    If the value is set to true, then namenode will reject datanode
                    registration if the topology mapping for a datanode is not resolved and
                    NULL is returned (script defined by net.topology.script.file.name fails
                    to execute). Otherwise, datanode will be registered and the default rack
                    will be assigned as the topology path. Topology paths are important for
                    data resiliency, since they define fault domains. Thus it may be unwanted
                    behavior to allow datanode registration with the default rack if the
                    resolving topology failed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.xattrs.enabled">dfs.namenode.xattrs.enabled</a></td>
                  <td>true</td>
                  <td>
                    Whether support for extended attributes is enabled on the NameNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.max-xattrs-per-inode">dfs.namenode.fs-limits.max-xattrs-per-inode</a></td>
                  <td>32</td>
                  <td>
                    Maximum number of extended attributes per inode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fs-limits.max-xattr-size">dfs.namenode.fs-limits.max-xattr-size</a></td>
                  <td>16384</td>
                  <td>
                    The maximum combined size of the name and value of an extended attribute
                    in bytes. It should be larger than 0, and less than or equal to maximum
                    size hard limit which is 32768.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.slow.io.warning.threshold.ms">dfs.client.slow.io.warning.threshold.ms</a></td>
                  <td>30000</td>
                  <td>The threshold in milliseconds at which we will log a slow
                    io warning in a dfsclient. By default, this parameter is set to 30000
                    milliseconds (30 seconds).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.slow.io.warning.threshold.ms">dfs.datanode.slow.io.warning.threshold.ms</a></td>
                  <td>300</td>
                  <td>The threshold in milliseconds at which we will log a slow
                    io warning in a datanode. By default, this parameter is set to 300
                    milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lease-recheck-interval-ms">dfs.namenode.lease-recheck-interval-ms</a></td>
                  <td>2000</td>
                  <td>During the release of lease a lock is hold that make any
                    operations on the namenode stuck. In order to not block them during
                    a too long duration we stop releasing lease after this max lock limit.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max-lock-hold-to-release-lease-ms">dfs.namenode.max-lock-hold-to-release-lease-ms</a></td>
                  <td>25</td>
                  <td>During the release of lease a lock is hold that make any
                    operations on the namenode stuck. In order to not block them during
                    a too long duration we stop releasing lease after this max lock limit.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.write-lock-reporting-threshold-ms">dfs.namenode.write-lock-reporting-threshold-ms</a></td>
                  <td>5000</td>
                  <td>When a write lock is held on the namenode for a long time,
                    this will be logged as the lock is released. This sets how long the
                    lock must be held for logging to occur.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.read-lock-reporting-threshold-ms">dfs.namenode.read-lock-reporting-threshold-ms</a></td>
                  <td>5000</td>
                  <td>When a read lock is held on the namenode for a long time,
                    this will be logged as the lock is released. This sets how long the
                    lock must be held for logging to occur.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.lock.detailed-metrics.enabled">dfs.namenode.lock.detailed-metrics.enabled</a></td>
                  <td>false</td>
                  <td>If true, the namenode will keep track of how long various
                    operations hold the Namesystem lock for and emit this as metrics. These
                    metrics have names of the form FSN(Read|Write)LockNanosOperationName,
                    where OperationName denotes the name of the operation that initiated the
                    lock hold (this will be OTHER for certain uncategorized operations) and
                    they export the hold time values in nanoseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.fslock.fair">dfs.namenode.fslock.fair</a></td>
                  <td>true</td>
                  <td>If this is true, the FS Namesystem lock will be used in Fair mode,
                    which will help to prevent writer threads from being starved, but can provide
                    lower lock throughput. See java.util.concurrent.locks.ReentrantReadWriteLock
                    for more information on fair/non-fair locks.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.startup.delay.block.deletion.sec">dfs.namenode.startup.delay.block.deletion.sec</a></td>
                  <td>0</td>
                  <td>The delay in seconds at which we will pause the blocks deletion
                    after Namenode startup. By default it's disabled.
                    In the case a directory has large number of directories and files are
                    deleted, suggested delay is one hour to give the administrator enough time
                    to notice large number of pending deletion blocks and take corrective
                    action.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.block.id.layout.upgrade.threads">dfs.datanode.block.id.layout.upgrade.threads</a></td>
                  <td>12</td>
                  <td>The number of threads to use when creating hard links from
                    current to previous blocks during upgrade of a DataNode to block ID-based
                    block layout (see HDFS-6482 for details on the layout).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.list.encryption.zones.num.responses">dfs.namenode.list.encryption.zones.num.responses</a></td>
                  <td>100</td>
                  <td>When listing encryption zones, the maximum number of zones
                    that will be returned in a batch. Fetching the list incrementally in
                    batches improves namenode performance.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.list.reencryption.status.num.responses">dfs.namenode.list.reencryption.status.num.responses</a></td>
                  <td>100</td>
                  <td>When listing re-encryption status, the maximum number of zones
                    that will be returned in a batch. Fetching the list incrementally in
                    batches improves namenode performance.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.list.openfiles.num.responses">dfs.namenode.list.openfiles.num.responses</a></td>
                  <td>1000</td>
                  <td>
                    When listing open files, the maximum number of open files that will be
                    returned in a single batch. Fetching the list incrementally in batches
                    improves namenode performance.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edekcacheloader.interval.ms">dfs.namenode.edekcacheloader.interval.ms</a></td>
                  <td>1000</td>
                  <td>When KeyProvider is configured, the interval time of warming
                    up edek cache on NN starts up / becomes active. All edeks will be loaded
                    from KMS into provider cache. The edek cache loader will try to warm up the
                    cache until succeed or NN leaves active state.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edekcacheloader.initial.delay.ms">dfs.namenode.edekcacheloader.initial.delay.ms</a></td>
                  <td>3000</td>
                  <td>When KeyProvider is configured, the time delayed until the first
                    attempt to warm up edek cache on NN start up / become active.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reencrypt.sleep.interval">dfs.namenode.reencrypt.sleep.interval</a></td>
                  <td>1m</td>
                  <td>Interval the re-encrypt EDEK thread sleeps in the main loop. The
                    interval accepts units. If none given, millisecond is assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reencrypt.batch.size">dfs.namenode.reencrypt.batch.size</a></td>
                  <td>1000</td>
                  <td>How many EDEKs should the re-encrypt thread process in one batch.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reencrypt.throttle.limit.handler.ratio">dfs.namenode.reencrypt.throttle.limit.handler.ratio</a></td>
                  <td>1.0</td>
                  <td>Throttling ratio for the re-encryption, indicating what fraction
                    of time should the re-encrypt handler thread work under NN read lock.
                    Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are
                    invalid values and will fail NN startup.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reencrypt.throttle.limit.updater.ratio">dfs.namenode.reencrypt.throttle.limit.updater.ratio</a></td>
                  <td>1.0</td>
                  <td>Throttling ratio for the re-encryption, indicating what fraction
                    of time should the re-encrypt updater thread work under NN write lock.
                    Larger than 1.0 values are interpreted as 1.0. Negative value or 0 are
                    invalid values and will fail NN startup.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reencrypt.edek.threads">dfs.namenode.reencrypt.edek.threads</a></td>
                  <td>10</td>
                  <td>Maximum number of re-encrypt threads to contact the KMS
                    and re-encrypt the edeks.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.inotify.max.events.per.rpc">dfs.namenode.inotify.max.events.per.rpc</a></td>
                  <td>1000</td>
                  <td>Maximum number of events that will be sent to an inotify client
                    in a single RPC response. The default value attempts to amortize away
                    the overhead for this RPC while avoiding huge memory requirements for the
                    client and NameNode (1000 events should consume no more than 1 MB.)
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.user.home.dir.prefix">dfs.user.home.dir.prefix</a></td>
                  <td>/user</td>
                  <td>The directory to prepend to user name to get the user's
                    home direcotry.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.cache.revocation.timeout.ms">dfs.datanode.cache.revocation.timeout.ms</a></td>
                  <td>900000</td>
                  <td>When the DFSClient reads from a block file which the DataNode is
                    caching, the DFSClient can skip verifying checksums.  The DataNode will
                    keep the block file in cache until the client is done.  If the client takes
                    an unusually long time, though, the DataNode may need to evict the block
                    file from the cache anyway.  This value controls how long the DataNode will
                    wait for the client to release a replica that it is reading without
                    checksums.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.cache.revocation.polling.ms">dfs.datanode.cache.revocation.polling.ms</a></td>
                  <td>500</td>
                  <td>How often the DataNode should poll to see if the clients have
                    stopped using a replica that the DataNode wants to uncache.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.storage.policy.enabled">dfs.storage.policy.enabled</a></td>
                  <td>true</td>
                  <td>
                    Allow users to change the storage policy on files and directories.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.legacy-oiv-image.dir">dfs.namenode.legacy-oiv-image.dir</a></td>
                  <td></td>
                  <td>Determines where to save the namespace in the old fsimage format
                    during checkpointing by standby NameNode or SecondaryNameNode. Users can
                    dump the contents of the old format fsimage by oiv_legacy command. If
                    the value is not specified, old format fsimage will not be saved in
                    checkpoint.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.top.enabled">dfs.namenode.top.enabled</a></td>
                  <td>true</td>
                  <td>Enable nntop: reporting top users on namenode
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.top.window.num.buckets">dfs.namenode.top.window.num.buckets</a></td>
                  <td>10</td>
                  <td>Number of buckets in the rolling window implementation of nntop
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.top.num.users">dfs.namenode.top.num.users</a></td>
                  <td>10</td>
                  <td>Number of top users returned by the top tool
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.top.windows.minutes">dfs.namenode.top.windows.minutes</a></td>
                  <td>1,5,25</td>
                  <td>comma separated list of nntop reporting periods in minutes
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.ugi.expire.after.access">dfs.webhdfs.ugi.expire.after.access</a></td>
                  <td>600000</td>
                  <td>How long in milliseconds after the last access
                    the cached UGI will expire. With 0, never expire.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.blocks.per.postponedblocks.rescan">dfs.namenode.blocks.per.postponedblocks.rescan</a></td>
                  <td>10000</td>
                  <td>Number of blocks to rescan for each iteration of
                    postponedMisreplicatedBlocks.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.block-pinning.enabled">dfs.datanode.block-pinning.enabled</a></td>
                  <td>false</td>
                  <td>Whether pin blocks on favored DataNode.</td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.locateFollowingBlock.initial.delay.ms">dfs.client.block.write.locateFollowingBlock.initial.delay.ms</a></td>
                  <td>400</td>
                  <td>The initial delay (unit is ms) for locateFollowingBlock,
                    the delay time will increase exponentially(double) for each retry.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.zkfc.nn.http.timeout.ms">dfs.ha.zkfc.nn.http.timeout.ms</a></td>
                  <td>20000</td>
                  <td>
                    The HTTP connection and read timeout value (unit is ms ) when DFS ZKFC
                    tries to get local NN thread dump after local NN becomes
                    SERVICE_NOT_RESPONDING or SERVICE_UNHEALTHY.
                    If it is set to zero, DFS ZKFC won't get local NN thread dump.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.tail-edits.in-progress">dfs.ha.tail-edits.in-progress</a></td>
                  <td>false</td>
                  <td>
                    Whether enable standby namenode to tail in-progress edit logs.
                    Clients might want to turn it on when they want Standby NN to have
                    more up-to-date data.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.ec.system.default.policy">dfs.namenode.ec.system.default.policy</a></td>
                  <td>RS-6-3-1024k</td>
                  <td>The default erasure coding policy name will be used
                    on the path if no policy name is passed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.ec.policies.max.cellsize">dfs.namenode.ec.policies.max.cellsize</a></td>
                  <td>4194304</td>
                  <td>The maximum cell size of erasure coding policy. Default is 4MB.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ec.reconstruction.stripedread.timeout.millis">dfs.datanode.ec.reconstruction.stripedread.timeout.millis</a></td>
                  <td>5000</td>
                  <td>Datanode striped read timeout in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ec.reconstruction.stripedread.buffer.size">dfs.datanode.ec.reconstruction.stripedread.buffer.size</a></td>
                  <td>65536</td>
                  <td>Datanode striped read buffer size.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ec.reconstruction.threads">dfs.datanode.ec.reconstruction.threads</a></td>
                  <td>8</td>
                  <td>
                    Number of threads used by the Datanode for background
                    reconstruction work.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ec.reconstruction.xmits.weight">dfs.datanode.ec.reconstruction.xmits.weight</a></td>
                  <td>0.5</td>
                  <td>
                    Datanode uses xmits weight to calculate the relative cost of EC recovery
                    tasks comparing to replicated block recovery, of which xmits is always 1.
                    Namenode then uses xmits reported from datanode to throttle recovery tasks
                    for EC and replicated blocks.
                    The xmits of an erasure coding recovery task is calculated as the maximum
                    value between the number of read streams and the number of write streams.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.quota.init-threads">dfs.namenode.quota.init-threads</a></td>
                  <td>4</td>
                  <td>
                    The number of concurrent threads to be used in quota initialization. The
                    speed of quota initialization also affects the namenode fail-over latency.
                    If the size of name space is big, try increasing this.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.transfer.socket.send.buffer.size">dfs.datanode.transfer.socket.send.buffer.size</a></td>
                  <td>0</td>
                  <td>
                    Socket send buffer size for DataXceiver (mirroring packets to downstream
                    in pipeline). This may affect TCP connection throughput.
                    If it is set to zero or negative value, no buffer size will be set
                    explicitly, thus enable tcp auto-tuning on some system.
                    The default value is 0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.transfer.socket.recv.buffer.size">dfs.datanode.transfer.socket.recv.buffer.size</a></td>
                  <td>0</td>
                  <td>
                    Socket receive buffer size for DataXceiver (receiving packets from client
                    during block writing). This may affect TCP connection throughput.
                    If it is set to zero or negative value, no buffer size will be set
                    explicitly, thus enable tcp auto-tuning on some system.
                    The default value is 0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.upgrade.domain.factor">dfs.namenode.upgrade.domain.factor</a></td>
                  <td>${dfs.replication}</td>
                  <td>
                    This is valid only when block placement policy is set to
                    BlockPlacementPolicyWithUpgradeDomain. It defines the number of
                    unique upgrade domains any block's replicas should have.
                    When the number of replicas is less or equal to this value, the policy
                    ensures each replica has an unique upgrade domain. When the number of
                    replicas is greater than this value, the policy ensures the number of
                    unique domains is at least this value.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.zkfc.port">dfs.ha.zkfc.port</a></td>
                  <td>8019</td>
                  <td>
                    RPC port for Zookeeper Failover Controller.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.bp-ready.timeout">dfs.datanode.bp-ready.timeout</a></td>
                  <td>20s</td>
                  <td>
                    The maximum wait time for datanode to be ready before failing the
                    received request. Setting this to 0 fails requests right away if the
                    datanode is not yet registered with the namenode. This wait time
                    reduces initial request failures after datanode restart.
                    Support multiple time unit suffix(case insensitive), as described
                    in dfs.heartbeat.interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.cached-dfsused.check.interval.ms">dfs.datanode.cached-dfsused.check.interval.ms</a></td>
                  <td>600000</td>
                  <td>
                    The interval check time of loading DU_CACHE_FILE in each volume.
                    When the cluster doing the rolling upgrade operations, it will
                    usually lead dfsUsed cache file of each volume expired and redo the
                    du operations in datanode and that makes datanode start slowly. Adjust
                    this property can make cache file be available for the time as you want.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.rest-csrf.enabled">dfs.webhdfs.rest-csrf.enabled</a></td>
                  <td>false</td>
                  <td>
                    If true, then enables WebHDFS protection against cross-site request forgery
                    (CSRF).  The WebHDFS client also uses this property to determine whether or
                    not it needs to send the custom CSRF prevention header in its HTTP requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.rest-csrf.custom-header">dfs.webhdfs.rest-csrf.custom-header</a></td>
                  <td>X-XSRF-HEADER</td>
                  <td>
                    The name of a custom header that HTTP requests must send when protection
                    against cross-site request forgery (CSRF) is enabled for WebHDFS by setting
                    dfs.webhdfs.rest-csrf.enabled to true.  The WebHDFS client also uses this
                    property to determine whether or not it needs to send the custom CSRF
                    prevention header in its HTTP requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.rest-csrf.methods-to-ignore">dfs.webhdfs.rest-csrf.methods-to-ignore</a></td>
                  <td>GET,OPTIONS,HEAD,TRACE</td>
                  <td>
                    A comma-separated list of HTTP methods that do not require HTTP requests to
                    include a custom header when protection against cross-site request forgery
                    (CSRF) is enabled for WebHDFS by setting dfs.webhdfs.rest-csrf.enabled to
                    true.  The WebHDFS client also uses this property to determine whether or
                    not it needs to send the custom CSRF prevention header in its HTTP requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.rest-csrf.browser-useragents-regex">dfs.webhdfs.rest-csrf.browser-useragents-regex</a></td>
                  <td>^Mozilla.*,^Opera.*</td>
                  <td>
                    A comma-separated list of regular expressions used to match against an HTTP
                    request's User-Agent header when protection against cross-site request
                    forgery (CSRF) is enabled for WebHDFS by setting
                    dfs.webhdfs.reset-csrf.enabled to true.  If the incoming User-Agent matches
                    any of these regular expressions, then the request is considered to be sent
                    by a browser, and therefore CSRF prevention is enforced.  If the request's
                    User-Agent does not match any of these regular expressions, then the request
                    is considered to be sent by something other than a browser, such as scripted
                    automation.  In this case, CSRF is not a potential attack vector, so
                    the prevention is not enforced.  This helps achieve backwards-compatibility
                    with existing automation that has not been updated to send the CSRF
                    prevention header.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.xframe.enabled">dfs.xframe.enabled</a></td>
                  <td>true</td>
                  <td>
                    If true, then enables protection against clickjacking by returning
                    X_FRAME_OPTIONS header value set to SAMEORIGIN.
                    Clickjacking protection prevents an attacker from using transparent or
                    opaque layers to trick a user into clicking on a button
                    or link on another page.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.xframe.value">dfs.xframe.value</a></td>
                  <td>SAMEORIGIN</td>
                  <td>
                    This configration value allows user to specify the value for the
                    X-FRAME-OPTIONS. The possible values for this field are
                    DENY, SAMEORIGIN and ALLOW-FROM. Any other value will throw an
                    exception when namenode and datanodes are starting up.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.keytab.enabled">dfs.balancer.keytab.enabled</a></td>
                  <td>false</td>
                  <td>
                    Set to true to enable login using a keytab for Kerberized Hadoop.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.address">dfs.balancer.address</a></td>
                  <td>0.0.0.0:0</td>
                  <td>
                    The hostname used for a keytab based Kerberos login. Keytab based login
                    can be enabled with dfs.balancer.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.keytab.file">dfs.balancer.keytab.file</a></td>
                  <td></td>
                  <td>
                    The keytab file used by the Balancer to login as its
                    service principal. The principal name is configured with
                    dfs.balancer.kerberos.principal. Keytab based login can be
                    enabled with dfs.balancer.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.kerberos.principal">dfs.balancer.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The Balancer principal. This is typically set to
                    balancer/_HOST@REALM.TLD. The Balancer will substitute _HOST with its
                    own fully qualified hostname at startup. The _HOST placeholder
                    allows using the same configuration setting on different servers.
                    Keytab based login can be enabled with dfs.balancer.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.retry.policy.enabled">dfs.http.client.retry.policy.enabled</a></td>
                  <td>false</td>
                  <td>
                    If "true", enable the retry policy of WebHDFS client.
                    If "false", retry policy is turned off.
                    Enabling the retry policy can be quite useful while using WebHDFS to
                    copy large files between clusters that could timeout, or
                    copy files between HA clusters that could failover during the copy.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.retry.policy.spec">dfs.http.client.retry.policy.spec</a></td>
                  <td>10000,6,60000,10</td>
                  <td>
                    Specify a policy of multiple linear random retry for WebHDFS client,
                    e.g. given pairs of number of retries and sleep time (n0, t0), (n1, t1),
                    ..., the first n0 retries sleep t0 milliseconds on average,
                    the following n1 retries sleep t1 milliseconds on average, and so on.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.failover.max.attempts">dfs.http.client.failover.max.attempts</a></td>
                  <td>15</td>
                  <td>
                    Specify the max number of failover attempts for WebHDFS client
                    in case of network exception.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.retry.max.attempts">dfs.http.client.retry.max.attempts</a></td>
                  <td>10</td>
                  <td>
                    Specify the max number of retry attempts for WebHDFS client,
                    if the difference between retried attempts and failovered attempts is
                    larger than the max number of retry attempts, there will be no more
                    retries.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.failover.sleep.base.millis">dfs.http.client.failover.sleep.base.millis</a></td>
                  <td>500</td>
                  <td>
                    Specify the base amount of time in milliseconds upon which the
                    exponentially increased sleep time between retries or failovers
                    is calculated for WebHDFS client.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.http.client.failover.sleep.max.millis">dfs.http.client.failover.sleep.max.millis</a></td>
                  <td>15000</td>
                  <td>
                    Specify the upper bound of sleep time in milliseconds between
                    retries or failovers for WebHDFS client.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.hosts.provider.classname">dfs.namenode.hosts.provider.classname</a></td>
                  <td>org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager</td>
                  <td>
                    The class that provides access for host files.
                    org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager is used
                    by default which loads files specified by dfs.hosts and dfs.hosts.exclude.
                    If org.apache.hadoop.hdfs.server.blockmanagement.CombinedHostFileManager is
                    used, it will load the JSON file defined in dfs.hosts.
                    To change class name, nn restart is required. "dfsadmin -refreshNodes" only
                    refreshes the configuration files used by the class.
                  </td>
                </tr>
                <tr>
                  <td><a name="datanode.https.port">datanode.https.port</a></td>
                  <td>50475</td>
                  <td>
                    HTTPS port for DataNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.dispatcherThreads">dfs.balancer.dispatcherThreads</a></td>
                  <td>200</td>
                  <td>
                    Size of the thread pool for the HDFS balancer block mover.
                    dispatchExecutor
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.movedWinWidth">dfs.balancer.movedWinWidth</a></td>
                  <td>5400000</td>
                  <td>
                    Window of time in ms for the HDFS balancer tracking blocks and its
                    locations.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.moverThreads">dfs.balancer.moverThreads</a></td>
                  <td>1000</td>
                  <td>
                    Thread pool size for executing block moves.
                    moverThreadAllocator
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.max-size-to-move">dfs.balancer.max-size-to-move</a></td>
                  <td>10737418240</td>
                  <td>
                    Maximum number of bytes that can be moved by the balancer in a single
                    thread.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.getBlocks.min-block-size">dfs.balancer.getBlocks.min-block-size</a></td>
                  <td>10485760</td>
                  <td>
                    Minimum block threshold size in bytes to ignore when fetching a source's
                    block list.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.getBlocks.size">dfs.balancer.getBlocks.size</a></td>
                  <td>2147483648</td>
                  <td>
                    Total size in bytes of Datanode blocks to get when fetching a source's
                    block list.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.block-move.timeout">dfs.balancer.block-move.timeout</a></td>
                  <td>0</td>
                  <td>
                    Maximum amount of time in milliseconds for a block to move. If this is set
                    greater than 0, Balancer will stop waiting for a block move completion
                    after this time. In typical clusters, a 3 to 5 minute timeout is reasonable.
                    If timeout happens to a large proportion of block moves, this needs to be
                    increased. It could also be that too much work is dispatched and many nodes
                    are constantly exceeding the bandwidth limit as a result. In that case,
                    other balancer parameters might need to be adjusted.
                    It is disabled (0) by default.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.balancer.max-no-move-interval">dfs.balancer.max-no-move-interval</a></td>
                  <td>60000</td>
                  <td>
                    If this specified amount of time has elapsed and no block has been moved
                    out of a source DataNode, on more effort will be made to move blocks out of
                    this DataNode in the current Balancer iteration.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.invalidate.limit">dfs.block.invalidate.limit</a></td>
                  <td>1000</td>
                  <td>
                    The maximum number of invalidate blocks sent by namenode to a datanode
                    per heartbeat deletion command. This property works with
                    "dfs.namenode.invalidate.work.pct.per.iteration" to throttle block
                    deletions.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.misreplication.processing.limit">dfs.block.misreplication.processing.limit</a></td>
                  <td>10000</td>
                  <td>
                    Maximum number of blocks to process for initializing replication queues.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.placement.ec.classname">dfs.block.placement.ec.classname</a></td>
                  <td>org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant</td>
                  <td>
                    Placement policy class for striped files.
                    Defaults to BlockPlacementPolicyRackFaultTolerant.class
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.block.replicator.classname">dfs.block.replicator.classname</a></td>
                  <td>org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault</td>
                  <td>
                    Class representing block placement policy for non-striped files.
                    There are four block placement policies currently being supported:
                    BlockPlacementPolicyDefault, BlockPlacementPolicyWithNodeGroup,
                    BlockPlacementPolicyRackFaultTolerant and BlockPlacementPolicyWithUpgradeDomain.
                    BlockPlacementPolicyDefault chooses the desired number of targets
                    for placing block replicas in a default way. BlockPlacementPolicyWithNodeGroup
                    places block replicas on environment with node-group layer. BlockPlacementPolicyRackFaultTolerant
                    places the replicas to more racks.
                    BlockPlacementPolicyWithUpgradeDomain places block replicas that honors upgrade domain policy.
                    The details of placing replicas are documented in the javadoc of the corresponding policy classes.
                    The default policy is BlockPlacementPolicyDefault, and the corresponding class is
                    org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.blockreport.incremental.intervalMsec">dfs.blockreport.incremental.intervalMsec</a></td>
                  <td>0</td>
                  <td>
                    If set to a positive integer, the value in ms to wait between sending
                    incremental block reports from the Datanode to the Namenode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.checksum.type">dfs.checksum.type</a></td>
                  <td>CRC32C</td>
                  <td>
                    Checksum type
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.block.write.locateFollowingBlock.retries">dfs.client.block.write.locateFollowingBlock.retries</a></td>
                  <td>5</td>
                  <td>
                    Number of retries to use when finding the next block during HDFS writes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.failover.proxy.provider">dfs.client.failover.proxy.provider</a></td>
                  <td></td>
                  <td>
                    The prefix (plus a required nameservice ID) for the class name of the
                    configured Failover proxy provider for the host.  For more detailed
                    information, please consult the "Configuration Details" section of
                    the HDFS High Availability documentation.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.key.provider.cache.expiry">dfs.client.key.provider.cache.expiry</a></td>
                  <td>864000000</td>
                  <td>
                    DFS client security key cache expiration in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.max.block.acquire.failures">dfs.client.max.block.acquire.failures</a></td>
                  <td>3</td>
                  <td>
                    Maximum failures allowed when trying to get block information from a specific datanode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.prefetch.size">dfs.client.read.prefetch.size</a></td>
                  <td></td>
                  <td>
                    The number of bytes for the DFSClient will fetch from the Namenode
                    during a read operation.  Defaults to 10 * ${dfs.blocksize}.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.short.circuit.replica.stale.threshold.ms">dfs.client.read.short.circuit.replica.stale.threshold.ms</a></td>
                  <td>1800000</td>
                  <td>
                    Threshold in milliseconds for read entries during short-circuit local reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.shortcircuit.buffer.size">dfs.client.read.shortcircuit.buffer.size</a></td>
                  <td>1048576</td>
                  <td>
                    Buffer size in bytes for short-circuit local reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.read.striped.threadpool.size">dfs.client.read.striped.threadpool.size</a></td>
                  <td>18</td>
                  <td>
                    The maximum number of threads used for parallel reading
                    in striped layout.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.replica.accessor.builder.classes">dfs.client.replica.accessor.builder.classes</a></td>
                  <td></td>
                  <td>
                    Comma-separated classes for building ReplicaAccessor.  If the classes
                    are specified, client will use external BlockReader that uses the
                    ReplicaAccessor built by the builder.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.interval-ms.get-last-block-length">dfs.client.retry.interval-ms.get-last-block-length</a></td>
                  <td>4000</td>
                  <td>
                    Retry interval in milliseconds to wait between retries in getting
                    block lengths from the datanodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.max.attempts">dfs.client.retry.max.attempts</a></td>
                  <td>10</td>
                  <td>
                    Max retry attempts for DFSClient talking to namenodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.policy.enabled">dfs.client.retry.policy.enabled</a></td>
                  <td>false</td>
                  <td>
                    If true, turns on DFSClient retry policy.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.policy.spec">dfs.client.retry.policy.spec</a></td>
                  <td>10000,6,60000,10</td>
                  <td>
                    Set to pairs of timeouts and retries for DFSClient.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.times.get-last-block-length">dfs.client.retry.times.get-last-block-length</a></td>
                  <td>3</td>
                  <td>
                    Number of retries for calls to fetchLocatedBlocksAndGetLastBlockLength().
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.retry.window.base">dfs.client.retry.window.base</a></td>
                  <td>3000</td>
                  <td>
                    Base time window in ms for DFSClient retries.  For each retry attempt,
                    this value is extended linearly (e.g. 3000 ms for first attempt and
                    first retry, 6000 ms for second retry, 9000 ms for third retry, etc.).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.socket-timeout">dfs.client.socket-timeout</a></td>
                  <td>60000</td>
                  <td>
                    Default timeout value in milliseconds for all sockets.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.socketcache.capacity">dfs.client.socketcache.capacity</a></td>
                  <td>16</td>
                  <td>
                    Socket cache capacity (in entries) for short-circuit reads.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.socketcache.expiryMsec">dfs.client.socketcache.expiryMsec</a></td>
                  <td>3000</td>
                  <td>
                    Socket cache expiration for short-circuit reads in msec.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.test.drop.namenode.response.number">dfs.client.test.drop.namenode.response.number</a></td>
                  <td>0</td>
                  <td>
                    The number of Namenode responses dropped by DFSClient for each RPC call.  Used
                    for testing the NN retry cache.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.hedged.read.threadpool.size">dfs.client.hedged.read.threadpool.size</a></td>
                  <td>0</td>
                  <td>
                    Support 'hedged' reads in DFSClient. To enable this feature, set the parameter
                    to a positive number. The threadpool size is how many threads to dedicate
                    to the running of these 'hedged', concurrent reads in your client.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.hedged.read.threshold.millis">dfs.client.hedged.read.threshold.millis</a></td>
                  <td>500</td>
                  <td>
                    Configure 'hedged' reads in DFSClient. This is the number of milliseconds
                    to wait before starting up a 'hedged' read.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.byte-array-manager.count-limit">dfs.client.write.byte-array-manager.count-limit</a></td>
                  <td>2048</td>
                  <td>
                    The maximum number of arrays allowed for each array length.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.byte-array-manager.count-reset-time-period-ms">dfs.client.write.byte-array-manager.count-reset-time-period-ms</a></td>
                  <td>10000</td>
                  <td>
                    The time period in milliseconds that the allocation count for each array length is
                    reset to zero if there is no increment.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.byte-array-manager.count-threshold">dfs.client.write.byte-array-manager.count-threshold</a></td>
                  <td>128</td>
                  <td>
                    The count threshold for each array length so that a manager is created only after the
                    allocation count exceeds the threshold. In other words, the particular array length
                    is not managed until the allocation count exceeds the threshold.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.byte-array-manager.enabled">dfs.client.write.byte-array-manager.enabled</a></td>
                  <td>false</td>
                  <td>
                    If true, enables byte array manager used by DFSOutputStream.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.client.write.max-packets-in-flight">dfs.client.write.max-packets-in-flight</a></td>
                  <td>80</td>
                  <td>
                    The maximum number of DFSPackets allowed in flight.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.content-summary.limit">dfs.content-summary.limit</a></td>
                  <td>5000</td>
                  <td>
                    The maximum content summary counts allowed in one locking period. 0 or a negative number
                    means no limit (i.e. no yielding).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.content-summary.sleep-microsec">dfs.content-summary.sleep-microsec</a></td>
                  <td>500</td>
                  <td>
                    The length of time in microseconds to put the thread to sleep, between reaquiring the locks
                    in content summary computation.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.data.transfer.client.tcpnodelay">dfs.data.transfer.client.tcpnodelay</a></td>
                  <td>true</td>
                  <td>
                    If true, set TCP_NODELAY to sockets for transferring data from DFS client.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.data.transfer.server.tcpnodelay">dfs.data.transfer.server.tcpnodelay</a></td>
                  <td>true</td>
                  <td>
                    If true, set TCP_NODELAY to sockets for transferring data between Datanodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.balance.max.concurrent.moves">dfs.datanode.balance.max.concurrent.moves</a></td>
                  <td>50</td>
                  <td>
                    Maximum number of threads for Datanode balancer pending moves.  This
                    value is reconfigurable via the "dfsadmin -reconfig" command.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.fsdataset.factory">dfs.datanode.fsdataset.factory</a></td>
                  <td></td>
                  <td>
                    The class name for the underlying storage that stores replicas for a
                    Datanode.  Defaults to
                    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.fsdataset.volume.choosing.policy">dfs.datanode.fsdataset.volume.choosing.policy</a></td>
                  <td></td>
                  <td>
                    The class name of the policy for choosing volumes in the list of
                    directories.  Defaults to
                    org.apache.hadoop.hdfs.server.datanode.fsdataset.RoundRobinVolumeChoosingPolicy.
                    If you would like to take into account available disk space, set the
                    value to
                    "org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy".
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.hostname">dfs.datanode.hostname</a></td>
                  <td></td>
                  <td>
                    Optional.  The hostname for the Datanode containing this
                    configuration file.  Will be different for each machine.
                    Defaults to current hostname.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.lazywriter.interval.sec">dfs.datanode.lazywriter.interval.sec</a></td>
                  <td>60</td>
                  <td>
                    Interval in seconds for Datanodes for lazy persist writes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.network.counts.cache.max.size">dfs.datanode.network.counts.cache.max.size</a></td>
                  <td>2147483647</td>
                  <td>
                    The maximum number of entries the datanode per-host network error
                    count cache may contain.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.oob.timeout-ms">dfs.datanode.oob.timeout-ms</a></td>
                  <td>1500,0,0,0</td>
                  <td>
                    Timeout value when sending OOB response for each OOB type, which are
                    OOB_RESTART, OOB_RESERVED1, OOB_RESERVED2, and OOB_RESERVED3,
                    respectively.  Currently, only OOB_RESTART is used.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.parallel.volumes.load.threads.num">dfs.datanode.parallel.volumes.load.threads.num</a></td>
                  <td></td>
                  <td>
                    Maximum number of threads to use for upgrading data directories.
                    The default value is the number of storage directories in the
                    DataNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.ram.disk.replica.tracker">dfs.datanode.ram.disk.replica.tracker</a></td>
                  <td></td>
                  <td>
                    Name of the class implementing the RamDiskReplicaTracker interface.
                    Defaults to
                    org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.RamDiskReplicaLruTracker.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.restart.replica.expiration">dfs.datanode.restart.replica.expiration</a></td>
                  <td>50</td>
                  <td>
                    During shutdown for restart, the amount of time in seconds budgeted for
                    datanode restart.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.socket.reuse.keepalive">dfs.datanode.socket.reuse.keepalive</a></td>
                  <td>4000</td>
                  <td>
                    The window of time in ms before the DataXceiver closes a socket for a
                    single request.  If a second request occurs within that window, the
                    socket can be reused.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.socket.write.timeout">dfs.datanode.socket.write.timeout</a></td>
                  <td>480000</td>
                  <td>
                    Timeout in ms for clients socket writes to DataNodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.sync.behind.writes.in.background">dfs.datanode.sync.behind.writes.in.background</a></td>
                  <td>false</td>
                  <td>
                    If set to true, then sync_file_range() system call will occur
                    asynchronously.  This property is only valid when the property
                    dfs.datanode.sync.behind.writes is true.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.transferTo.allowed">dfs.datanode.transferTo.allowed</a></td>
                  <td>true</td>
                  <td>
                    If false, break block transfers on 32-bit machines greater than
                    or equal to 2GB into smaller chunks.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.fencing.methods">dfs.ha.fencing.methods</a></td>
                  <td></td>
                  <td>
                    A list of scripts or Java classes which will be used to fence
                    the Active NameNode during a failover.  See the HDFS High
                    Availability documentation for details on automatic HA
                    configuration.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.standby.checkpoints">dfs.ha.standby.checkpoints</a></td>
                  <td>true</td>
                  <td>
                    If true, a NameNode in Standby state periodically takes a checkpoint
                    of the namespace, saves it to its local storage and then upload to
                    the remote NameNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ha.zkfc.port">dfs.ha.zkfc.port</a></td>
                  <td>8019</td>
                  <td>
                    The port number that the zookeeper failover controller RPC
                    server binds to.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.edits.dir">dfs.journalnode.edits.dir</a></td>
                  <td>/tmp/hadoop/dfs/journalnode/</td>
                  <td>
                    The directory where the journal edit files are stored.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.enable.sync">dfs.journalnode.enable.sync</a></td>
                  <td>false</td>
                  <td>
                    If true, the journal nodes wil sync with each other. The journal nodes
                    will periodically gossip with other journal nodes to compare edit log
                    manifests and if they detect any missing log segment, they will download
                    it from the other journal nodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.sync.interval">dfs.journalnode.sync.interval</a></td>
                  <td>120000</td>
                  <td>
                    Time interval, in milliseconds, between two Journal Node syncs.
                    This configuration takes effect only if the journalnode sync is enabled
                    by setting the configuration parameter dfs.journalnode.enable.sync to true.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.kerberos.internal.spnego.principal">dfs.journalnode.kerberos.internal.spnego.principal</a></td>
                  <td></td>
                  <td>
                    Kerberos SPNEGO principal name used by the journal node.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.kerberos.principal">dfs.journalnode.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    Kerberos principal name for the journal node.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.journalnode.keytab.file">dfs.journalnode.keytab.file</a></td>
                  <td></td>
                  <td>
                    Kerberos keytab file for the journal node.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.ls.limit">dfs.ls.limit</a></td>
                  <td>1000</td>
                  <td>
                    Limit the number of files printed by ls. If less or equal to
                    zero, at most DFS_LIST_LIMIT_DEFAULT (= 1000) will be printed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.movedWinWidth">dfs.mover.movedWinWidth</a></td>
                  <td>5400000</td>
                  <td>
                    The minimum time interval, in milliseconds, that a block can be
                    moved to another location again.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.moverThreads">dfs.mover.moverThreads</a></td>
                  <td>1000</td>
                  <td>
                    Configure the balancer's mover thread pool size.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.retry.max.attempts">dfs.mover.retry.max.attempts</a></td>
                  <td>10</td>
                  <td>
                    The maximum number of retries before the mover consider the
                    move failed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.keytab.enabled">dfs.mover.keytab.enabled</a></td>
                  <td>false</td>
                  <td>
                    Set to true to enable login using a keytab for Kerberized Hadoop.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.address">dfs.mover.address</a></td>
                  <td>0.0.0.0:0</td>
                  <td>
                    The hostname used for a keytab based Kerberos login. Keytab based login
                    can be enabled with dfs.mover.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.keytab.file">dfs.mover.keytab.file</a></td>
                  <td></td>
                  <td>
                    The keytab file used by the Mover to login as its
                    service principal. The principal name is configured with
                    dfs.mover.kerberos.principal. Keytab based login can be
                    enabled with dfs.mover.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.kerberos.principal">dfs.mover.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    The Mover principal. This is typically set to
                    mover/_HOST@REALM.TLD. The Mover will substitute _HOST with its
                    own fully qualified hostname at startup. The _HOST placeholder
                    allows using the same configuration setting on different servers.
                    Keytab based login can be enabled with dfs.mover.keytab.enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.mover.max-no-move-interval">dfs.mover.max-no-move-interval</a></td>
                  <td>60000</td>
                  <td>
                    If this specified amount of time has elapsed and no block has been moved
                    out of a source DataNode, on more effort will be made to move blocks out of
                    this DataNode in the current Mover iteration.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.audit.log.async">dfs.namenode.audit.log.async</a></td>
                  <td>false</td>
                  <td>
                    If true, enables asynchronous audit log.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.audit.log.token.tracking.id">dfs.namenode.audit.log.token.tracking.id</a></td>
                  <td>false</td>
                  <td>
                    If true, adds a tracking ID for all audit log events.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction">dfs.namenode.available-space-block-placement-policy.balanced-space-preference-fraction</a></td>
                  <td>0.6</td>
                  <td>
                    Only used when the dfs.block.replicator.classname is set to
                    org.apache.hadoop.hdfs.server.blockmanagement.AvailableSpaceBlockPlacementPolicy.
                    Special value between 0 and 1, noninclusive.  Increases chance of
                    placing blocks on Datanodes with less disk space used.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.backup.dnrpc-address">dfs.namenode.backup.dnrpc-address</a></td>
                  <td></td>
                  <td>
                    Service RPC address for the backup Namenode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.delegation.token.always-use">dfs.namenode.delegation.token.always-use</a></td>
                  <td>false</td>
                  <td>
                    For testing.  Setting to true always allows the DT secret manager
                    to be used, even if security is disabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.asynclogging">dfs.namenode.edits.asynclogging</a></td>
                  <td>true</td>
                  <td>
                    If set to true, enables asynchronous edit logs in the Namenode.  If set
                    to false, the Namenode uses the traditional synchronous edit logs.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.dir.minimum">dfs.namenode.edits.dir.minimum</a></td>
                  <td>1</td>
                  <td>
                    dfs.namenode.edits.dir includes both required directories
                    (specified by dfs.namenode.edits.dir.required) and optional directories.
                    The number of usable optional directories must be greater than or equal
                    to this property.  If the number of usable optional directories falls
                    below dfs.namenode.edits.dir.minimum, HDFS will issue an error.
                    This property defaults to 1.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.edits.journal-plugin">dfs.namenode.edits.journal-plugin</a></td>
                  <td></td>
                  <td>
                    When FSEditLog is creating JournalManagers from dfs.namenode.edits.dir,
                    and it encounters a URI with a schema different to "file" it loads the
                    name of the implementing class from
                    "dfs.namenode.edits.journal-plugin.[schema]". This class must implement
                    JournalManager and have a constructor which takes (Configuration, URI).
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.file.close.num-committed-allowed">dfs.namenode.file.close.num-committed-allowed</a></td>
                  <td>0</td>
                  <td>
                    Normally a file can only be closed with all its blocks are committed.
                    When this value is set to a positive integer N, a file can be closed
                    when N blocks are committed and the rest complete.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.inode.attributes.provider.class">dfs.namenode.inode.attributes.provider.class</a></td>
                  <td></td>
                  <td>
                    Name of class to use for delegating HDFS authorization.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.inode.attributes.provider.bypass.users">dfs.namenode.inode.attributes.provider.bypass.users</a></td>
                  <td></td>
                  <td>
                    A list of user principals (in secure cluster) or user names (in insecure
                    cluster) for whom the external attributes provider will be bypassed for all
                    operations. This means file attributes stored in HDFS instead of the
                    external provider will be used for permission checking and be returned when
                    requested.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max-num-blocks-to-log">dfs.namenode.max-num-blocks-to-log</a></td>
                  <td>1000</td>
                  <td>
                    Puts a limit on the number of blocks printed to the log by the Namenode
                    after a block report.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.max.op.size">dfs.namenode.max.op.size</a></td>
                  <td>52428800</td>
                  <td>
                    Maximum opcode size in bytes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.missing.checkpoint.periods.before.shutdown">dfs.namenode.missing.checkpoint.periods.before.shutdown</a></td>
                  <td>3</td>
                  <td>
                    The number of checkpoint period windows (as defined by the property
                    dfs.namenode.checkpoint.period) allowed by the Namenode to perform
                    saving the namespace before shutdown.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.name.cache.threshold">dfs.namenode.name.cache.threshold</a></td>
                  <td>10</td>
                  <td>
                    Frequently accessed files that are accessed more times than this
                    threshold are cached in the FSDirectory nameCache.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.replication.max-streams">dfs.namenode.replication.max-streams</a></td>
                  <td>2</td>
                  <td>
                    Hard limit for the number of highest-priority replication streams.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.replication.max-streams-hard-limit">dfs.namenode.replication.max-streams-hard-limit</a></td>
                  <td>4</td>
                  <td>
                    Hard limit for all replication streams.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.reconstruction.pending.timeout-sec">dfs.namenode.reconstruction.pending.timeout-sec</a></td>
                  <td>300</td>
                  <td>
                    Timeout in seconds for block reconstruction.  If this value is 0 or less,
                    then it will default to 5 minutes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.stale.datanode.minimum.interval">dfs.namenode.stale.datanode.minimum.interval</a></td>
                  <td>3</td>
                  <td>
                    Minimum number of missed heartbeats intervals for a datanode to
                    be marked stale by the Namenode.  The actual interval is calculated as
                    (dfs.namenode.stale.datanode.minimum.interval * dfs.heartbeat.interval)
                    in seconds.  If this value is greater than the property
                    dfs.namenode.stale.datanode.interval, then the calculated value above
                    is used.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.storageinfo.defragment.timeout.ms">dfs.namenode.storageinfo.defragment.timeout.ms</a></td>
                  <td>4</td>
                  <td>
                    Timeout value in ms for the StorageInfo compaction run.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.storageinfo.defragment.interval.ms">dfs.namenode.storageinfo.defragment.interval.ms</a></td>
                  <td>600000</td>
                  <td>
                    The thread for checking the StorageInfo for defragmentation will
                    run periodically.  The time between runs is determined by this
                    property.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.storageinfo.defragment.ratio">dfs.namenode.storageinfo.defragment.ratio</a></td>
                  <td>0.75</td>
                  <td>
                    The defragmentation threshold for the StorageInfo.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.snapshot.capture.openfiles">dfs.namenode.snapshot.capture.openfiles</a></td>
                  <td>false</td>
                  <td>
                    If true, snapshots taken will have an immutable shared copy of
                    the open files that have valid leases. Even after the open files
                    grow or shrink in size, snapshot will always have the previous
                    point-in-time version of the open files, just like all other
                    closed files. Default is false.
                    Note: The file length captured for open files in snapshot is
                    whats recorded in NameNode at the time of snapshot and it may
                    be shorter than what the client has written till then. In order
                    to capture the latest length, the client can call hflush/hsync
                    with the flag SyncFlag.UPDATE_LENGTH on the open files handles.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.snapshot.skip.capture.accesstime-only-change">dfs.namenode.snapshot.skip.capture.accesstime-only-change</a></td>
                  <td>false</td>
                  <td>
                    If accessTime of a file/directory changed but there is no other
                    modification made to the file/directory, the changed accesstime will
                    not be captured in next snapshot. However, if there is other modification
                    made to the file/directory, the latest access time will be captured
                    together with the modification in next snapshot.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.namenode.snapshotdiff.allow.snap-root-descendant">dfs.namenode.snapshotdiff.allow.snap-root-descendant</a></td>
                  <td>true</td>
                  <td>
                    If enabled, snapshotDiff command can be run for any descendant directory
                    under a snapshot root directory and the diff calculation will be scoped
                    to the given descendant directory. Otherwise, snapshot diff command can
                    only be run for a snapshot root directory.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.pipeline.ecn">dfs.pipeline.ecn</a></td>
                  <td>false</td>
                  <td>
                    If true, allows ECN (explicit congestion notification) from the
                    Datanode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.accept-recovery.timeout.ms">dfs.qjournal.accept-recovery.timeout.ms</a></td>
                  <td>120000</td>
                  <td>
                    Quorum timeout in milliseconds during accept phase of
                    recovery/synchronization for a specific segment.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.finalize-segment.timeout.ms">dfs.qjournal.finalize-segment.timeout.ms</a></td>
                  <td>120000</td>
                  <td>
                    Quorum timeout in milliseconds during finalizing for a specific
                    segment.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.get-journal-state.timeout.ms">dfs.qjournal.get-journal-state.timeout.ms</a></td>
                  <td>120000</td>
                  <td>
                    Timeout in milliseconds when calling getJournalState().
                    JournalNodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.new-epoch.timeout.ms">dfs.qjournal.new-epoch.timeout.ms</a></td>
                  <td>120000</td>
                  <td>
                    Timeout in milliseconds when getting an epoch number for write
                    access to JournalNodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.prepare-recovery.timeout.ms">dfs.qjournal.prepare-recovery.timeout.ms</a></td>
                  <td>120000</td>
                  <td>
                    Quorum timeout in milliseconds during preparation phase of
                    recovery/synchronization for a specific segment.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.queued-edits.limit.mb">dfs.qjournal.queued-edits.limit.mb</a></td>
                  <td>10</td>
                  <td>
                    Queue size in MB for quorum journal edits.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.select-input-streams.timeout.ms">dfs.qjournal.select-input-streams.timeout.ms</a></td>
                  <td>20000</td>
                  <td>
                    Timeout in milliseconds for accepting streams from JournalManagers.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.start-segment.timeout.ms">dfs.qjournal.start-segment.timeout.ms</a></td>
                  <td>20000</td>
                  <td>
                    Quorum timeout in milliseconds for starting a log segment.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjournal.write-txns.timeout.ms">dfs.qjournal.write-txns.timeout.ms</a></td>
                  <td>20000</td>
                  <td>
                    Write timeout in milliseconds when writing to a quorum of remote
                    journals.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.quota.by.storage.type.enabled">dfs.quota.by.storage.type.enabled</a></td>
                  <td>true</td>
                  <td>
                    If true, enables quotas based on storage type.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.secondary.namenode.kerberos.principal">dfs.secondary.namenode.kerberos.principal</a></td>
                  <td></td>
                  <td>
                    Kerberos principal name for the Secondary NameNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.secondary.namenode.keytab.file">dfs.secondary.namenode.keytab.file</a></td>
                  <td></td>
                  <td>
                    Kerberos keytab file for the Secondary NameNode.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.web.authentication.filter">dfs.web.authentication.filter</a></td>
                  <td>org.apache.hadoop.hdfs.web.AuthFilter</td>
                  <td>
                    Authentication filter class used for WebHDFS.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.web.authentication.simple.anonymous.allowed">dfs.web.authentication.simple.anonymous.allowed</a></td>
                  <td></td>
                  <td>
                    If true, allow anonymous user to access WebHDFS. Set to
                    false to disable anonymous authentication.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.web.ugi">dfs.web.ugi</a></td>
                  <td></td>
                  <td>
                    dfs.web.ugi is deprecated. Use hadoop.http.staticuser.user instead.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.netty.high.watermark">dfs.webhdfs.netty.high.watermark</a></td>
                  <td>65535</td>
                  <td>
                    High watermark configuration to Netty for Datanode WebHdfs.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.netty.low.watermark">dfs.webhdfs.netty.low.watermark</a></td>
                  <td>32768</td>
                  <td>
                    Low watermark configuration to Netty for Datanode WebHdfs.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.oauth2.access.token.provider">dfs.webhdfs.oauth2.access.token.provider</a></td>
                  <td></td>
                  <td>
                    Access token provider class for WebHDFS using OAuth2.
                    Defaults to org.apache.hadoop.hdfs.web.oauth2.ConfCredentialBasedAccessTokenProvider.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.oauth2.client.id">dfs.webhdfs.oauth2.client.id</a></td>
                  <td></td>
                  <td>
                    Client id used to obtain access token with either credential or
                    refresh token.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.oauth2.enabled">dfs.webhdfs.oauth2.enabled</a></td>
                  <td>false</td>
                  <td>
                    If true, enables OAuth2 in WebHDFS
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.oauth2.refresh.url">dfs.webhdfs.oauth2.refresh.url</a></td>
                  <td></td>
                  <td>
                    URL against which to post for obtaining bearer token with
                    either credential or refresh token.
                  </td>
                </tr>
                <tr>
                  <td><a name="ssl.server.keystore.keypassword">ssl.server.keystore.keypassword</a></td>
                  <td></td>
                  <td>
                    Keystore key password for HTTPS SSL configuration
                  </td>
                </tr>
                <tr>
                  <td><a name="ssl.server.keystore.location">ssl.server.keystore.location</a></td>
                  <td></td>
                  <td>
                    Keystore location for HTTPS SSL configuration
                  </td>
                </tr>
                <tr>
                  <td><a name="ssl.server.keystore.password">ssl.server.keystore.password</a></td>
                  <td></td>
                  <td>
                    Keystore password for HTTPS SSL configuration
                  </td>
                </tr>
                <tr>
                  <td><a name="ssl.server.truststore.location">ssl.server.truststore.location</a></td>
                  <td></td>
                  <td>
                    Truststore location for HTTPS SSL configuration
                  </td>
                </tr>
                <tr>
                  <td><a name="ssl.server.truststore.password">ssl.server.truststore.password</a></td>
                  <td></td>
                  <td>
                    Truststore password for HTTPS SSL configuration
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.disk.balancer.max.disk.throughputInMBperSec">dfs.disk.balancer.max.disk.throughputInMBperSec</a></td>
                  <td>10</td>
                  <td>Maximum disk bandwidth used by diskbalancer
                    during read from a source disk. The unit is MB/sec.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.disk.balancer.block.tolerance.percent">dfs.disk.balancer.block.tolerance.percent</a></td>
                  <td>10</td>
                  <td>
                    When a disk balancer copy operation is proceeding, the datanode is still
                    active. So it might not be possible to move the exactly specified
                    amount of data. So tolerance allows us to define a percentage which
                    defines a good enough move.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.disk.balancer.max.disk.errors">dfs.disk.balancer.max.disk.errors</a></td>
                  <td>5</td>
                  <td>
                    During a block move from a source to destination disk, we might
                    encounter various errors. This defines how many errors we can tolerate
                    before we declare a move between 2 disks (or a step) has failed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.disk.balancer.enabled">dfs.disk.balancer.enabled</a></td>
                  <td>false</td>
                  <td>
                    This enables the diskbalancer feature on a cluster. By default, disk
                    balancer is disabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.disk.balancer.plan.threshold.percent">dfs.disk.balancer.plan.threshold.percent</a></td>
                  <td>10</td>
                  <td>
                    The percentage threshold value for volume Data Density in a plan.
                    If the absolute value of volume Data Density which is out of
                    threshold value in a node, it means that the volumes corresponding to
                    the disks should do the balancing in the plan. The default value is 10.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.lock.suppress.warning.interval">dfs.lock.suppress.warning.interval</a></td>
                  <td>10s</td>
                  <td>Instrumentation reporting long critical sections will suppress
                    consecutive warnings within this interval.
                  </td>
                </tr>
                <tr>
                  <td><a name="httpfs.buffer.size">httpfs.buffer.size</a></td>
                  <td>4096</td>
                  <td>
                    The size buffer to be used when creating or opening httpfs filesystem IO stream.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.webhdfs.use.ipc.callq">dfs.webhdfs.use.ipc.callq</a></td>
                  <td>true</td>
                  <td>Enables routing of webhdfs calls through rpc
                    call queue
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.disk.check.min.gap">dfs.datanode.disk.check.min.gap</a></td>
                  <td>15m</td>
                  <td>
                    The minimum gap between two successive checks of the same DataNode
                    volume. This setting supports multiple time unit suffixes as described
                    in dfs.heartbeat.interval. If no suffix is specified then milliseconds
                    is assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.datanode.disk.check.timeout">dfs.datanode.disk.check.timeout</a></td>
                  <td>10m</td>
                  <td>
                    Maximum allowed time for a disk check to complete during DataNode
                    startup. If the check does not complete within this time interval
                    then the disk is declared as failed. This setting supports
                    multiple time unit suffixes as described in dfs.heartbeat.interval.
                    If no suffix is specified then milliseconds is assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.use.dfs.network.topology">dfs.use.dfs.network.topology</a></td>
                  <td>true</td>
                  <td>
                    Enables DFSNetworkTopology to choose nodes for placing replicas.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.qjm.operations.timeout">dfs.qjm.operations.timeout</a></td>
                  <td>60s</td>
                  <td>
                    Common key to set timeout for related operations in
                    QuorumJournalManager. This setting supports multiple time unit suffixes
                    as described in dfs.heartbeat.interval.
                    If no suffix is specified then milliseconds is assumed.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.reformat.disabled">dfs.reformat.disabled</a></td>
                  <td>false</td>
                  <td>
                    Disable reformat of NameNode. If it's value is set to "true"
                    and metadata directories already exist then attempt to format NameNode
                    will throw NameNodeFormatException.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.default.nameserviceId">dfs.federation.router.default.nameserviceId</a></td>
                  <td></td>
                  <td>
                    Nameservice identifier of the default subcluster to monitor.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.rpc.enable">dfs.federation.router.rpc.enable</a></td>
                  <td>true</td>
                  <td>
                    If true, the RPC service to handle client requests in the router is
                    enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.rpc-address">dfs.federation.router.rpc-address</a></td>
                  <td>0.0.0.0:8888</td>
                  <td>
                    RPC address that handles all clients requests.
                    The value of this property will take the form of router-host1:rpc-port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.rpc-bind-host">dfs.federation.router.rpc-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the RPC server will bind to. If this optional address is
                    set, it overrides only the hostname portion of
                    dfs.federation.router.rpc-address. This is useful for making the name node
                    listen on all interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.handler.count">dfs.federation.router.handler.count</a></td>
                  <td>10</td>
                  <td>
                    The number of server threads for the router to handle RPC requests from
                    clients.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.handler.queue.size">dfs.federation.router.handler.queue.size</a></td>
                  <td>100</td>
                  <td>
                    The size of the queue for the number of handlers to handle RPC client requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.reader.count">dfs.federation.router.reader.count</a></td>
                  <td>1</td>
                  <td>
                    The number of readers for the router to handle RPC client requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.reader.queue.size">dfs.federation.router.reader.queue.size</a></td>
                  <td>100</td>
                  <td>
                    The size of the queue for the number of readers for the router to handle RPC client requests.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.connection.pool-size">dfs.federation.router.connection.pool-size</a></td>
                  <td>1</td>
                  <td>
                    Size of the pool of connections from the router to namenodes.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.connection.clean.ms">dfs.federation.router.connection.clean.ms</a></td>
                  <td>10000</td>
                  <td>
                    Time interval, in milliseconds, to check if the connection pool should
                    remove unused connections.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.connection.pool.clean.ms">dfs.federation.router.connection.pool.clean.ms</a></td>
                  <td>60000</td>
                  <td>
                    Time interval, in milliseconds, to check if the connection manager should
                    remove unused connection pools.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.metrics.enable">dfs.federation.router.metrics.enable</a></td>
                  <td>true</td>
                  <td>
                    If the metrics in the router are enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.metrics.class">dfs.federation.router.metrics.class</a></td>
                  <td>org.apache.hadoop.hdfs.server.federation.metrics.FederationRPCPerformanceMonitor</td>
                  <td>
                    Class to monitor the RPC system in the router. It must implement the
                    RouterRpcMonitor interface.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.admin.enable">dfs.federation.router.admin.enable</a></td>
                  <td>true</td>
                  <td>
                    If true, the RPC admin service to handle client requests in the router is
                    enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.admin-address">dfs.federation.router.admin-address</a></td>
                  <td>0.0.0.0:8111</td>
                  <td>
                    RPC address that handles the admin requests.
                    The value of this property will take the form of router-host1:rpc-port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.admin-bind-host">dfs.federation.router.admin-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the RPC admin server will bind to. If this optional
                    address is set, it overrides only the hostname portion of
                    dfs.federation.router.admin-address. This is useful for making the name
                    node listen on all interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.admin.handler.count">dfs.federation.router.admin.handler.count</a></td>
                  <td>1</td>
                  <td>
                    The number of server threads for the router to handle RPC requests from
                    admin.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.http-address">dfs.federation.router.http-address</a></td>
                  <td>0.0.0.0:50071</td>
                  <td>
                    HTTP address that handles the web requests to the Router.
                    The value of this property will take the form of router-host1:http-port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.http-bind-host">dfs.federation.router.http-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the HTTP server will bind to. If this optional
                    address is set, it overrides only the hostname portion of
                    dfs.federation.router.http-address. This is useful for making the name
                    node listen on all interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.https-address">dfs.federation.router.https-address</a></td>
                  <td>0.0.0.0:50072</td>
                  <td>
                    HTTPS address that handles the web requests to the Router.
                    The value of this property will take the form of router-host1:https-port.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.https-bind-host">dfs.federation.router.https-bind-host</a></td>
                  <td></td>
                  <td>
                    The actual address the HTTPS server will bind to. If this optional
                    address is set, it overrides only the hostname portion of
                    dfs.federation.router.https-address. This is useful for making the name
                    node listen on all interfaces by setting it to 0.0.0.0.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.http.enable">dfs.federation.router.http.enable</a></td>
                  <td>true</td>
                  <td>
                    If the HTTP service to handle client requests in the router is enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.metrics.enable">dfs.federation.router.metrics.enable</a></td>
                  <td>true</td>
                  <td>
                    If the metrics service in the router is enabled.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.file.resolver.client.class">dfs.federation.router.file.resolver.client.class</a></td>
                  <td>org.apache.hadoop.hdfs.server.federation.MockResolver</td>
                  <td>
                    Class to resolve files to subclusters.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.namenode.resolver.client.class">dfs.federation.router.namenode.resolver.client.class</a></td>
                  <td>org.apache.hadoop.hdfs.server.federation.resolver.MembershipNamenodeResolver</td>
                  <td>
                    Class to resolve the namenode for a subcluster.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.store.enable">dfs.federation.router.store.enable</a></td>
                  <td>true</td>
                  <td>
                    If true, the Router connects to the State Store.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.store.serializer">dfs.federation.router.store.serializer</a></td>
                  <td>org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreSerializerPBImpl</td>
                  <td>
                    Class to serialize State Store records.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.store.driver.class">dfs.federation.router.store.driver.class</a></td>
                  <td>org.apache.hadoop.hdfs.server.federation.store.driver.impl.StateStoreFileImpl</td>
                  <td>
                    Class to implement the State Store. By default it uses the local disk.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.store.connection.test">dfs.federation.router.store.connection.test</a></td>
                  <td>60000</td>
                  <td>
                    How often to check for the connection to the State Store in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.cache.ttl">dfs.federation.router.cache.ttl</a></td>
                  <td>60000</td>
                  <td>
                    How often to refresh the State Store caches in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.store.membership.expiration">dfs.federation.router.store.membership.expiration</a></td>
                  <td>300000</td>
                  <td>
                    Expiration time in milliseconds for a membership record.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.heartbeat.enable">dfs.federation.router.heartbeat.enable</a></td>
                  <td>true</td>
                  <td>
                    If true, the Router heartbeats into the State Store.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.heartbeat.interval">dfs.federation.router.heartbeat.interval</a></td>
                  <td>5000</td>
                  <td>
                    How often the Router should heartbeat into the State Store in milliseconds.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.monitor.namenode">dfs.federation.router.monitor.namenode</a></td>
                  <td></td>
                  <td>
                    The identifier of the namenodes to monitor and heartbeat.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.federation.router.monitor.localnamenode.enable">dfs.federation.router.monitor.localnamenode.enable</a></td>
                  <td>true</td>
                  <td>
                    If true, the Router should monitor the namenode in the local machine.
                  </td>
                </tr>
                <tr>
                  <td><a name="dfs.reformat.disabled">dfs.reformat.disabled</a></td>
                  <td>false</td>
                  <td>
                    Disable reformat of NameNode. If it's value is set to "true"
                    and metadata directories already exist then attempt to format NameNode
                    will throw NameNodeFormatException.
                  </td>
                </tr>
              </tbody>
            </table>
          </section>
          <section>
            <div id="bodyColumn">
              <div id="contentBox">
                <!---
                  Licensed under the Apache License, Version 2.0 (the "License");
                  you may not use this file except in compliance with the License.
                  You may obtain a copy of the License at

                   http://www.apache.org/licenses/LICENSE-2.0

                  Unless required by applicable law or agreed to in writing, software
                  distributed under the License is distributed on an "AS IS" BASIS,
                  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
                  See the License for the specific language governing permissions and
                  limitations under the License. See accompanying LICENSE file.
                  -->
                <h1>The YARN Timeline Server</h1>
                <ul>
                  <li><a href="#Overview">Overview</a></li>
                  <li><a href="#Deployment">Deployment</a></li>
                  <li><a href="#Timeline_Server_REST_API_V1">Timeline Server REST API V1</a></li>
                  <li><a href="#Domains_.2Fws.2Fv1.2Ftimeline.2Fdomain">Domains /ws/v1/timeline/domain</a></li>
                  <li><a href="#Posting_Timeline_Entities"> Posting Timeline Entities</a></li>
                  <li><a href="#Timeline_Entity_List">Timeline Entity List</a></li>
                  <li><a href="#Timeline_Entity">Timeline Entity</a></li>
                  <li><a href="#Timeline_Event_List">Timeline Event List</a></li>
                  <li><a href="#About">About</a></li>
                  <li><a href="#Application_List">Application List</a></li>
                  <li><a href="#Application">Application</a></li>
                  <li><a href="#Application_Attempt_List">Application Attempt List</a></li>
                  <li><a href="#Application_Attempt">Application Attempt</a></li>
                  <li><a href="#Container_List">Container List</a></li>
                  <li><a href="#Container">Container</a></li>
                  <li><a href="#Timeline_Server_Performance_Test_Tool"> Timeline Server Performance Test Tool</a></li>
                </ul>
                <section>
                  <h2><a name="Overview"></a>Overview</h2>
                  <section>
                    <h3><a name="Introduction"></a>Introduction</h3>
                    <p>The Storage and retrieval of application’s current and historic information in a generic fashion is addressed in YARN through the Timeline Server. It has two responsibilities:</p>
                    <section>
                      <h4><a name="Persisting_Application_Specific_Information"></a>Persisting Application Specific Information</h4>
                      <p>The collection and retrieval of information completely specific to an application or framework. For example, the Hadoop MapReduce framework can include pieces of information like number of map tasks, reduce tasks, counters…etc. Application developers can publish the specific information to the Timeline server via <code>TimelineClient</code> in the Application Master and/or the application’s containers.</p>
                      <p>This information is then queryable via REST APIs for rendering by application/framework specific UIs.</p>
                    </section>
                    <section>
                      <h4><a name="Persisting_Generic_Information_about_Completed_Applications"></a>Persisting Generic Information about Completed Applications</h4>
                      <p>Previously this was supported purely for MapReduce jobs by the Application History Server. With the introduction of the timeline server, the Application History Server becomes just one use of the Timeline Server.</p>
                      <p>Generic information includes application level data such as</p>
                      <ul>
                        <li>queue-name,</li>
                        <li>user information and the like set in the <code>ApplicationSubmissionContext</code>,</li>
                        <li>a list of application-attempts that ran for an application</li>
                        <li>information about each application-attempt</li>
                        <li>the list of containers run under each application-attempt</li>
                        <li>information about each container.</li>
                      </ul>
                      <p>Generic data is published by the YARN Resource Manager to the timeline store and used by its web-UI to display information about completed applications.</p>
                    </section>
                  </section>
                  <section>
                    <h3><a name="Current_Status_and_Future_Plans"></a><a name="Current_Status"></a>Current Status and Future Plans</h3>
                    <p>Current status</p>
                    <ol style="list-style-type: decimal">
                      <li>The core functionality of the timeline server has been completed.</li>
                      <li>It works in both secure and non secure clusters.</li>
                      <li>The generic history service is built on the timeline store.</li>
                      <li>The history can be stored in memory or in a leveldb database store; the latter ensures the history is preserved over Timeline Server restarts.</li>
                      <li>The ability to install framework specific UIs in YARN is not supported.</li>
                      <li>Application specific information is only available via RESTful APIs using JSON type content.</li>
                      <li>The “Timeline Server v1” REST API has been declared one of the REST APIs whose compatibility will be maintained in future releases.</li>
                      <li>The single-server implementation of the Timeline Server places a limit on the scalability of the service; it also prevents the service being High-Availability component of the YARN infrastructure.</li>
                    </ol>
                    <p>Future Plans</p>
                    <ol style="list-style-type: decimal">
                      <li>Future releases will introduce a next generation timeline service which is scalable and reliable, <a href="./TimelineServiceV2.html">“Timeline Service v2”</a>.</li>
                      <li>The expanded features of this service <i>may not</i> be available to applications using the Timeline Server v1 REST API. That includes extended data structures as well as the ability of the client to failover between Timeline Server instances.</li>
                    </ol>
                  </section>
                  <section>
                    <h3><a name="Timeline_Structure"></a>Timeline Structure</h3>
                    <p><img src="./images/timeline_structure.jpg" alt="Timeline Structure"></p>
                    <section>
                      <h4><a name="Timeline_Domain"></a>Timeline Domain</h4>
                      <p>The Timeline Domain offers a namespace for Timeline server allowing users to host multiple entities, isolating them from other users and applications. Timeline server Security is defined at this level.</p>
                      <p>A “Domain” primarily stores owner info, read and&amp; write ACL information, created and modified time stamp information. Each Domain is identified by an ID which must be unique across all users in the YARN cluster.</p>
                    </section>
                    <section>
                      <h4><a name="Timeline_Entity"></a>Timeline Entity</h4>
                      <p>A Timeline Entity contains the the meta information of a conceptual entity and its related events.</p>
                      <p>The entity can be an application, an application attempt, a container or any user-defined object.</p>
                      <p>It contains <b>Primary filters</b> which will be used to index the entities in the Timeline Store. Accordingly, users/applications should carefully choose the information they want to store as the primary filters.</p>
                      <p>The remaining data can be stored as unindexed information. Each Entity is uniquely identified by an <code>EntityId</code> and <code>EntityType</code>.</p>
                    </section>
                    <section>
                      <h4><a name="Timeline_Events"></a>Timeline Events</h4>
                      <p>A Timeline Event describes an event that is related to a specific Timeline Entity of an application.</p>
                      <p>Users are free to define what an event means —such as starting an application, getting allocated a container, an operation failures or other information considered relevant to users and cluster operators.</p>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Deployment"></a>Deployment</h2>
                  <section>
                    <h3><a name="Configurations"></a>Configurations</h3>
                    <section>
                      <h4><a name="Basic_Configuration"></a>Basic Configuration</h4>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.enabled</code> </td>
                            <td align="left"> In the server side it indicates whether timeline service is enabled or not. And in the client side, users can enable it to indicate whether client wants to use timeline service. If it’s enabled in the client side along with security, then yarn client tries to fetch the delegation tokens for the timeline server. Defaults to <code>false</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.resourcemanager.system-metrics-publisher.enabled</code> </td>
                            <td align="left"> The setting that controls whether or not YARN system metrics are published on the timeline server by RM. Defaults to <code>false</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.generic-application-history.enabled</code> </td>
                            <td align="left"> Indicate to clients whether to query generic application data from timeline history-service or not. If not enabled then application data is queried only from Resource Manager. Defaults to <code>false</code>. </td>
                          </tr>
                        </tbody>
                      </table>
                    </section>
                    <section>
                      <h4><a name="Timeline_store_and_state_store_configuration"></a>Timeline store and state store configuration</h4>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.store-class</code> </td>
                            <td align="left"> Store class name for timeline store. Defaults to <code>org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.leveldb-timeline-store.path</code> </td>
                            <td align="left"> Store file name for leveldb timeline store. Defaults to <code>${hadoop.tmp.dir}/yarn/timeline</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.leveldb-timeline-store.ttl-interval-ms</code> </td>
                            <td align="left"> Length of time to wait between deletion cycles of leveldb timeline store in milliseconds. Defaults to <code>300000</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.leveldb-timeline-store.read-cache-size</code> </td>
                            <td align="left"> Size of read cache for uncompressed blocks for leveldb timeline store in bytes. Defaults to <code>104857600</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.leveldb-timeline-store.start-time-read-cache-size</code> </td>
                            <td align="left"> Size of cache for recently read entity start times for leveldb timeline store in number of entities. Defaults to <code>10000</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.leveldb-timeline-store.start-time-write-cache-size</code> </td>
                            <td align="left"> Size of cache for recently written entity start times for leveldb timeline store in number of entities. Defaults to <code>10000</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.recovery.enabled</code> </td>
                            <td align="left"> Defaults to <code>false</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.state-store-class</code> </td>
                            <td align="left"> Store class name for timeline state store. Defaults to <code>org.apache.hadoop.yarn.server.timeline.recovery.LeveldbTimelineStateStore</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.leveldb-state-store.path</code> </td>
                            <td align="left"> Store file name for leveldb timeline state store. </td>
                          </tr>
                        </tbody>
                      </table>
                    </section>
                    <section>
                      <h4><a name="Web_and_RPC_Configuration"></a>Web and RPC Configuration</h4>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.hostname</code> </td>
                            <td align="left"> The hostname of the Timeline service web application. Defaults to <code>0.0.0.0</code> </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.address</code> </td>
                            <td align="left"> Address for the Timeline server to start the RPC server. Defaults to <code>${yarn.timeline-service.hostname}:10200</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.webapp.address</code> </td>
                            <td align="left"> The http address of the Timeline service web application. Defaults to <code>${yarn.timeline-service.hostname}:8188</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.webapp.https.address</code> </td>
                            <td align="left"> The https address of the Timeline service web application. Defaults to <code>${yarn.timeline-service.hostname}:8190</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.bind-host</code> </td>
                            <td align="left"> The actual address the server will bind to. If this optional address is set, the RPC and webapp servers will bind to this address and the port specified in <code>yarn.timeline-service.address</code> and <code>yarn.timeline-service.webapp.address</code>, respectively. This is most useful for making the service listen on all interfaces by setting to <code>0.0.0.0</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.http-cross-origin.enabled</code> </td>
                            <td align="left"> Enables cross-origin support (CORS) for web services where cross-origin web response headers are needed. For example, javascript making a web services request to the timeline server. Defaults to <code>false</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.http-cross-origin.allowed-origins</code> </td>
                            <td align="left"> Comma separated list of origins that are allowed. Values prefixed with <code>regex:</code> are interpreted as regular expressions. Values containing wildcards (<code>*</code>) are possible as well, here a regular expression is generated, the use is discouraged and support is only available for backward compatibility. Defaults to <code>*</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.http-cross-origin.allowed-methods</code> </td>
                            <td align="left"> Comma separated list of methods that are allowed for web services needing cross-origin (CORS) support. Defaults to <code>GET,POST,HEAD</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.http-cross-origin.allowed-headers</code> </td>
                            <td align="left"> Comma separated list of headers that are allowed for web services needing cross-origin (CORS) support. Defaults to <code>X-Requested-With,Content-Type,Accept,Origin</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.http-cross-origin.max-age</code> </td>
                            <td align="left"> The number of seconds a pre-flighted request can be cached for web services needing cross-origin (CORS) support. Defaults to <code>1800</code>. </td>
                          </tr>
                        </tbody>
                      </table>
                      <p>Note that the selection between the HTTP and HTTPS binding is made in the <code>TimelineClient</code> based upon the value of the YARN-wide configuration option <code>yarn.http.policy</code>; the HTTPS endpoint will be selected if this policy is <code>HTTPS_ONLY</code>.</p>
                    </section>
                    <section>
                      <h4><a name="Advanced_Server-side_configuration"></a>Advanced Server-side configuration</h4>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.ttl-enable</code> </td>
                            <td align="left"> Enable deletion of aged data within the timeline store. Defaults to <code>true</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.ttl-ms</code> </td>
                            <td align="left"> Time to live for timeline store data in milliseconds. Defaults to <code>604800000</code> (7 days). </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.handler-thread-count</code> </td>
                            <td align="left"> Handler thread count to serve the client RPC requests. Defaults to <code>10</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.client.max-retries</code> </td>
                            <td align="left"> The maximum number of retries for attempts to publish data to the timeline service.Defaults to <code>30</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.client.retry-interval-ms</code> </td>
                            <td align="left"> The interval in milliseconds between retries for the timeline service client. Defaults to <code>1000</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.generic-application-history.max-applications</code> </td>
                            <td align="left"> The max number of applications could be fetched by using REST API or application history protocol and shown in timeline server web ui. Defaults to <code>10000</code>. </td>
                          </tr>
                        </tbody>
                      </table>
                    </section>
                    <section>
                      <h4><a name="UI_Hosting_Configuration"></a>UI Hosting Configuration</h4>
                      <p>The timeline service can host multiple UIs if enabled. The service can support both static web sites hosted in a directory or war files bundled. The web UI is then hosted on the timeline service HTTP port under the path configured.</p>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.ui-names</code> </td>
                            <td align="left"> Comma separated list of UIs that will be hosted. Defaults to <code>none</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.ui-on-disk-path.$name</code> </td>
                            <td align="left"> For each of the ui-names, an on disk path should be specified to the directory service static content or the location of a web archive (war file). </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.ui-web-path.$name</code> </td>
                            <td align="left"> For each of the ui-names, the web path should be specified relative to the Timeline server root. Paths should begin with a starting slash. </td>
                          </tr>
                        </tbody>
                      </table>
                    </section>
                    <section>
                      <h4><a name="Security_Configuration"></a>Security Configuration</h4>
                      <p>Security can be enabled by setting <code>yarn.timeline-service.http-authentication.type</code> to <code>kerberos</code>, after which the following configuration options are available:</p>
                      <table border="0" class="bodyTable">
                        <thead>
                          <tr class="a">
                            <th align="left"> Configuration Property </th>
                            <th align="left"> Description </th>
                          </tr>
                        </thead>
                        <tbody>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.http-authentication.type</code> </td>
                            <td align="left"> Defines authentication used for the timeline server HTTP endpoint. Supported values are: <code>simple</code> / <code>kerberos</code> / #AUTHENTICATION_HANDLER_CLASSNAME#. Defaults to <code>simple</code>. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.http-authentication.simple.anonymous.allowed</code> </td>
                            <td align="left"> Indicates if anonymous requests are allowed by the timeline server when using ‘simple’ authentication. Defaults to <code>true</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.principal</code> </td>
                            <td align="left"> The Kerberos principal for the timeline server. </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.keytab</code> </td>
                            <td align="left"> The Kerberos keytab for the timeline server. Defaults on Unix to to <code>/etc/krb5.keytab</code>. </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.delegation.key.update-interval</code> </td>
                            <td align="left"> Defaults to <code>86400000</code> (1 day). </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.delegation.token.renew-interval</code> </td>
                            <td align="left"> Defaults to <code>86400000</code> (1 day). </td>
                          </tr>
                          <tr class="b">
                            <td align="left"> <code>yarn.timeline-service.delegation.token.max-lifetime</code> </td>
                            <td align="left"> Defaults to <code>604800000</code> (7 days). </td>
                          </tr>
                          <tr class="a">
                            <td align="left"> <code>yarn.timeline-service.client.best-effort</code> </td>
                            <td align="left"> Should the failure to obtain a delegation token be considered an application failure (option = false),  or should the client attempt to continue to publish information without it (option=true). Default: <code>false</code> </td>
                          </tr>
                        </tbody>
                      </table>
                    </section>
                    <section>
                      <h4><a name="Enabling_the_timeline_service_and_the_generic_history_service"></a>Enabling the timeline service and the generic history service</h4>
                      <p>Following are the basic configuration to start Timeline server.</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;property&gt;
  &lt;description&gt;Indicate to clients whether Timeline service is enabled or not.
  If enabled, the TimelineClient library used by end-users will post entities
  and events to the Timeline server.&lt;/description&gt;
  &lt;name&gt;yarn.timeline-service.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;description&gt;The setting that controls whether yarn system metrics is
  published on the timeline server or not by RM.&lt;/description&gt;
  &lt;name&gt;yarn.resourcemanager.system-metrics-publisher.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;description&gt;Indicate to clients whether to query generic application
  data from timeline history-service or not. If not enabled then application
  data is queried only from Resource Manager.&lt;/description&gt;
  &lt;name&gt;yarn.timeline-service.generic-application-history.enabled&lt;/name&gt;
  &lt;value&gt;true&lt;/value&gt;
&lt;/property&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                  <section>
                    <h3><a name="Running_the_Timeline_Server"></a><a name="Running_Timeline_Server"></a> Running the Timeline Server</h3>
                    <p>Assuming all the aforementioned configurations are set properly admins can start the Timeline server/history service with the following command:</p>
                    <div class="source">
                      <div class="source">
                        <pre>yarn timelineserver
</pre>
                      </div>
                    </div>
                    <p>To start the Timeline server / history service as a daemon, the command is</p>
                    <div class="source">
                      <div class="source">
                        <pre>$HADOOP_YARN_HOME/sbin/yarn-daemon.sh start timelineserver
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Accessing_generic-data_via_command-line"></a> Accessing generic-data via command-line</h3>
                    <p>Users can access applications’ generic historic data via the command line below</p>
                    <div class="source">
                      <div class="source">
                        <pre>$ yarn application -status &lt;Application ID&gt;
$ yarn applicationattempt -list &lt;Application ID&gt;
$ yarn applicationattempt -status &lt;Application Attempt ID&gt;
$ yarn container -list &lt;Application Attempt ID&gt;
$ yarn container -status &lt;Container ID&gt;
</pre>
                      </div>
                    </div>
                    <p>Note that the same commands are usable to obtain the corresponding information about running applications.</p>
                  </section>
                  <section>
                    <h3><a name="Publishing_application_specific_data"></a><a name="Publishing_of_application_specific_data"></a> Publishing application specific data</h3>
                    <p>Developers can define what information they want to record for their applications by constructing <code>TimelineEntity</code> and  <code>TimelineEvent</code> objects then publishing the entities and events to the Timeline Server via the <code>TimelineClient</code> API.</p>
                    <p>Here is an example:</p>
                    <div class="source">
                      <div class="source">
                        <pre>// Create and start the Timeline client
TimelineClient client = TimelineClient.createTimelineClient();
client.init(conf);
client.start();

try {
  TimelineDomain myDomain = new TimelineDomain();
  myDomain.setId("MyDomain");
  // Compose other Domain info ....

  client.putDomain(myDomain);

  TimelineEntity myEntity = new TimelineEntity();
  myEntity.setDomainId(myDomain.getId());
  myEntity.setEntityType("APPLICATION");
  myEntity.setEntityId("MyApp1");
  // Compose other entity info

  TimelinePutResponse response = client.putEntities(entity);

  TimelineEvent event = new TimelineEvent();
  event.setEventType("APP_FINISHED");
  event.setTimestamp(System.currentTimeMillis());
  event.addEventInfo("Exit Status", "SUCCESS");
  // Compose other Event info ....

  myEntity.addEvent(event);
  TimelinePutResponse response = timelineClient.putEntities(entity);

} catch (IOException e) {
  // Handle the exception
} catch (RuntimeException e) {
  // In Hadoop 2.6, if attempts submit information to the Timeline Server fail more than the retry limit,
  // a RuntimeException will be raised. This may change in future releases, being
  // replaced with a IOException that is (or wraps) that which triggered retry failures.
} catch (YarnException e) {
  // Handle the exception
} finally {
  // Stop the Timeline client
  client.stop();
}
</pre>
                      </div>
                    </div>
                    <ol style="list-style-type: decimal">
                      <li>Publishing of data to Timeline Server is a synchronous operation; the call will not return until successful.</li>
                      <li>The <code>TimelineClient</code> implementation class is a subclass of the YARN <code>Service</code> API; it can be placed under a <code>CompositeService</code> to ease its lifecycle management.</li>
                      <li>The result of a <code>putEntities()</code> call is a <code>TimelinePutResponse</code> object. This contains a (hopefully empty) list of those timeline entities reject by the timeline server, along with an error code indicating the cause of each failure.</li>
                    </ol>
                    <p>In Hadoop 2.6 and 2.7, the error codes are:</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Error Code </th>
                          <th align="left"> Description </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left">1 </td>
                          <td align="left"> No start time </td>
                        </tr>
                        <tr class="a">
                          <td align="left">2 </td>
                          <td align="left"> IOException </td>
                        </tr>
                        <tr class="b">
                          <td align="left">3 </td>
                          <td align="left"> System Filter conflict (reserved filter key used) </td>
                        </tr>
                        <tr class="a">
                          <td align="left">4 </td>
                          <td align="left"> Access Denied </td>
                        </tr>
                        <tr class="b">
                          <td align="left">5 </td>
                          <td align="left"> No domain </td>
                        </tr>
                        <tr class="a">
                          <td align="left">6 </td>
                          <td align="left"> Forbidden relation </td>
                        </tr>
                      </tbody>
                    </table>
                    <p>Further error codes may be defined in future.</p>
                    <p><b>Note</b> : Following are the points which need to be observed when updating a entity.</p>
                    <ul>
                      <li>Domain ID should not be modified for already existing entity.</li>
                      <li>After a modification of a Primary filter value, the new value will be appended to the old value; the original value will not be replaced.</li>
                      <li>It’s advisable to have same primary filters for all updates on entity. Any on modification of a primary filter by in an update will result in queries with updated primary filter to not fetching the information before the update</li>
                    </ul>
                    <h1>Generic Data Web UI</h1>
                    <p>Users can access the generic historic information of applications via web UI:</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/applicationhistory
</pre>
                      </div>
                    </div>
                  </section>
                </section>
                <section>
                  <h2><a name="Timeline_Server_REST_API_V1"></a><a name="Timeline_Server_REST_API_v1"></a>Timeline Server REST API V1</h2>
                  <p>Querying the timeline server is currently only supported via REST API calls; there is no API client implemented in the YARN libraries. In Java, the Jersey client is effective at querying the server, even in secure mode (provided the caller has the appropriate Kerberos tokens or keytab).</p>
                  <p>The v1 REST API is implemented at under the path, <code>/ws/v1/timeline/</code> on the Timeline Server web service.</p>
                  <p>Here is a non-normative description of the API.</p>
                  <section>
                    <h3><a name="Root_path"></a>Root path</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET /ws/v1/timeline/
</pre>
                      </div>
                    </div>
                    <p>Returns a JSON object describing the server instance and version information.</p>
                    <div class="source">
                      <div class="source">
                        <pre> {
   About: "Timeline API",
   timeline-service-version: "3.0.0-SNAPSHOT",
   timeline-service-build-version: "3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum e9ec74ea3ff7bc9f3d35e9cac694fb",
   timeline-service-version-built-on: "2015-05-13T19:45Z",
   hadoop-version: "3.0.0-SNAPSHOT",
   hadoop-build-version: "3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum 95874b192923b43cdb96a6e483afd60",
   hadoop-version-built-on: "2015-05-13T19:44Z"
 }
</pre>
                      </div>
                    </div>
                  </section>
                </section>
                <section>
                  <h2><a name="Domains_.2Fws.2Fv1.2Ftimeline.2Fdomain"></a><a name="REST_API_DOMAINS"></a>Domains <code>/ws/v1/timeline/domain</code></h2>
                  <section>
                    <h3><a name="Domain_summary_information_.2Fws.2Fv1.2Ftimeline.2Fdomain"></a>Domain summary information <code>/ws/v1/timeline/domain</code></h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET /ws/v1/timeline/domain?owner=$OWNER
</pre>
                      </div>
                    </div>
                    <p>Returns a list of domains belonging to a specific user, in the JSON-marshalled <code>TimelineDomains</code> data structure.</p>
                    <p>The <code>owner</code> MUST be set on a GET which is not authenticated.</p>
                    <p>On an authenticated request, the <code>owner</code> defaults to the caller.</p>
                    <div class="source">
                      <div class="source">
                        <pre>PUT /ws/v1/timeline/domain
</pre>
                      </div>
                    </div>
                    <p>A PUT of a serialized <code>TimelineDomain</code> structure to this path will add the domain to the list of domains owned by the specified/current user. A successful operation returns status code of 200 and a <code>TimelinePutResponse</code> containing no errors.</p>
                  </section>
                  <section>
                    <h3><a name="Specific_information_about_a_Domain_.2Fws.2Fv1.2Ftimeline.2Fdomain.2F.7BdomainId.7D"></a>Specific information about a Domain <code>/ws/v1/timeline/domain/{domainId}</code></h3>
                    <p>Returns a JSON-marshalled <code>TimelineDomain</code> structure describing a domain.</p>
                    <p>If the domain is not found, then an HTTP 404 response is returned.</p>
                  </section>
                  <section>
                    <h3><a name="POST_new_domain_.2Fws.2Fv1.2Ftimeline.2Fdomain"></a>POST new domain <code>/ws/v1/timeline/domain</code></h3>
                    <p>Creates a new timeline domain, or overrides an existing one.</p>
                    <p>When attempting to create a new domain, the ID in the submission MUST be unique across all domains in the cluster.</p>
                    <p>When attempting to update an existing domain, the ID of that domain must be set. The submitter must have the appropriate permissions to update the domain.</p>
                    <p>submission: <code>TimelineDomain</code></p>
                    <p>response: <code>TimelinePutResponse</code></p>
                  </section>
                  <section>
                    <h3><a name="List_domains_of_a_user:_GET_.2Fws.2Fv1.2Ftimeline.2Fdomain"></a>List domains of a user: GET <code>/ws/v1/timeline/domain</code></h3>
                    <p>Retrieves a list of all domains of a user.</p>
                    <p>If an owner is specified, that owner name overrides that of the caller.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Query Parameter </th>
                          <th align="left"> Description </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>owner</code></td>
                          <td align="left"> owner of the domains to list</td>
                        </tr>
                      </tbody>
                    </table>
                    <div class="source">
                      <div class="source">
                        <pre>GET http://localhost:8188/ws/v1/timeline/domain?owner=alice

{
"domains":
  [
    {
    "id":"DS_DOMAIN_2",
    "owner":"alice",
    "readers":"peter",
    "writers":"john",
    "createdtime":1430425000337,
    "modifiedtime":1430425000337
    },
    {
    "id":"DS_DOMAIN_1",
    "owner":"alice",
    "readers":"bar",
    "writers":"foo",
    "createdtime":1430424955963,
    "modifiedtime":1430424955963
    }
    ,
    {"id":"DEFAULT",
    "description":"System Default Domain",
    "owner":"alice",
    "readers":"*",
    "writers":"*",
    "createdtime":1430424022699,
    "modifiedtime":1430424022699
    }
  ]
}
</pre>
                      </div>
                    </div>
                    <p>response: <code>TimelineDomains</code></p>
                    <p>If the user lacks the permission to list the domains of the specified owner, an <code>TimelineDomains</code> response with no domain listings is returned.</p>
                    <section>
                      <h4><a name="Retrieve_details_of_a_specific_domain:_GET_.2Fws.2Fv1.2Ftimeline.2Fdomain.2F.7BdomainId.7D"></a>Retrieve details of a specific domain: GET <code>/ws/v1/timeline/domain/{domainId}</code></h4>
                      <p>Retrieves the details of a single domain</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/timeline/domain/DS_DOMAIN_1
</pre>
                        </div>
                      </div>
                      <p>Response: <code>TimelineDomain</code></p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "id":"DS_DOMAIN_1",
  "owner":"zshen",
  "readers":"bar",
  "writers":"foo",
  "createdtime":1430424955963,
  "modifiedtime":1430424955963
}
</pre>
                        </div>
                      </div>
                      <p>If the user lacks the permission to query the details of that domain, a 404, not found exception is returned —the same response which is returned if there is no entry with that ID.</p>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Posting_Timeline_Entities"></a><a name="REST_API_POST_TIMELINE_ENTITIES"></a> Posting Timeline Entities</h2>
                  <p>With the Posting Entities API, you can post the entities and events, which contain the per-framework information you want to record, to the timeline server.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/timeline
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>POST
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="response:_TimelinePutResponse"></a>response: <code>TimelinePutResponse</code></h3>
                  </section>
                  <section>
                    <h3><a name="Request_Examples:"></a>Request Examples:</h3>
                    <section>
                      <h4><a name="JSON_request"></a>JSON request</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>POST http://&lt;timeline server http address:port&gt;/ws/v1/timeline
</pre>
                        </div>
                      </div>
                      <p>Request Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>POST /ws/v1/timeline HTTP/1.1
Accept: application/json
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Request Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "entities" : [ {
    "entity" : "entity id 0",
    "entitytype" : "entity type 0",
    "relatedentities" : {
      "test ref type 2" : [ "test ref id 2" ],
      "test ref type 1" : [ "test ref id 1" ]
    },
    "events" : [ {
      "timestamp" : 1395818851590,
      "eventtype" : "event type 0",
      "eventinfo" : {
        "key2" : "val2",
        "key1" : "val1"
      }
    }, {
      "timestamp" : 1395818851590,
      "eventtype" : "event type 1",
      "eventinfo" : {
        "key2" : "val2",
        "key1" : "val1"
      }
    } ],
    "primaryfilters" : {
      "pkey2" : [ "pval2" ],
      "pkey1" : [ "pval1" ]
    },
    "otherinfo" : {
      "okey2" : "oval2",
      "okey1" : "oval1"
    },
    "starttime" : 1395818851588
  }, {
    "entity" : "entity id 1",
    "entitytype" : "entity type 0",
    "relatedentities" : {
      "test ref type 2" : [ "test ref id 2" ],
      "test ref type 1" : [ "test ref id 1" ]
    },
    "events" : [ {
      "timestamp" : 1395818851590,
      "eventtype" : "event type 0",
      "eventinfo" : {
        "key2" : "val2",
        "key1" : "val1"
      }
    }, {
      "timestamp" : 1395818851590,
      "eventtype" : "event type 1",
      "eventinfo" : {
        "key2" : "val2",
        "key1" : "val1"
      }
    } ],
    "primaryfilters" : {
      "pkey2" : [ "pval2" ],
      "pkey1" : [ "pval1" ]
    },
    "otherinfo" : {
      "okey2" : "oval2",
      "okey1" : "oval1"
    },
    "starttime" : 1395818851590
  } ]
}
</pre>
                        </div>
                      </div>
                      <p>Required fields</p>
                      <p>Entity: <code>type</code> and <code>id</code>. <code>starttime</code> is required unless the entity contains one or more event). Event: <code>type</code> and <code>timestamp</code>.</p>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Timeline_Entity_List"></a><a name="REST_API_LIST_TIMELINE_ENTITIES"></a>Timeline Entity List</h2>
                  <p>With the Timeline Entity List API,  you can retrieve a list of entity object, sorted by the starting timestamp for the entity, descending. The starting timestamp of an entity can be a timestamp specified by the your application. If it is not explicitly specified, it will be chosen by the store to be the earliest timestamp of the events received in the first post for the entity.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain all the entity objects of a given <code>entityType</code>.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/timeline/{entityType}
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <ol style="list-style-type: decimal">
                      <li><code>limit</code> - A limit on the number of entities to return. If null, defaults to 100.</li>
                      <li><code>windowStart</code> - The earliest start timestamp to retrieve (exclusive). If null, defaults to retrieving all entities until the limit is reached.</li>
                      <li><code>windowEnd</code> - The latest start timestamp to retrieve (inclusive). If null, defaults to the max value of Long.</li>
                      <li><code>fromId</code> - If <code>fromId</code> is not null, retrieve entities earlier than and including the specified ID. If no start time is found for the specified ID, an empty list of entities will be returned. The <code>windowEnd</code> parameter will take precedence if the start time of this entity falls later than <code>windowEnd</code>.</li>
                      <li><code>fromTs</code> - If <code>fromTs</code> is not null, ignore entities that were inserted into the store after the given timestamp. The entity’s insert timestamp used for this comparison is the store’s system time when the first put for the entity was received (not the entity’s start time).</li>
                      <li><code>primaryFilter</code> - Retrieves only entities that have the specified primary filter. If null, retrieves all entities. This is an indexed retrieval, and no entities that do not match the filter are scanned.</li>
                      <li><code>secondaryFilters</code> - Retrieves only entities that have exact matches for all the specified filters in their primary filters or other info. This is not an indexed retrieval, so all entities are scanned but only those matching the filters are returned.</li>
                      <li>fields - Specifies which fields of the entity object to retrieve: <code>EVENTS</code>, <code>RELATED_ENTITIES</code>, <code>PRIMARY_FILTERS</code>, <code>OTHER_INFO</code>, <code>LAST_EVENT_ONLY</code>. If the set of fields contains <code>LAST_EVENT_ONLY</code> and not <code>EVENTS</code>, the most recent event for each entity is retrieved. If null, retrieves all fields.</li>
                    </ol>
                    <p>Note that the value of the key/value pair for <code>primaryFilter</code> and <code>secondaryFilters</code> parameters can be of different data types, and matching is data type sensitive. Users need to format the value properly. For example, <code>123</code> and <code>"123"</code> means an integer and a string respectively. If the entity has a string <code>"123"</code> for <code>primaryFilter</code>, but the parameter is set to the integer <code>123</code>, the entity will not be matched. Similarly, <code>true</code> means a boolean while <code>"true"</code> means a string. In general, the value will be casted as a certain Java type in consistent with <code>jackson</code> library parsing a JSON clip.</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_entities_.28Timeline_Entity_List.29_Object"></a>Elements of the <code>entities</code> (Timeline Entity List) Object</h3>
                    <p>When you make a request for the list of timeline entities, the information will be returned as a collection of container objects. See also <code>Timeline Entity</code> for syntax of the timeline entity object.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type </th>
                          <th align="left"> Description</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>entities</code> </td>
                          <td align="left"> array of timeline entity objects(JSON) </td>
                          <td align="left"> The collection of timeline entity objects </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/timeline/DS_APP_ATTEMPT
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "entities":[
  {
  "entitytype":"DS_APP_ATTEMPT",
  "entity":"appattempt_1430424020775_0004_000001",
  "events":[
    {
    "timestamp":1430425008796,
    "eventtype":"DS_APP_ATTEMPT_END",
    "eventinfo": { }
    }
    {
    "timestamp":1430425004161,
    "eventtype":"DS_APP_ATTEMPT_START",
    "eventinfo": { }
    }
  ]
  "starttime":1430425004161,
  "domain":"DS_DOMAIN_2",
  "relatedentities": { },
  "primaryfilters":
    {
    "user":["zshen"]
    },
  "otherinfo": { }
  }
  {
  "entitytype":"DS_APP_ATTEMPT",
  "entity":"appattempt_1430424020775_0003_000001",
  "starttime":1430424959169,
  "domain":"DS_DOMAIN_1",
  "events":[
    {
    "timestamp":1430424963836,
    "eventinfo": { }
     }
    {
    "timestamp":1430424959169,
    "eventinfo": { }
    }
   ]
  "relatedentities": { },
  "primaryfilters": {
    "user":["zshen"]
   },
  "otherinfo": { }
   }
  ]
}
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Timeline_Entity"></a><a name="REST_API_GET_TIMELINE_ENTITY"></a>Timeline Entity</h2>
                  <p>With the Timeline Entity API, you can retrieve the entity information for a given entity identifier.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain the entity object identified by the <code>entityType</code> value and the <code>entityId</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/timeline/{entityType}/{entityId}
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <ol style="list-style-type: decimal">
                      <li>fields - Specifies which fields of the entity object to retrieve: <code>EVENTS</code>, <code>RELATED_ENTITIES</code>, <code>PRIMARY_FILTERS</code>, <code>OTHER_INFO</code>, <code>LAST_EVENT_ONLY</code>. If the set of fields contains LAST_EVENT_ONLY and not EVENTS, the most recent event for each entity is retrieved. If null, retrieves all fields.</li>
                    </ol>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_entity_.28Timeline_Entity.29_Object:"></a>Elements of the <code>entity</code> (Timeline Entity) Object:</h3>
                    <p>See also <code>Timeline Event List</code> for syntax of the timeline event object. Note that <code>value</code> of <code>primaryfilters</code> and <code>otherinfo</code> is an Object instead of a String.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type </th>
                          <th align="left"> Description</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>entity</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The entity id </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>entitytype</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The entity type </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>relatedentities</code> </td>
                          <td align="left"> map </td>
                          <td align="left"> The related entities’ identifiers, which are organized in a map of entityType : [entity1, entity2, …] </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>events</code> </td>
                          <td align="left"> list </td>
                          <td align="left"> The events of the entity </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>primaryfilters</code> </td>
                          <td align="left"> map </td>
                          <td align="left"> The primary filters of the entity, which are organized in a map of key : [value1, value2, …] </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>otherinfo</code> </td>
                          <td align="left"> map </td>
                          <td align="left"> The other information of the entity, which is organized in a map of key : value </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>starttime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The start time of the entity </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/timeline/DS_APP_ATTEMPT/appattempt_1430424020775_0003_000001
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "events":[
    {
    "timestamp":1430424959169,
    "eventtype":"DS_APP_ATTEMPT_START",
    "eventinfo":  {}}],
    "entitytype":"DS_APP_ATTEMPT",
    "entity":"appattempt_1430424020775_0003_000001",
    "starttime":1430424959169,
    "domain":"DS_DOMAIN_1",
    "relatedentities":  {},
    "primaryfilters":  {
        "user":["zshen"]
        },
    "otherinfo":  {}
    }
  ]
}
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Timeline_Event_List"></a><a name="REST_API_LIST_TIMELINE_EVENTS"></a>Timeline Event List</h2>
                  <p>With the Timeline Events API, you can retrieve the event objects for a list of entities all of the same entity type. The events for each entity are sorted in order of their timestamps, descending.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain the event objects of the given <code>entityType</code>.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/timeline/{entityType}/events
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <ol style="list-style-type: decimal">
                      <li><code>entityId</code> - The entity IDs to retrieve events for. If null, no events will be returned. Multiple entityIds can be given as comma separated values.</li>
                      <li><code>limit</code> - A limit on the number of events to return for each entity. If null, defaults to 100 events per entity.</li>
                      <li><code>windowStart</code> - If not null, retrieves only events later than the given time (exclusive)</li>
                      <li><code>windowEnd</code> - If not null, retrieves only events earlier than the given time (inclusive)</li>
                      <li><code>eventType</code> - Restricts the events returned to the given types. If null, events of all types will be returned. Multiple eventTypes can be given as comma separated values.</li>
                    </ol>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_events_.28Timeline_Entity_List.29_Object"></a>Elements of the <code>events</code> (Timeline Entity List) Object</h3>
                    <p>When you make a request for the list of timeline events, the information will be returned as a collection of event objects.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type </th>
                          <th align="left"> Description</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>events</code> </td>
                          <td align="left"> array of timeline event objects(JSON) </td>
                          <td align="left"> The collection of timeline event objects </td>
                        </tr>
                      </tbody>
                    </table>
                    <p>Below is the elements of a single event object.  Note that <code>value</code> of <code>eventinfo</code> and <code>otherinfo</code> is an Object instead of a String.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type </th>
                          <th align="left"> Description</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>eventtype</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The event type </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>eventinfo</code> </td>
                          <td align="left"> map </td>
                          <td align="left"> The information of the event, which is orgainzied in a map of <code>key</code> : <code>value</code> </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>timestamp</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The timestamp of the event </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/timeline/DS_APP_ATTEMPT/events?entityId=appattempt_1430424020775_0003_000001
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
"events": [
  {
  "entity":"appattempt_1430424020775_0003_000001",
  "entitytype":"DS_APP_ATTEMPT"}
  "events":[
    {
    "timestamp":1430424963836,
    "eventtype":"DS_APP_ATTEMPT_END",
    "eventinfo":{}},
    {
    "timestamp":1430424959169,
    "eventtype":"DS_APP_ATTEMPT_START",
    "eventinfo":{}}
    ],
   }
  ]
}
</pre>
                        </div>
                      </div>
                      <h1><a name="GENERIC_DATA_REST_APIS"></a> Generic Data REST APIs</h1>
                      <p>Users can access the generic historic information of applications via REST APIs.</p>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="About"></a><a name="REST_API_ABOUT"></a>About</h2>
                  <p>With the about API, you can get an timeline about resource that contains generic history REST API description and version information.</p>
                  <p>It is essentially a XML/JSON-serialized form of the YARN <code>TimelineAbout</code> structure.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain an timeline about object.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/about
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_about_.28Application.29_Object:"></a>Elements of the <code>about</code> (Application) Object:</h3>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item         </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description                   </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>About</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The description about the service </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>timeline-service-version</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The timeline service version </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>timeline-service-build-version</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The timeline service build version </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>timeline-service-version-built-on</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> On what time the timeline service is built </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>hadoop-version</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> Hadoop version </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>hadoop-build-version</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> Hadoop build version </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>hadoop-version-built-on</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> On what time Hadoop is built </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>http://localhost:8188/ws/v1/applicationhistory/about
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  About: "Generic History Service API",
  timeline-service-version: "3.0.0-SNAPSHOT",
  timeline-service-build-version: "3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum e9ec74ea3ff7bc9f3d35e9cac694fb",
  timeline-service-version-built-on: "2015-05-13T19:45Z",
  hadoop-version: "3.0.0-SNAPSHOT",
  hadoop-build-version: "3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum 95874b192923b43cdb96a6e483afd60",
  hadoop-version-built-on: "2015-05-13T19:44Z"
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/about
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
Content-Length: 748
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre> &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
 &lt;about&gt;
   &lt;About&gt;Generic History Service API&lt;/About&gt;
   &lt;hadoop-build-version&gt;3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum 95874b192923b43cdb96a6e483afd60&lt;/hadoop-build-version&gt;
   &lt;hadoop-version&gt;3.0.0-SNAPSHOT&lt;/hadoop-version&gt;
   &lt;hadoop-version-built-on&gt;2015-05-13T19:44Z&lt;/hadoop-version-built-on&gt;
   &lt;timeline-service-build-version&gt;3.0.0-SNAPSHOT from fcd0702c10ce574b887280476aba63d6682d5271 by zshen source checksum e9ec74ea3ff7bc9f3d35e9cac694fb&lt;/timeline-service-build-version&gt;
   &lt;timeline-service-version&gt;3.0.0-SNAPSHOT&lt;/timeline-service-version&gt;
   &lt;timeline-service-version-built-on&gt;2015-05-13T19:45Z&lt;/timeline-service-version-built-on&gt;
 &lt;/about&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Application_List"></a><a name="REST_API_LIST_APPLICATIONS"></a>Application List</h2>
                  <p>With the Application List API, you can obtain a collection of resources, each of which represents an application. When you run a GET operation on this resource, you obtain a collection of application objects.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <ol style="list-style-type: decimal">
                      <li>
                        <p><code>states</code> - applications matching the given application states, specified as a comma-separated list</p>
                      </li>
                      <li>
                        <p><code>finalStatus</code> - the final status of the application - reported by the application itself</p>
                      </li>
                      <li>
                        <p><code>user</code> - user name</p>
                      </li>
                      <li>
                        <p><code>queue</code> - queue name</p>
                      </li>
                      <li>
                        <p><code>limit</code> - total number of app objects to be returned</p>
                      </li>
                      <li>
                        <p><code>startedTimeBegin</code> - applications with start time beginning with this time, specified in ms since epoch</p>
                      </li>
                      <li>
                        <p><code>startedTimeEnd</code> - applications with start time ending with this time, specified in ms since epoch</p>
                      </li>
                      <li>
                        <p><code>finishedTimeBegin</code> - applications with finish time beginning with this time, specified in ms since epoch</p>
                      </li>
                      <li>
                        <p><code>finishedTimeEnd</code> - applications with finish time ending with this time, specified in ms since epoch</p>
                      </li>
                      <li>
                        <p><code>applicationTypes</code> - applications matching the given application types, specified as a comma-separated list</p>
                      </li>
                    </ol>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_apps_.28Application_List.29_Object"></a>Elements of the <code>apps</code> (Application List) Object</h3>
                    <p>When you make a request for the list of applications, the information will be returned as a collection of application objects. See also <code>Application</code> for syntax of the application object.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item  </th>
                          <th align="left"> Data Type </th>
                          <th align="left"> Description  </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>app</code> </td>
                          <td align="left"> array of app objects(JSON)/zero or more application objects(XML) </td>
                          <td align="left"> The collection of application objects </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://&lt;timeline server http address:port&gt;/ws/v1/applicationhistory/apps
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "app":
  [
      {
      "appId":"application_1430424020775_0004",
      "currentAppAttemptId":"appattempt_1430424020775_0004_000001",
      "user":"zshen",
      "name":"DistributedShell",
      "queue":"default",
      "type":"YARN",
      "host":"d-69-91-129-173.dhcp4.washington.edu/69.91.129.173",
      "rpcPort":-1,
      "appState":"FINISHED",
      "progress":100.0,
      "diagnosticsInfo":"",
      "originalTrackingUrl":"N/A",
      "trackingUrl":"http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0004/",
      "finalAppStatus":"SUCCEEDED",
      "submittedTime":1430425001004,
      "startedTime":1430425001004,
      "finishedTime":1430425008861,
      "elapsedTime":7857,
      "unmanagedApplication":"false",
      "applicationPriority":0,
      "appNodeLabelExpression":"",
      "amNodeLabelExpression":""
      },
      {
      "appId":"application_1430424020775_0003",
      "currentAppAttemptId":"appattempt_1430424020775_0003_000001",
      "user":"zshen",
      "name":"DistributedShell",
      "queue":"default",
      "type":"YARN",
      "host":"d-69-91-129-173.dhcp4.washington.edu/69.91.129.173",
      "rpcPort":-1,
      "appState":"FINISHED",
      "progress":100.0,
      "diagnosticsInfo":"",
      "originalTrackingUrl":"N/A",
      "trackingUrl":"http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0003/",
      "finalAppStatus":"SUCCEEDED",
      "submittedTime":1430424956650,
      "startedTime":1430424956650,
      "finishedTime":1430424963907,
      "elapsedTime":7257,
      "unmanagedApplication":"false",
      "applicationPriority":0,
      "appNodeLabelExpression":"",
      "amNodeLabelExpression":""
      },
      {
      "appId":"application_1430424020775_0002",
      "currentAppAttemptId":"appattempt_1430424020775_0002_000001",
      "user":"zshen",
      "name":"DistributedShell",
      "queue":"default",
      "type":"YARN",
      "host":"d-69-91-129-173.dhcp4.washington.edu/69.91.129.173",
      "rpcPort":-1,
      "appState":"FINISHED",
      "progress":100.0,
      "diagnosticsInfo":"",
      "originalTrackingUrl":"N/A",
      "trackingUrl":"http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0002/",
      "finalAppStatus":"SUCCEEDED",
      "submittedTime":1430424769395,
      "startedTime":1430424769395,
      "finishedTime":1430424776594,
      "elapsedTime":7199,
      "unmanagedApplication":"false",
      "applicationPriority":0,
      "appNodeLabelExpression":"",
      "amNodeLabelExpression":""
      },
      {
      "appId":"application_1430424020775_0001",
      "currentAppAttemptId":"appattempt_1430424020775_0001_000001",
      "user":"zshen",
      "name":"QuasiMonteCarlo",
      "queue":"default",
      "type":"MAPREDUCE",
      "host":"localhost",
      "rpcPort":56264,
      "appState":"FINISHED",
      "progress":100.0,
      "diagnosticsInfo":"",
      "originalTrackingUrl":"http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001",
      "trackingUrl":"http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/",
      "finalAppStatus":"SUCCEEDED",
      "submittedTime":1430424053809,
      "startedTime":1430424072153,
      "finishedTime":1430424776594,
      "elapsedTime":18344,
      "applicationTags":"mrapplication,ta-example",
      "unmanagedApplication":"false",
      "applicationPriority":0,
      "appNodeLabelExpression":"",
      "amNodeLabelExpression":""
      }
  ]
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
Content-Length: 1710
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;apps&gt;
  &lt;app&gt;
    &lt;appId&gt;application_1430424020775_0004&lt;/appId&gt;
    &lt;currentAppAttemptId&gt;appattempt_1430424020775_0004_000001&lt;/currentAppAttemptId&gt;
    &lt;user&gt;zshen&lt;/user&gt;
    &lt;name&gt;DistributedShell&lt;/name&gt;
    &lt;queue&gt;default&lt;/queue&gt;
    &lt;type&gt;YARN&lt;/type&gt;
    &lt;host&gt;d-69-91-129-173.dhcp4.washington.edu/69.91.129.173&lt;/host&gt;
    &lt;rpcPort&gt;-1&lt;/rpcPort&gt;
    &lt;appState&gt;FINISHED&lt;/appState&gt;
    &lt;progress&gt;100.0&lt;/progress&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;originalTrackingUrl&gt;N/A&lt;/originalTrackingUrl&gt;
    &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0004/&lt;/trackingUrl&gt;
    &lt;finalAppStatus&gt;SUCCEEDED&lt;/finalAppStatus&gt;
    &lt;submittedTime&gt;1430425001004&lt;/submittedTime&gt;
    &lt;startedTime&gt;1430425001004&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430425008861&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7857&lt;/elapsedTime&gt;
    &lt;unmanagedApplication&gt;false&lt;/unmanagedApplication&gt;
    &lt;applicationPriority&gt;0&lt;/applicationPriority&gt;
    &lt;appNodeLabelExpression&gt;&lt;/appNodeLabelExpression&gt;
    &lt;amNodeLabelExpression&gt;&lt;/amNodeLabelExpression&gt;
  &lt;/app&gt;
  &lt;app&gt;
    &lt;appId&gt;application_1430424020775_0003&lt;/appId&gt;
    &lt;currentAppAttemptId&gt;appattempt_1430424020775_0003_000001&lt;/currentAppAttemptId&gt;
    &lt;user&gt;zshen&lt;/user&gt;
    &lt;name&gt;DistributedShell&lt;/name&gt;
    &lt;queue&gt;default&lt;/queue&gt;
    &lt;type&gt;YARN&lt;/type&gt;
    &lt;host&gt;d-69-91-129-173.dhcp4.washington.edu/69.91.129.173&lt;/host&gt;
    &lt;rpcPort&gt;-1&lt;/rpcPort&gt;
    &lt;appState&gt;FINISHED&lt;/appState&gt;
    &lt;progress&gt;100.0&lt;/progress&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;originalTrackingUrl&gt;N/A&lt;/originalTrackingUrl&gt;
    &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0003/&lt;/trackingUrl&gt;
    &lt;finalAppStatus&gt;SUCCEEDED&lt;/finalAppStatus&gt;
    &lt;submittedTime&gt;1430424956650&lt;/submittedTime&gt;
    &lt;startedTime&gt;1430424956650&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424963907&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7257&lt;/elapsedTime&gt;
    &lt;unmanagedApplication&gt;false&lt;/unmanagedApplication&gt;
    &lt;applicationPriority&gt;0&lt;/applicationPriority&gt;
    &lt;appNodeLabelExpression&gt;&lt;/appNodeLabelExpression&gt;
    &lt;amNodeLabelExpression&gt;&lt;/amNodeLabelExpression&gt;
  &lt;/app&gt;
  &lt;app&gt;
    &lt;appId&gt;application_1430424020775_0002&lt;/appId&gt;
    &lt;currentAppAttemptId&gt;appattempt_1430424020775_0002_000001&lt;/currentAppAttemptId&gt;
    &lt;user&gt;zshen&lt;/user&gt;
    &lt;name&gt;DistributedShell&lt;/name&gt;
    &lt;queue&gt;default&lt;/queue&gt;
    &lt;type&gt;YARN&lt;/type&gt;
    &lt;host&gt;d-69-91-129-173.dhcp4.washington.edu/69.91.129.173&lt;/host&gt;
    &lt;rpcPort&gt;-1&lt;/rpcPort&gt;
    &lt;appState&gt;FINISHED&lt;/appState&gt;
    &lt;progress&gt;100.0&lt;/progress&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;originalTrackingUrl&gt;N/A&lt;/originalTrackingUrl&gt;
    &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0002/&lt;/trackingUrl&gt;
    &lt;finalAppStatus&gt;SUCCEEDED&lt;/finalAppStatus&gt;
    &lt;submittedTime&gt;1430424769395&lt;/submittedTime&gt;
    &lt;startedTime&gt;1430424769395&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424776594&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7199&lt;/elapsedTime&gt;
    &lt;unmanagedApplication&gt;false&lt;/unmanagedApplication&gt;
    &lt;applicationPriority&gt;0&lt;/applicationPriority&gt;
    &lt;appNodeLabelExpression&gt;&lt;/appNodeLabelExpression&gt;
    &lt;amNodeLabelExpression&gt;&lt;/amNodeLabelExpression&gt;
  &lt;/app&gt;
  &lt;app&gt;
    &lt;appId&gt;application_1430424020775_0001&lt;/appId&gt;
    &lt;currentAppAttemptId&gt;appattempt_1430424020775_0001_000001&lt;/currentAppAttemptId&gt;
    &lt;user&gt;zshen&lt;/user&gt;
    &lt;name&gt;QuasiMonteCarlo&lt;/name&gt;
    &lt;queue&gt;default&lt;/queue&gt;
    &lt;type&gt;MAPREDUCE&lt;/type&gt;
    &lt;host&gt;localhost&lt;/host&gt;
    &lt;rpcPort&gt;56264&lt;/rpcPort&gt;
    &lt;appState&gt;FINISHED&lt;/appState&gt;
    &lt;progress&gt;100.0&lt;/progress&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;originalTrackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001&lt;/originalTrackingUrl&gt;
    &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/&lt;/trackingUrl&gt;
    &lt;finalAppStatus&gt;SUCCEEDED&lt;/finalAppStatus&gt;
    &lt;submittedTime&gt;1430424053809&lt;/submittedTime&gt;
    &lt;startedTime&gt;1430424053809&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424072153&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;18344&lt;/elapsedTime&gt;
    &lt;applicationTags&gt;mrapplication,ta-example&lt;/applicationTags&gt;
    &lt;unmanagedApplication&gt;false&lt;/unmanagedApplication&gt;
    &lt;applicationPriority&gt;0&lt;/applicationPriority&gt;
    &lt;appNodeLabelExpression&gt;&lt;/appNodeLabelExpression&gt;
    &lt;amNodeLabelExpression&gt;&lt;/amNodeLabelExpression&gt;
  &lt;/app&gt;
&lt;/apps&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Application"></a><a name="REST_API_GET_APPLICATION"></a>Application</h2>
                  <p>With the Application API, you can get an application resource contains information about a particular application that was running on an YARN cluster.</p>
                  <p>It is essentially a XML/JSON-serialized form of the YARN <code>ApplicationReport</code> structure.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain an application object identified by the <code>appid</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps/{appid}
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_app_.28Application.29_Object:"></a>Elements of the <code>app</code> (Application) Object:</h3>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item         </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description                   </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>appId</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The application ID </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>user</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The user who started the application </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>name</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The application name </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>type</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The application type </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>queue</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The queue to which the application submitted </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>appState</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The application state according to the ResourceManager - valid values are members of the YarnApplicationState enum: <code>FINISHED</code>, <code>FAILED</code>, <code>KILLED</code> </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>finalStatus</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The final status of the application if finished - reported by the application itself - valid values are: <code>UNDEFINED</code>, <code>SUCCEEDED</code>, <code>FAILED</code>, <code>KILLED</code> </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>progress</code> </td>
                          <td align="left"> float </td>
                          <td align="left"> The reported progress of the application as a percent. Long-lived YARN services may not provide a meaninful value here —or use it as a metric of actual vs desired container counts </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>trackingUrl</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The web URL of the application (via the RM Proxy) </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>originalTrackingUrl</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The actual web URL of the application </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>diagnosticsInfo</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> Detailed diagnostics information on a completed application</td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>startedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The time in which application started (in ms since epoch) </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>finishedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The time in which the application finished (in ms since epoch) </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>elapsedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The elapsed time since the application started (in ms) </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>allocatedMB</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The sum of memory in MB allocated to the application’s running containers </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>allocatedVCores</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The sum of virtual cores allocated to the application’s running containers </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>currentAppAttemptId</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The latest application attempt ID </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>host</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The host of the ApplicationMaster </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>rpcPort</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The RPC port of the ApplicationMaster; zero if no IPC service declared </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>applicationTags</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The application tags. </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>unmanagedApplication</code> </td>
                          <td align="left"> boolean </td>
                          <td align="left"> Is the application unmanaged. </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>applicationPriority</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> Priority of the submitted application. </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>appNodeLabelExpression</code> </td>
                          <td align="left"> string </td>
                          <td align="left">Node Label expression which is used to identify the nodes on which application’s containers are expected to run by default.</td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>amNodeLabelExpression</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> Node Label expression which is used to identify the node on which application’s  AM container is expected to run.</td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "appId": "application_1430424020775_0001",
  "currentAppAttemptId": "appattempt_1430424020775_0001_000001",
  "user": "zshen",
  "name": "QuasiMonteCarlo",
  "queue": "default",
  "type": "MAPREDUCE",
  "host": "localhost",
  "rpcPort": 56264,
  "appState": "FINISHED",
  "progress": 100.0,
  "diagnosticsInfo": "",
  "originalTrackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001",
  "trackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/",
  "finalAppStatus": "SUCCEEDED",
  "submittedTime": 1430424053809,
  "startedTime": 1430424053809,
  "finishedTime": 1430424072153,
  "elapsedTime": 18344,
  "applicationTags": mrapplication,tag-example,
  "unmanagedApplication": "false",
  "applicationPriority": 0,
  "appNodeLabelExpression": "",
  "amNodeLabelExpression": ""
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
Content-Length: 873
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre> &lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
 &lt;app&gt;
   &lt;appId&gt;application_1430424020775_0001&lt;/appId&gt;
   &lt;currentAppAttemptId&gt;appattempt_1430424020775_0001_000001&lt;/currentAppAttemptId&gt;
   &lt;user&gt;zshen&lt;/user&gt;
   &lt;name&gt;QuasiMonteCarlo&lt;/name&gt;
   &lt;queue&gt;default&lt;/queue&gt;
   &lt;type&gt;MAPREDUCE&lt;/type&gt;
   &lt;host&gt;localhost&lt;/host&gt;
   &lt;rpcPort&gt;56264&lt;/rpcPort&gt;
   &lt;appState&gt;FINISHED&lt;/appState&gt;
   &lt;progress&gt;100.0&lt;/progress&gt;
   &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
   &lt;originalTrackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001&lt;/originalTrackingUrl&gt;
   &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/&lt;/trackingUrl&gt;
   &lt;finalAppStatus&gt;SUCCEEDED&lt;/finalAppStatus&gt;
   &lt;submittedTime&gt;1430424053809&lt;/submittedTime&gt;
   &lt;startedTime&gt;1430424053809&lt;/startedTime&gt;
   &lt;finishedTime&gt;1430424072153&lt;/finishedTime&gt;
   &lt;elapsedTime&gt;18344&lt;/elapsedTime&gt;
   &lt;applicationTags&gt;mrapplication,ta-example&lt;/applicationTags&gt;
   &lt;unmanagedApplication&gt;false&lt;/unmanagedApplication&gt;
   &lt;applicationPriority&gt;0&lt;/applicationPriority&gt;
   &lt;appNodeLabelExpression&gt;&lt;appNodeLabelExpression&gt;
   &lt;amNodeLabelExpression&gt;&lt;amNodeLabelExpression&gt;
 &lt;/app&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Application_Attempt_List"></a><a name="REST_API_APPLICATION_ATTEMPT_LIST"></a>Application Attempt List</h2>
                  <p>With the Application Attempt List API, you can obtain a collection of resources, each of which represents an application attempt. When you run a GET operation on this resource, you obtain a collection of application attempt objects.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain all the attempt objects of an application identified by the <code>appid</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps/{appid}/appattempts
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_appattempts_.28Application_Attempt_List.29_Object"></a>Elements of the <code>appattempts</code> (Application Attempt List) Object</h3>
                    <p>When you make a request for the list of application attempts, the information will be returned as a collection of application attempt objects. See <a href="#REST_API_APPLICATION_ATTEMPT">Application Attempt</a> for the syntax of the application attempt object.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item         </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description                  </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>appattempt</code> </td>
                          <td align="left"> array of appattempt objects(JSON)/zero or more application attempt objects(XML) </td>
                          <td align="left"> The collection of application attempt objects </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET  http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "appAttempt": [
    {
      "appAttemptId": "appattempt_1430424020775_0001_000001",
      "host": "localhost",
      "rpcPort": 56264,
      "trackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/",
      "originalTrackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001",
      "diagnosticsInfo": "",
      "appAttemptState": "FINISHED",
      "amContainerId": "container_1430424020775_0001_01_000001"
    }
  ]
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;appAttempts&gt;
  &lt;appAttempt&gt;
    &lt;appAttemptId&gt;appattempt_1430424020775_0001_000001&lt;/appAttemptId&gt;
    &lt;host&gt;localhost&lt;/host&gt;
    &lt;rpcPort&gt;56264&lt;/rpcPort&gt;
    &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/&lt;/trackingUrl&gt;
    &lt;originalTrackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001&lt;/originalTrackingUrl&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;appAttemptState&gt;FINISHED&lt;/appAttemptState&gt;
    &lt;amContainerId&gt;container_1430424020775_0001_01_000001&lt;/amContainerId&gt;
  &lt;/appAttempt&gt;
&lt;/appAttempts&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Application_Attempt"></a><a name="REST_API_APPLICATION_ATTEMPT"></a>Application Attempt</h2>
                  <p>With the Application Attempt API, you can get an application attempt resource contains information about a particular application attempt of an application that was running on an YARN cluster.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain an application attempt object identified by the <code>appid</code> value and the <code>appattemptid</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps/{appid}/appattempts/{appattemptid}
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_appattempt_.28Application_Attempt.29_Object:"></a>Elements of the <code>appattempt</code> (Application Attempt) Object:</h3>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description  </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>appAttemptId</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The application attempt Id </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>amContainerId</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The ApplicationMaster container Id </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>appAttemptState</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The application attempt state according to the ResourceManager - valid values are members of the YarnApplicationAttemptState enum: FINISHED, FAILED, KILLED </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>trackingUrl</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The web URL that can be used to track the application </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>originalTrackingUrl</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The actual web URL of the application </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>diagnosticsInfo</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> Detailed diagnostics information </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>host</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The host of the ApplicationMaster </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>rpcPort</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The rpc port of the ApplicationMaster </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts/appattempt_1430424020775_0001_000001
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "appAttemptId": "appattempt_1430424020775_0001_000001",
  "host": "localhost",
  "rpcPort": 56264,
  "trackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/",
  "originalTrackingUrl": "http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001",
  "diagnosticsInfo": "",
  "appAttemptState": "FINISHED",
  "amContainerId": "container_1430424020775_0001_01_000001"
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://&lt;timeline server http address:port&gt;/ws/v1/applicationhistory/apps/application_1395789200506_0001/appattempts/appattempt_1395789200506_0001_000001
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
Content-Length: 488
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;appAttempt&gt;
  &lt;appAttemptId&gt;appattempt_1430424020775_0001_000001&lt;/appAttemptId&gt;
  &lt;host&gt;localhost&lt;/host&gt;
  &lt;rpcPort&gt;56264&lt;/rpcPort&gt;
  &lt;trackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:8088/proxy/application_1430424020775_0001/&lt;/trackingUrl&gt;
  &lt;originalTrackingUrl&gt;http://d-69-91-129-173.dhcp4.washington.edu:19888/jobhistory/job/job_1430424020775_0001&lt;/originalTrackingUrl&gt;
  &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
  &lt;appAttemptState&gt;FINISHED&lt;/appAttemptState&gt;
  &lt;amContainerId&gt;container_1430424020775_0001_01_000001&lt;/amContainerId&gt;
&lt;/appAttempt&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Container_List"></a><a name="REST_API_CONTAINER_LIST"></a>Container List</h2>
                  <p>With the Container List API, you can obtain a collection of resources, each of which represents a container. When you run a GET operation on this resource, you obtain a collection of container objects.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain all the container objects of an application attempt identified by the <code>appid</code> value and the <code>appattemptid</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps/{appid}/appattempts/{appattemptid}/containers
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <div class="source">
                      <div class="source">
                        <pre>GET
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_containers_.28Container_List.29_Object"></a>Elements of the <code>containers</code> (Container List) Object</h3>
                    <p>When you make a request for the list of containers, the information will be returned as a collection of container objects. See also <code>Container</code> for syntax of the container object.</p>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>container</code> </td>
                          <td align="left"> array of container objects(JSON)/zero or more container objects(XML) </td>
                          <td align="left"> The collection of container objects </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts/appattempt_1430424020775_0001_000001/containers?
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "container": [
    {
      "containerId": "container_1430424020775_0001_01_000007",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 10,
      "startedTime": 1430424068296,
      "finishedTime": 1430424073006,
      "elapsedTime": 4710,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000007/container_1430424020775_0001_01_000007/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000006",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 20,
      "startedTime": 1430424060317,
      "finishedTime": 1430424068293,
      "elapsedTime": 7976,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000006/container_1430424020775_0001_01_000006/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000005",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 20,
      "startedTime": 1430424060316,
      "finishedTime": 1430424068294,
      "elapsedTime": 7978,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000005/container_1430424020775_0001_01_000005/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000003",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 20,
      "startedTime": 1430424060315,
      "finishedTime": 1430424068289,
      "elapsedTime": 7974,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000003/container_1430424020775_0001_01_000003/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000004",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 20,
      "startedTime": 1430424060315,
      "finishedTime": 1430424068291,
      "elapsedTime": 7976,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000004/container_1430424020775_0001_01_000004/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000002",
      "allocatedMB": 1024,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 20,
      "startedTime": 1430424060313,
      "finishedTime": 1430424067250,
      "elapsedTime": 6937,
      "diagnosticsInfo": "Container killed by the ApplicationMaster.\nContainer killed on request. Exit code is 143\nContainer exited with a non-zero exit code 143\n",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000002/container_1430424020775_0001_01_000002/zshen",
      "containerExitStatus": -105,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    },
    {
      "containerId": "container_1430424020775_0001_01_000001",
      "allocatedMB": 2048,
      "allocatedVCores": 1,
      "assignedNodeId": "localhost:9105",
      "priority": 0,
      "startedTime": 1430424054314,
      "finishedTime": 1430424079022,
      "elapsedTime": 24708,
      "diagnosticsInfo": "",
      "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000001/container_1430424020775_0001_01_000001/zshen",
      "containerExitStatus": 0,
      "containerState": "COMPLETE",
      "nodeHttpAddress": "http://localhost:8042"
    }
  ]
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts/appattempt_1430424020775_0001_000001/containers
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>  HTTP/1.1 200 OK
  Content-Type: application/xml
  Content-Length: 1428
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;containers&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000007&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;10&lt;/priority&gt;
    &lt;startedTime&gt;1430424068296&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424073006&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;4710&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000007/container_1430424020775_0001_01_000007/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000006&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;20&lt;/priority&gt;
    &lt;startedTime&gt;1430424060317&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424068293&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7976&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000006/container_1430424020775_0001_01_000006/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000005&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;20&lt;/priority&gt;
    &lt;startedTime&gt;1430424060316&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424068294&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7978&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000005/container_1430424020775_0001_01_000005/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000003&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;20&lt;/priority&gt;
    &lt;startedTime&gt;1430424060315&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424068289&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7974&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000003/container_1430424020775_0001_01_000003/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000004&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;20&lt;/priority&gt;
    &lt;startedTime&gt;1430424060315&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424068291&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;7976&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000004/container_1430424020775_0001_01_000004/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000002&lt;/containerId&gt;
    &lt;allocatedMB&gt;1024&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;20&lt;/priority&gt;
    &lt;startedTime&gt;1430424060313&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424067250&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;6937&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;Container killed by the ApplicationMaster.
      Container killed on request. Exit code is 143
      Container exited with a non-zero exit code 143
    &lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000002/container_1430424020775_0001_01_000002/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;-105&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
  &lt;container&gt;
    &lt;containerId&gt;container_1430424020775_0001_01_000001&lt;/containerId&gt;
    &lt;allocatedMB&gt;2048&lt;/allocatedMB&gt;
    &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
    &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
    &lt;priority&gt;0&lt;/priority&gt;
    &lt;startedTime&gt;1430424054314&lt;/startedTime&gt;
    &lt;finishedTime&gt;1430424079022&lt;/finishedTime&gt;
    &lt;elapsedTime&gt;24708&lt;/elapsedTime&gt;
    &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
    &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000001/container_1430424020775_0001_01_000001/zshen&lt;/logUrl&gt;
    &lt;containerExitStatus&gt;0&lt;/containerExitStatus&gt;
    &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
    &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
  &lt;/container&gt;
&lt;/containers&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                </section>
                <section>
                  <h2><a name="Container"></a><a name="REST_API_CONTAINER"></a>Container</h2>
                  <p>With the Container API, you can get a container resource contains information about a particular container of an application attempt of an application that was running on an YARN cluster.</p>
                  <section>
                    <h3><a name="URI:"></a>URI:</h3>
                    <p>Use the following URI to obtain a container object identified by the <code>appid</code> value, the <code>appattemptid</code> value and the <code>containerid</code> value.</p>
                    <div class="source">
                      <div class="source">
                        <pre>http(s)://&lt;timeline server http(s) address:port&gt;/ws/v1/applicationhistory/apps/{appid}/appattempts/{appattemptid}/containers/{containerid}
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="HTTP_Operations_Supported:"></a>HTTP Operations Supported:</h3>
                    <p>GET</p>
                  </section>
                  <section>
                    <h3><a name="Query_Parameters_Supported:"></a>Query Parameters Supported:</h3>
                    <p>None</p>
                  </section>
                  <section>
                    <h3><a name="Elements_of_the_container_.28Container.29_Object:"></a>Elements of the <code>container</code> (Container) Object:</h3>
                    <table border="0" class="bodyTable">
                      <thead>
                        <tr class="a">
                          <th align="left"> Item </th>
                          <th align="left"> Data Type   </th>
                          <th align="left"> Description   </th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr class="b">
                          <td align="left"> <code>containerId</code> </td>
                          <td align="left"> string  </td>
                          <td align="left"> The container Id </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>containerState</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The container state according to the ResourceManager - valid values are members of the ContainerState enum: COMPLETE </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>containerExitStatus</code> </td>
                          <td align="left"> int  </td>
                          <td align="left"> The container exit status </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>logUrl</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The log URL that can be used to access the container aggregated log </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>diagnosticsInfo</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> Detailed diagnostics information </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>startedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The time in which container started (in ms since epoch) </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>finishedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The time in which the container finished (in ms since epoch) </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>elapsedTime</code> </td>
                          <td align="left"> long </td>
                          <td align="left"> The elapsed time since the container started (in ms) </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>allocatedMB</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The memory in MB allocated to the container </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>allocatedVCores</code> </td>
                          <td align="left"> int </td>
                          <td align="left"> The virtual cores allocated to the container </td>
                        </tr>
                        <tr class="b">
                          <td align="left"> <code>priority</code> </td>
                          <td align="left"> int  </td>
                          <td align="left"> The priority of the container </td>
                        </tr>
                        <tr class="a">
                          <td align="left"> <code>assignedNodeId</code> </td>
                          <td align="left"> string </td>
                          <td align="left"> The assigned node host and port of the container </td>
                        </tr>
                      </tbody>
                    </table>
                  </section>
                  <section>
                    <h3><a name="Response_Examples:"></a>Response Examples:</h3>
                    <section>
                      <h4><a name="JSON_response"></a>JSON response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts/appattempt_1430424020775_0001_000001/containers/container_1430424020775_0001_01_000001
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/json
Transfer-Encoding: chunked
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>{
  "containerId": "container_1430424020775_0001_01_000001",
  "allocatedMB": 2048,
  "allocatedVCores": 1,
  "assignedNodeId": "localhost:9105",
  "priority": 0,
  "startedTime": 1430424054314,
  "finishedTime": 1430424079022,
  "elapsedTime": 24708,
  "diagnosticsInfo": "",
  "logUrl": "http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000001/container_1430424020775_0001_01_000001/zshen",
  "containerExitStatus": 0,
  "containerState": "COMPLETE",
  "nodeHttpAddress": "http://localhost:8042"
}
</pre>
                        </div>
                      </div>
                    </section>
                    <section>
                      <h4><a name="XML_response"></a>XML response</h4>
                      <p>HTTP Request:</p>
                      <div class="source">
                        <div class="source">
                          <pre>GET http://localhost:8188/ws/v1/applicationhistory/apps/application_1430424020775_0001/appattempts/appattempt_1430424020775_0001_000001/containers/container_1430424020775_0001_01_000001
Accept: application/xml
</pre>
                        </div>
                      </div>
                      <p>Response Header:</p>
                      <div class="source">
                        <div class="source">
                          <pre>HTTP/1.1 200 OK
Content-Type: application/xml
Content-Length: 669
</pre>
                        </div>
                      </div>
                      <p>Response Body:</p>
                      <div class="source">
                        <div class="source">
                          <pre>&lt;?xml version="1.0" encoding="UTF-8" standalone="yes"?&gt;
&lt;container&gt;
  &lt;containerId&gt;container_1430424020775_0001_01_000001&lt;/containerId&gt;
  &lt;allocatedMB&gt;2048&lt;/allocatedMB&gt;
  &lt;allocatedVCores&gt;1&lt;/allocatedVCores&gt;
  &lt;assignedNodeId&gt;localhost:9105&lt;/assignedNodeId&gt;
  &lt;priority&gt;0&lt;/priority&gt;
  &lt;startedTime&gt;1430424054314&lt;/startedTime&gt;
  &lt;finishedTime&gt;1430424079022&lt;/finishedTime&gt;
  &lt;elapsedTime&gt;24708&lt;/elapsedTime&gt;
  &lt;diagnosticsInfo&gt;&lt;/diagnosticsInfo&gt;
  &lt;logUrl&gt;http://0.0.0.0:8188/applicationhistory/logs/localhost:9105/container_1430424020775_0001_01_000001/container_1430424020775_0001_01_000001/zshen&lt;/logUrl&gt;
  &lt;containerExitStatus&gt;0&lt;/containerExitStatus&gt;
  &lt;containerState&gt;COMPLETE&lt;/containerState&gt;
  &lt;nodeHttpAddress&gt;http://localhost:8042&lt;/nodeHttpAddress&gt;
&lt;/container&gt;
</pre>
                        </div>
                      </div>
                    </section>
                  </section>
                  <section>
                    <h3><a name="Response_Codes"></a>Response Codes</h3>
                    <ol style="list-style-type: decimal">
                      <li>Queries where a domain, entity type, entity ID or similar cannot be resolved result in HTTP 404, “Not Found” responses.</li>
                      <li>Requests in which the path, parameters or values are invalid result in Bad Request, 400, responses.</li>
                      <li>In a secure cluster, a 401, “Forbidden”, response is generated when attempting to perform operations to which the caller does not have the sufficient rights. There is an exception to this when querying some entities, such as Domains; here the API deliberately downgrades permission-denied outcomes as empty and not-founds responses. This hides details of other domains from an unauthorized caller.</li>
                      <li>If the content of timeline entity PUT operations is invalid, this failure <i>will not</i> result in an HTTP error code being retured. A status code of 200 will be returned —however, there will be an error code in the list of failed entities for each entity which could not be added.</li>
                    </ol>
                  </section>
                </section>
                <section>
                  <h2><a name="Timeline_Server_Performance_Test_Tool"></a><a name="TIMELINE_SERVER_PERFORMANCE_TEST_TOOL"></a> Timeline Server Performance Test Tool</h2>
                  <section>
                    <h3><a name="Highlights"></a><a name="HIGHLIGHTS"></a>Highlights</h3>
                    <p>The timeline server performance test tool helps measure timeline server’s write performance. The test launches SimpleEntityWriter mappers or JobHistoryFileReplay mappers to write timeline entities to the timeline server. At the end, the transaction rate(ops/s) per mapper and the total transaction rate will be measured and printed out. Running the test with SimpleEntityWriter mappers will also measure and show the IO rate(KB/s) per mapper and the total IO rate.</p>
                  </section>
                  <section>
                    <h3><a name="Usage"></a><a name="USAGE"></a>Usage</h3>
                    <p>Mapper Types Description:</p>
                    <ol style="list-style-type: decimal">
                      <li>
                        SimpleEntityWriter mapper Each mapper writes a user-specified number of timeline entities with a user-specified size to the timeline server. SimpleEntityWrite is a default mapper of the performance test tool.
                        <ol style="list-style-type: decimal">
                          <li>JobHistoryFileReplay mapper Each mapper replays jobhistory files under a specified directory (both the jhist file and its corresponding conf.xml are required to be present in order to be replayed. The number of mappers should be no more than the number of jobhistory files). Each mapper will get assigned some jobhistory files to replay. For each job history file, a mapper will parse it to get jobinfo and then create timeline entities. Each mapper also has the choice to write all the timeline entities created at once or one at a time.</li>
                        </ol>
                      </li>
                    </ol>
                    <p>Options:</p>
                    <div class="source">
                      <div class="source">
                        <pre>[-m &lt;maps&gt;] number of mappers (default: 1)
[-v] timeline service version
[-mtype &lt;mapper type in integer&gt;]
      1. simple entity write mapper (default)
      2. jobhistory files replay mapper
[-s &lt;(KBs)test&gt;] number of KB per put (mtype=1, default: 1 KB)
[-t] package sending iterations per mapper (mtype=1, default: 100)
[-d &lt;path&gt;] root path of job history files (mtype=2)
[-r &lt;replay mode&gt;] (mtype=2)
      1. write all entities for a job in one put (default)
      2. write one entity at a time
</pre>
                      </div>
                    </div>
                  </section>
                  <section>
                    <h3><a name="Sample_Runs"></a><a name="SAMPLE_RUNS"></a>Sample Runs</h3>
                    <p>Run SimpleEntityWriter test:</p>
                    <div class="source">
                      <div class="source">
                        <pre>bin/hadoop jar performanceTest.jar timelineperformance -m 4 -mtype 1 -s 3 -t 200
</pre>
                      </div>
                    </div>
                    <p>Example output of SimpleEntityWriter test :</p>
                    <div class="source">
                      <div class="source">
                        <pre>TRANSACTION RATE (per mapper): 20000.0 ops/s
IO RATE (per mapper): 60000.0 KB/s
TRANSACTION RATE (total): 80000.0 ops/s
IO RATE (total): 240000.0 KB/s
</pre>
                      </div>
                    </div>
                    <p>Run JobHistoryFileReplay mapper test</p>
                    <div class="source">
                      <div class="source">
                        <pre>$ bin/hadoop jar performanceTest.jar timelineperformance -m 2 -mtype 2 -d /testInput -r 2
</pre>
                      </div>
                    </div>
                    <p>Example input of JobHistoryFileReplay mapper test:</p>
                    <div class="source">
                      <div class="source">
                        <pre>$ bin/hadoop fs -ls /testInput
/testInput/job_1.jhist
/testInput/job_1_conf.xml
/testInput/job_2.jhist
/testInput/job_2_conf.xml
</pre>
                      </div>
                    </div>
                    <p>Example output of JobHistoryFileReplay test:</p>
                    <div class="source">
                      <div class="source">
                        <pre>TRANSACTION RATE (per mapper): 4000.0 ops/s
IO RATE (per mapper): 0.0 KB/s
TRANSACTION RATE (total): 8000.0 ops/s
IO RATE (total): 0.0 KB/s
</pre>
                      </div>
                    </div>
                  </section>
                </section>
              </div>
            </div>
          </section>
          <section>
            <div id="bodyColumn">
              <div id="contentBox">
                <!---
                  Licensed under the Apache License, Version 2.0 (the "License");
                  you may not use this file except in compliance with the License.
                  You may obtain a copy of the License at

                   http://www.apache.org/licenses/LICENSE-2.0

                  Unless required by applicable law or agreed to in writing, software
                  distributed under the License is distributed on an "AS IS" BASIS,
                  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
                  See the License for the specific language governing permissions and
                  limitations under the License. See accompanying LICENSE file.
                  -->
                <h1>Native Libraries Guide</h1>
                <ul>
                  <li><a href="#Overview">Overview</a></li>
                  <li><a href="#Native_Hadoop_Library">Native Hadoop Library</a></li>
                  <li><a href="#Usage">Usage</a></li>
                  <li><a href="#Components">Components</a></li>
                  <li><a href="#Supported_Platforms">Supported Platforms</a></li>
                  <li><a href="#Download">Download</a></li>
                  <li><a href="#Build">Build</a></li>
                  <li><a href="#Runtime">Runtime</a></li>
                  <li><a href="#Check">Check</a></li>
                  <li><a href="#Native_Shared_Libraries">Native Shared Libraries</a></li>
                </ul>
                <section>
                  <h2><a name="Overview"></a>Overview</h2>
                  <p>This guide describes the native hadoop library and includes a small discussion about native shared libraries.</p>
                  <p>Note: Depending on your environment, the term “native libraries” could refer to all *.so’s you need to compile; and, the term “native compression” could refer to all *.so’s you need to compile that are specifically related to compression. Currently, however, this document only addresses the native hadoop library (<code>libhadoop.so</code>). The document for libhdfs library (<code>libhdfs.so</code>) is <a href="../hadoop-hdfs/LibHdfs.html">here</a>.</p>
                </section>
                <section>
                  <h2><a name="Native_Hadoop_Library"></a>Native Hadoop Library</h2>
                  <p>Hadoop has native implementations of certain components for performance reasons and for non-availability of Java implementations. These components are available in a single, dynamically-linked native library called the native hadoop library. On the *nix platforms the library is named <code>libhadoop.so</code>.</p>
                </section>
                <section>
                  <h2><a name="Usage"></a>Usage</h2>
                  <p>It is fairly easy to use the native hadoop library:</p>
                  <ol style="list-style-type: decimal">
                    <li>Review the components.</li>
                    <li>Review the supported platforms.</li>
                    <li>Either download a hadoop release, which will include a pre-built version of the native hadoop library, or build your own version of the native hadoop library. Whether you download or build, the name for the library is the same: libhadoop.so</li>
                    <li>
                      Install the compression codec development packages (&gt;zlib-1.2, &gt;gzip-1.2):
                      <ul>
                        <li>If you download the library, install one or more development packages - whichever compression codecs you want to use with your deployment.</li>
                        <li>If you build the library, it is mandatory to install both development packages.</li>
                      </ul>
                    </li>
                    <li>Check the runtime log files.</li>
                  </ol>
                </section>
                <section>
                  <h2><a name="Components"></a>Components</h2>
                  <p>The native hadoop library includes various components:</p>
                  <ul>
                    <li>Compression Codecs (bzip2, lz4, zlib)</li>
                    <li>Native IO utilities for <a href="../hadoop-hdfs/ShortCircuitLocalReads.html">HDFS Short-Circuit Local Reads</a> and <a href="../hadoop-hdfs/CentralizedCacheManagement.html">Centralized Cache Management in HDFS</a></li>
                    <li>CRC32 checksum implementation</li>
                  </ul>
                </section>
                <section>
                  <h2><a name="Supported_Platforms"></a>Supported Platforms</h2>
                  <p>The native hadoop library is supported on *nix platforms only. The library does not to work with Cygwin or the Mac OS X platform.</p>
                  <p>The native hadoop library is mainly used on the GNU/Linus platform and has been tested on these distributions:</p>
                  <ul>
                    <li>RHEL4/Fedora</li>
                    <li>Ubuntu</li>
                    <li>Gentoo</li>
                  </ul>
                  <p>On all the above distributions a 32/64 bit native hadoop library will work with a respective 32/64 bit jvm.</p>
                </section>
                <section>
                  <h2><a name="Download"></a>Download</h2>
                  <p>The pre-built 32-bit i386-Linux native hadoop library is available as part of the hadoop distribution and is located in the <code>lib/native</code> directory. You can download the hadoop distribution from Hadoop Common Releases.</p>
                  <p>Be sure to install the zlib and/or gzip development packages - whichever compression codecs you want to use with your deployment.</p>
                </section>
                <section>
                  <h2><a name="Build"></a>Build</h2>
                  <p>The native hadoop library is written in ANSI C and is built using the GNU autotools-chain (autoconf, autoheader, automake, autoscan, libtool). This means it should be straight-forward to build the library on any platform with a standards-compliant C compiler and the GNU autotools-chain (see the supported platforms).</p>
                  <p>The packages you need to install on the target platform are:</p>
                  <ul>
                    <li>C compiler (e.g. GNU C Compiler)</li>
                    <li>GNU Autools Chain: autoconf, automake, libtool</li>
                    <li>zlib-development package (stable version &gt;= 1.2.0)</li>
                    <li>openssl-development package(e.g. libssl-dev)</li>
                  </ul>
                  <p>Once you installed the prerequisite packages use the standard hadoop pom.xml file and pass along the native flag to build the native hadoop library:</p>
                  <div class="source">
                    <div class="source">
                      <pre>   $ mvn package -Pdist,native -DskipTests -Dtar
</pre>
                    </div>
                  </div>
                  <p>You should see the newly-built library in:</p>
                  <div class="source">
                    <div class="source">
                      <pre>   $ hadoop-dist/target/hadoop-3.3.6/lib/native
</pre>
                    </div>
                  </div>
                  <p>Please note the following:</p>
                  <ul>
                    <li>It is mandatory to install both the zlib and gzip development packages on the target platform in order to build the native hadoop library; however, for deployment it is sufficient to install just one package if you wish to use only one codec.</li>
                    <li>It is necessary to have the correct 32/64 libraries for zlib, depending on the 32/64 bit jvm for the target platform, in order to build and deploy the native hadoop library.</li>
                  </ul>
                </section>
                <section>
                  <h2><a name="Runtime"></a>Runtime</h2>
                  <p>The bin/hadoop script ensures that the native hadoop library is on the library path via the system property: <code>-Djava.library.path=&lt;path&gt;</code></p>
                  <p>During runtime, check the hadoop log files for your MapReduce tasks.</p>
                  <ul>
                    <li>If everything is all right, then: <code>DEBUG util.NativeCodeLoader - Trying to load the custom-built native-hadoop library...</code> <code>INFO util.NativeCodeLoader - Loaded the native-hadoop library</code></li>
                    <li>If something goes wrong, then: <code>INFO util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</code></li>
                  </ul>
                </section>
                <section>
                  <h2><a name="Check"></a>Check</h2>
                  <p>NativeLibraryChecker is a tool to check whether native libraries are loaded correctly. You can launch NativeLibraryChecker as follows:</p>
                  <div class="source">
                    <div class="source">
                      <pre>   $ hadoop checknative -a
   14/12/06 01:30:45 WARN bzip2.Bzip2Factory: Failed to load/initialize native-bzip2 library system-native, will use pure-Java version
   14/12/06 01:30:45 INFO zlib.ZlibFactory: Successfully loaded &amp; initialized native-zlib library
   Native library checking:
   hadoop: true /home/ozawa/hadoop/lib/native/libhadoop.so.1.0.0
   zlib:   true /lib/x86_64-linux-gnu/libz.so.1
   zstd: true /usr/lib/libzstd.so.1
   lz4:    true revision:99
   bzip2:  false
</pre>
                    </div>
                  </div>
                </section>
                <section>
                  <h2><a name="Native_Shared_Libraries"></a>Native Shared Libraries</h2>
                  <p>You can load any native shared library using DistributedCache for distributing and symlinking the library files.</p>
                  <p>This example shows you how to distribute a shared library, mylib.so, and load it from a MapReduce task.</p>
                  <ol style="list-style-type: decimal">
                    <li>First copy the library to the HDFS: <code>bin/hadoop fs -copyFromLocal mylib.so.1 /libraries/mylib.so.1</code></li>
                    <li>The job launching program should contain the following: <code>DistributedCache.createSymlink(conf);</code> <code>DistributedCache.addCacheFile("hdfs://host:port/libraries/mylib.so. 1#mylib.so", conf);</code></li>
                    <li>The MapReduce task can contain: <code>System.loadLibrary("mylib.so");</code></li>
                  </ol>
                  <p>Note: If you downloaded or built the native hadoop library, you don’t need to use DistibutedCache to make the library available to your MapReduce tasks.</p>
                </section>
              </div>
            </div>
          </section>
        </div>
      </div>
    </main>
  </body>
</html>